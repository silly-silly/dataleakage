<script>
    let highlighted = [];
    function highlight_lines(lines) {
        for (let line of highlighted) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = '';
        }
        highlighted = lines;
        for (let line of highlighted) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = 'yellow';
        }
    }
    let marked = [];
    function mark_leak_lines(lines) {
        for (let line of marked) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = '';
        }
        marked = lines;
        for (let line of marked) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = ele.style.backgroundColor = 'lightgreen';
        }
    }
    function show_infos(lines) {
        for (let line of lines) {
            let ele = document.getElementById(String(line) + "-info");
            if (ele) {
                ele.style.display = ele.style.display == 'none'? '': 'none'
            }
        }
    }
</script>
    <style type="text/css">
    .sum table {
    font-family: arial, sans-serif;
    border-collapse: collapse;
    width: 100%;
    }

    .sum td, .sum th {
    border: 1px solid #dddddd;
    text-align: left;
    padding: 8px;
    }

    .sum tr:hover {background-color: #D6EEEE;}
</style>
<center>
<table class="sum">
  <tbody><tr>
    <th>Leakage</th>
    <th>#Detected</th>
    <th>Locations</th>
  </tr>
  <tr>
    <td>Pre-processing leakage</td>
    <td>3</td>
    <td><a href="#327"><button type="button" style="line-height: 85%; None" onclick="None">327</button></a> <a href="#488"><button type="button" style="line-height: 85%; None" onclick="None">488</button></a> <a href="#82"><button type="button" style="line-height: 85%; None" onclick="None">82</button></a></td>
  </tr>
  <tr>
    <td>Overlap leakage</td>
    <td>0</td>
    <td></td>
  </tr>
  <tr>
    <td>No independence test data</td>
    <td>0</td>
    <td></td>
  </tr>
</tbody></table></center>

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<!--
generated by Pygments <https://pygments.org/>
Copyright 2006-2021 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
-->
<html>
<head>
  <title></title>
  <meta http-equiv="content-type" content="text/html; charset=None">
  <style type="text/css">
/*
generated by Pygments <https://pygments.org/>
Copyright 2006-2021 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
pre { line-height: 145%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
body .hll { background-color: #ffffcc }
body { background: #f8f8f8; }
body .c { color: #408080; font-style: italic } /* Comment */
body .err { border: 1px solid #FF0000 } /* Error */
body .k { color: #008000; font-weight: bold } /* Keyword */
body .o { color: #666666 } /* Operator */
body .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
body .cm { color: #408080; font-style: italic } /* Comment.Multiline */
body .cp { color: #BC7A00 } /* Comment.Preproc */
body .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
body .c1 { color: #408080; font-style: italic } /* Comment.Single */
body .cs { color: #408080; font-style: italic } /* Comment.Special */
body .gd { color: #A00000 } /* Generic.Deleted */
body .ge { font-style: italic } /* Generic.Emph */
body .gr { color: #FF0000 } /* Generic.Error */
body .gh { color: #000080; font-weight: bold } /* Generic.Heading */
body .gi { color: #00A000 } /* Generic.Inserted */
body .go { color: #888888 } /* Generic.Output */
body .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
body .gs { font-weight: bold } /* Generic.Strong */
body .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
body .gt { color: #0044DD } /* Generic.Traceback */
body .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
body .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
body .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
body .kp { color: #008000 } /* Keyword.Pseudo */
body .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
body .kt { color: #B00040 } /* Keyword.Type */
body .m { color: #666666 } /* Literal.Number */
body .s { color: #BA2121 } /* Literal.String */
body .na { color: #7D9029 } /* Name.Attribute */
body .nb { color: #008000 } /* Name.Builtin */
body .nc { color: #0000FF; font-weight: bold } /* Name.Class */
body .no { color: #880000 } /* Name.Constant */
body .nd { color: #AA22FF } /* Name.Decorator */
body .ni { color: #999999; font-weight: bold } /* Name.Entity */
body .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
body .nf { color: #0000FF } /* Name.Function */
body .nl { color: #A0A000 } /* Name.Label */
body .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
body .nt { color: #008000; font-weight: bold } /* Name.Tag */
body .nv { color: #19177C } /* Name.Variable */
body .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
body .w { color: #bbbbbb } /* Text.Whitespace */
body .mb { color: #666666 } /* Literal.Number.Bin */
body .mf { color: #666666 } /* Literal.Number.Float */
body .mh { color: #666666 } /* Literal.Number.Hex */
body .mi { color: #666666 } /* Literal.Number.Integer */
body .mo { color: #666666 } /* Literal.Number.Oct */
body .sa { color: #BA2121 } /* Literal.String.Affix */
body .sb { color: #BA2121 } /* Literal.String.Backtick */
body .sc { color: #BA2121 } /* Literal.String.Char */
body .dl { color: #BA2121 } /* Literal.String.Delimiter */
body .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
body .s2 { color: #BA2121 } /* Literal.String.Double */
body .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
body .sh { color: #BA2121 } /* Literal.String.Heredoc */
body .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
body .sx { color: #008000 } /* Literal.String.Other */
body .sr { color: #BB6688 } /* Literal.String.Regex */
body .s1 { color: #BA2121 } /* Literal.String.Single */
body .ss { color: #19177C } /* Literal.String.Symbol */
body .bp { color: #008000 } /* Name.Builtin.Pseudo */
body .fm { color: #0000FF } /* Name.Function.Magic */
body .vc { color: #19177C } /* Name.Variable.Class */
body .vg { color: #19177C } /* Name.Variable.Global */
body .vi { color: #19177C } /* Name.Variable.Instance */
body .vm { color: #19177C } /* Name.Variable.Magic */
body .il { color: #666666 } /* Literal.Number.Integer.Long */

  </style>
</head>
<body>
<h2></h2>

<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span id="2"><span class="c1"># coding: utf-8</span></span>
<span id="3"></span>
<span id="4"><span class="c1"># # Classification</span></span>
<span id="5"></span>
<span id="6"><span class="c1"># $$</span></span>
<span id="7"><span class="c1"># \renewcommand{\like}{{\cal L}}</span></span>
<span id="8"><span class="c1"># \renewcommand{\loglike}{{\ell}}</span></span>
<span id="9"><span class="c1"># \renewcommand{\err}{{\cal E}}</span></span>
<span id="10"><span class="c1"># \renewcommand{\dat}{{\cal D}}</span></span>
<span id="11"><span class="c1"># \renewcommand{\hyp}{{\cal H}}</span></span>
<span id="12"><span class="c1"># \renewcommand{\Ex}[2]{E_{#1}[#2]}</span></span>
<span id="13"><span class="c1"># \renewcommand{\x}{{\mathbf x}}</span></span>
<span id="14"><span class="c1"># \renewcommand{\v}[1]{{\mathbf #1}}</span></span>
<span id="15"><span class="c1"># $$</span></span>
<span id="16"></span>
<span id="17"><span class="c1"># **Note:** We&#39;ve adapted this Mini Project from [Lab 5 in the CS109](https://github.com/cs109/2015lab5) course. Please feel free to check out the original lab, both for more exercises, as well as solutions.</span></span>
<span id="18"></span>
<span id="19"><span class="c1"># We turn our attention to **classification**[^classification]. Classification tries to predict, which of a small set of classes, a sample in a population belongs to. Mathematically, the aim is to find $y$, a **label** based on knowing a feature vector $\x$. For instance, consider predicting gender from seeing a person&#39;s face, something we do fairly well as humans. To have a machine do this well, we would typically feed the machine a bunch of images of people which have been labelled &quot;male&quot; or &quot;female&quot; (the training set), and have it learn the gender of the person in the image. Then, given a new photo, the algorithm learned returns us the gender of the person in the photo.</span></span>
<span id="20"><span class="c1"># </span></span>
<span id="21"><span class="c1"># There are different ways of making classifications. One idea is shown schematically in the image below, where we find a line that divides &quot;things&quot; of two different types in a 2-dimensional feature space.</span></span>
<span id="22"><span class="c1"># </span></span>
<span id="23"><span class="c1"># ![Splitting using a single line](images/onelinesplit.png)</span></span>
<span id="24"><span class="c1"># </span></span>
<span id="25"><span class="c1"># </span></span>
<span id="26"></span>
<span id="27"><span class="c1"># In[3]:</span></span>
<span id="28"></span>
<span id="29"></span>
<span id="30"><span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">&#39;matplotlib&#39;</span><span class="p">,</span> <span class="s1">&#39;inline&#39;</span><span class="p">)</span></span>
<span id="31"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span></span>
<span id="32"><span class="kn">import</span> <span class="nn">scipy</span> <span class="k">as</span> <span class="nn">sp</span></span>
<span id="33"><span class="kn">import</span> <span class="nn">matplotlib</span> <span class="k">as</span> <span class="nn">mpl</span></span>
<span id="34"><span class="kn">import</span> <span class="nn">matplotlib.cm</span> <span class="k">as</span> <span class="nn">cm</span></span>
<span id="35"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span></span>
<span id="36"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span></span>
<span id="37"><span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.width&#39;</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span></span>
<span id="38"><span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.max_columns&#39;</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span></span>
<span id="39"><span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.notebook_repr_html&#39;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span></span>
<span id="40"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span></span>
<span id="41"><span class="n">sns</span><span class="o">.</span><span class="n">set_style</span><span class="p">(</span><span class="s2">&quot;whitegrid&quot;</span><span class="p">)</span></span>
<span id="42"><span class="n">sns</span><span class="o">.</span><span class="n">set_context</span><span class="p">(</span><span class="s2">&quot;poster&quot;</span><span class="p">)</span></span>
<span id="43"></span>
<span id="44"></span>
<span id="45"><span class="c1"># In[4]:</span></span>
<span id="46"></span>
<span id="47"></span>
<span id="48"><span class="n">c0</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span></span>
<span id="49"><span class="n">c1</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">1</span><span class="p">]</span></span>
<span id="50"><span class="n">c2</span><span class="o">=</span><span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">()[</span><span class="mi">2</span><span class="p">]</span></span>
<span id="51"></span>
<span id="52"></span>
<span id="53"><span class="c1"># In[5]:</span></span>
<span id="54"></span>
<span id="55"></span>
<span id="56"><span class="kn">from</span> <span class="nn">matplotlib.colors</span> <span class="kn">import</span> <span class="n">ListedColormap</span></span>
<span id="57"><span class="n">cmap_light</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FFAAAA&#39;</span><span class="p">,</span> <span class="s1">&#39;#AAFFAA&#39;</span><span class="p">,</span> <span class="s1">&#39;#AAAAFF&#39;</span><span class="p">])</span></span>
<span id="58"><span class="n">cmap_bold</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="s1">&#39;#00FF00&#39;</span><span class="p">,</span> <span class="s1">&#39;#0000FF&#39;</span><span class="p">])</span></span>
<span id="59"><span class="n">cm</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">cm</span><span class="o">.</span><span class="n">RdBu</span></span>
<span id="60"><span class="n">cm_bright</span> <span class="o">=</span> <span class="n">ListedColormap</span><span class="p">([</span><span class="s1">&#39;#FF0000&#39;</span><span class="p">,</span> <span class="s1">&#39;#0000FF&#39;</span><span class="p">])</span></span>
<span id="61"></span>
<span id="62"><span class="k">def</span> <span class="nf">points_plot</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">Xtr</span><span class="p">,</span> <span class="n">Xte</span><span class="p">,</span> <span class="n">ytr</span><span class="p">,</span> <span class="n">yte</span><span class="p">,</span> <span class="n">clf</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">colorscale</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">cdiscrete</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">psize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">zfunc</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">predicted</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span></span>
<span id="63">    <span class="n">h</span> <span class="o">=</span> <span class="mf">.02</span></span>
<span id="64">    <span class="n">X</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">Xtr</span><span class="p">,</span> <span class="n">Xte</span><span class="p">))</span></span>
<span id="65">    <span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">.5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">.5</span></span>
<span id="66">    <span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">-</span> <span class="mf">.5</span><span class="p">,</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">+</span> <span class="mf">.5</span></span>
<span id="67">    <span class="n">xx</span><span class="p">,</span> <span class="n">yy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">meshgrid</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">x_min</span><span class="p">,</span> <span class="n">x_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span></span>
<span id="68">                         <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">y_min</span><span class="p">,</span> <span class="n">y_max</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span></span>
<span id="69"></span>
<span id="70">    <span class="c1">#plt.figure(figsize=(10,6))</span></span>
<span id="71">    <span class="k">if</span> <span class="n">zfunc</span><span class="p">:</span></span>
<span id="72">        <span class="n">p0</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])[:,</span> <span class="mi">0</span><span class="p">]</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([325, 326, 327, 488, 72, 73, 76, 81, 82, 99])">highlight train/test sites</button>
<span id="73">        <span class="n">p1</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])[:,</span> <span class="mi">1</span><span class="p">]</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([325, 326, 327, 488, 72, 73, 76, 81, 82, 99])">highlight train/test sites</button>
<span id="74">        <span class="n">Z</span><span class="o">=</span><span class="n">zfunc</span><span class="p">(</span><span class="n">p0</span><span class="p">,</span> <span class="n">p1</span><span class="p">)</span></span>
<span id="75">    <span class="k">else</span><span class="p">:</span></span>
<span id="76">        <span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([325, 326, 327, 488, 72, 73, 76, 81, 82, 99])">highlight train/test sites</button>
<span id="77">    <span class="n">ZZ</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></span>
<span id="78">    <span class="k">if</span> <span class="n">mesh</span><span class="p">:</span></span>
<span id="79">        <span class="n">plt</span><span class="o">.</span><span class="n">pcolormesh</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">ZZ</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span></span>
<span id="80">    <span class="k">if</span> <span class="n">predicted</span><span class="p">:</span></span>
<span id="81">        <span class="n">showtr</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtr</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([325, 326, 327, 488, 72, 73, 76, 81, 82, 99])">highlight train/test sites</button>
<span id="82">        <span class="n">showte</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xte</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([325, 326, 327, 488, 72, 73, 76, 81, 82, 99])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">potential preprocessing leakage</button> <a href="#318"><button type="button" style="line-height: 85%; None" onclick="mark_leak_lines([318, 318])">show and go to first leak src</button></a>
<span id="83">    <span class="k">else</span><span class="p">:</span></span>
<span id="84">        <span class="n">showtr</span> <span class="o">=</span> <span class="n">ytr</span></span>
<span id="85">        <span class="n">showte</span> <span class="o">=</span> <span class="n">yte</span></span>
<span id="86">    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xtr</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Xtr</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">showtr</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">psize</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span><span class="n">edgecolor</span><span class="o">=</span><span class="s2">&quot;k&quot;</span><span class="p">)</span></span>
<span id="87">    <span class="c1"># and testing points</span></span>
<span id="88">    <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">Xte</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">Xte</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">c</span><span class="o">=</span><span class="n">showte</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;s&quot;</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="n">psize</span><span class="o">+</span><span class="mi">10</span><span class="p">)</span></span>
<span id="89">    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">xx</span><span class="o">.</span><span class="n">max</span><span class="p">())</span></span>
<span id="90">    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">yy</span><span class="o">.</span><span class="n">min</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">max</span><span class="p">())</span></span>
<span id="91">    <span class="k">return</span> <span class="n">ax</span><span class="p">,</span><span class="n">xx</span><span class="p">,</span><span class="n">yy</span></span>
<span id="92"></span>
<span id="93"></span>
<span id="94"><span class="c1"># In[6]:</span></span>
<span id="95"></span>
<span id="96"></span>
<span id="97"><span class="k">def</span> <span class="nf">points_plot_prob</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">Xtr</span><span class="p">,</span> <span class="n">Xte</span><span class="p">,</span> <span class="n">ytr</span><span class="p">,</span> <span class="n">yte</span><span class="p">,</span> <span class="n">clf</span><span class="p">,</span> <span class="n">colorscale</span><span class="o">=</span><span class="n">cmap_light</span><span class="p">,</span> <span class="n">cdiscrete</span><span class="o">=</span><span class="n">cmap_bold</span><span class="p">,</span> <span class="n">ccolor</span><span class="o">=</span><span class="n">cm</span><span class="p">,</span> <span class="n">psize</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span></span>
<span id="98">    <span class="n">ax</span><span class="p">,</span><span class="n">xx</span><span class="p">,</span><span class="n">yy</span> <span class="o">=</span> <span class="n">points_plot</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">Xtr</span><span class="p">,</span> <span class="n">Xte</span><span class="p">,</span> <span class="n">ytr</span><span class="p">,</span> <span class="n">yte</span><span class="p">,</span> <span class="n">clf</span><span class="p">,</span> <span class="n">mesh</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">colorscale</span><span class="o">=</span><span class="n">colorscale</span><span class="p">,</span> <span class="n">cdiscrete</span><span class="o">=</span><span class="n">cdiscrete</span><span class="p">,</span> <span class="n">psize</span><span class="o">=</span><span class="n">psize</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">predicted</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> </span>
<span id="99">    <span class="n">Z</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">c_</span><span class="p">[</span><span class="n">xx</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">yy</span><span class="o">.</span><span class="n">ravel</span><span class="p">()])[:,</span> <span class="mi">1</span><span class="p">]</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([325, 326, 327, 488, 72, 73, 76, 81, 82, 99])">highlight train/test sites</button>
<span id="100">    <span class="n">Z</span> <span class="o">=</span> <span class="n">Z</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">xx</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span></span>
<span id="101">    <span class="n">plt</span><span class="o">.</span><span class="n">contourf</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">ccolor</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.2</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span></span>
<span id="102">    <span class="n">cs2</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span> <span class="n">yy</span><span class="p">,</span> <span class="n">Z</span><span class="p">,</span> <span class="n">cmap</span><span class="o">=</span><span class="n">ccolor</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.6</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span></span>
<span id="103">    <span class="n">plt</span><span class="o">.</span><span class="n">clabel</span><span class="p">(</span><span class="n">cs2</span><span class="p">,</span> <span class="n">fmt</span> <span class="o">=</span> <span class="s1">&#39;</span><span class="si">%2.1f</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">colors</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">axes</span><span class="o">=</span><span class="n">ax</span><span class="p">)</span></span>
<span id="104">    <span class="k">return</span> <span class="n">ax</span> </span>
<span id="105"></span>
<span id="106"></span>
<span id="107"><span class="c1"># ## Using `sklearn`: The heights and weights example</span></span>
<span id="108"></span>
<span id="109"><span class="c1"># We&#39;ll use a dataset of heights and weights of males and females to hone our understanding of classifiers. We load the data into a dataframe and plot it.</span></span>
<span id="110"></span>
<span id="111"><span class="c1"># In[7]:</span></span>
<span id="112"></span>
<span id="113"></span>
<span id="114"><span class="n">dflog</span><span class="o">=</span><span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;data/01_heights_weights_genders.csv&quot;</span><span class="p">)</span></span>
<span id="115"><span class="n">dflog</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></span>
<span id="116"></span>
<span id="117"></span>
<span id="118"><span class="c1"># Remember that the form of data we will use always is</span></span>
<span id="119"><span class="c1"># </span></span>
<span id="120"><span class="c1"># ![dataform](images/dataform.jpg)</span></span>
<span id="121"><span class="c1"># </span></span>
<span id="122"><span class="c1"># with the &quot;response&quot; as a plain array</span></span>
<span id="123"><span class="c1"># </span></span>
<span id="124"><span class="c1"># `[1,1,0,0,0,1,0,1,0....]`.</span></span>
<span id="125"></span>
<span id="126"><span class="c1"># **Your turn:** </span></span>
<span id="127"><span class="c1"># </span></span>
<span id="128"><span class="c1"># * Create a scatter plot of Weight vs. Height</span></span>
<span id="129"><span class="c1"># * Color the points differently by Gender</span></span>
<span id="130"></span>
<span id="131"><span class="c1"># In[8]:</span></span>
<span id="132"></span>
<span id="133"></span>
<span id="134"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span></span>
<span id="135"></span>
<span id="136"><span class="n">male</span> <span class="o">=</span> <span class="n">dflog</span><span class="p">[</span><span class="n">dflog</span><span class="o">.</span><span class="n">Gender</span><span class="o">==</span><span class="s1">&#39;Male&#39;</span><span class="p">]</span></span>
<span id="137"><span class="n">female</span> <span class="o">=</span> <span class="n">dflog</span><span class="p">[</span><span class="n">dflog</span><span class="o">.</span><span class="n">Gender</span><span class="o">==</span><span class="s1">&#39;Female&#39;</span><span class="p">]</span></span>
<span id="138"></span>
<span id="139"><span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span></span>
<span id="140"><span class="n">ax1</span> <span class="o">=</span> <span class="n">fig</span><span class="o">.</span><span class="n">add_subplot</span><span class="p">(</span><span class="mi">111</span><span class="p">)</span></span>
<span id="141"></span>
<span id="142"><span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">male</span><span class="o">.</span><span class="n">Weight</span><span class="p">,</span> <span class="n">male</span><span class="o">.</span><span class="n">Height</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Male&#39;</span><span class="p">)</span></span>
<span id="143"><span class="n">ax1</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">female</span><span class="o">.</span><span class="n">Weight</span><span class="p">,</span> <span class="n">female</span><span class="o">.</span><span class="n">Height</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Female&#39;</span><span class="p">)</span></span>
<span id="144"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s1">&#39;upper left&#39;</span><span class="p">)</span></span>
<span id="145"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Male &amp; Female: Weight vs Height&#39;</span><span class="p">)</span></span>
<span id="146"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Weight&#39;</span><span class="p">)</span></span>
<span id="147"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Height&#39;</span><span class="p">)</span></span>
<span id="148"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span>
<span id="149"></span>
<span id="150"></span>
<span id="151"><span class="c1"># In[ ]:</span></span>
<span id="152"></span>
<span id="153"></span>
<span id="154"></span>
<span id="155"></span>
<span id="156"></span>
<span id="157"><span class="c1"># In the Linear Regression Mini Project, the last (extra credit) exercise was to write a K-Fold cross-validation. Feel free to use that code below, or just use the `cv_score` function we&#39;ve provided. </span></span>
<span id="158"></span>
<span id="159"><span class="c1"># In[9]:</span></span>
<span id="160"></span>
<span id="161"></span>
<span id="162"><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">KFold</span></span>
<span id="163"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span></span>
<span id="164"></span>
<span id="165"><span class="k">def</span> <span class="nf">cv_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">score_func</span><span class="o">=</span><span class="n">accuracy_score</span><span class="p">):</span></span>
<span id="166">    <span class="n">result</span> <span class="o">=</span> <span class="mi">0</span></span>
<span id="167">    <span class="n">nfold</span> <span class="o">=</span> <span class="mi">5</span></span>
<span id="168">    <span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">KFold</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">size</span><span class="p">,</span> <span class="n">nfold</span><span class="p">):</span> <span class="c1"># split data into train/test groups, 5 times</span></span>
<span id="169">        <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">])</span> <span class="c1"># fit</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">train</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([169, 170])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">no independent test data</button>
<span id="170">        <span class="n">result</span> <span class="o">+=</span> <span class="n">score_func</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="n">test</span><span class="p">]),</span> <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">])</span> <span class="c1"># evaluate score function on held-out data</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">validation</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([169, 170])">highlight train/test sites</button>
<span id="171">    <span class="k">return</span> <span class="n">result</span> <span class="o">/</span> <span class="n">nfold</span> <span class="c1"># average</span></span>
<span id="172"></span>
<span id="173"></span>
<span id="174"><span class="c1"># First, we try a basic Logistic Regression:</span></span>
<span id="175"><span class="c1"># </span></span>
<span id="176"><span class="c1"># * Split the data into a training and test (hold-out) set</span></span>
<span id="177"><span class="c1"># * Train on the training set, and test for accuracy on the testing set</span></span>
<span id="178"></span>
<span id="179"><span class="c1"># In[10]:</span></span>
<span id="180"></span>
<span id="181"></span>
<span id="182"><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span></span>
<span id="183"><span class="n">Xlr</span><span class="p">,</span> <span class="n">Xtestlr</span><span class="p">,</span> <span class="n">ylr</span><span class="p">,</span> <span class="n">ytestlr</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">dflog</span><span class="p">[[</span><span class="s1">&#39;Height&#39;</span><span class="p">,</span><span class="s1">&#39;Weight&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> </span>
<span id="184">                                              <span class="p">(</span><span class="n">dflog</span><span class="o">.</span><span class="n">Gender</span><span class="o">==</span><span class="s2">&quot;Male&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span></span>
<span id="185"><span class="nb">len</span><span class="p">(</span><span class="n">Xlr</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">Xtestlr</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">dflog</span><span class="p">[[</span><span class="s1">&#39;Height&#39;</span><span class="p">,</span><span class="s1">&#39;Weight&#39;</span><span class="p">]]</span><span class="o">.</span><span class="n">values</span><span class="p">),</span> <span class="nb">len</span><span class="p">((</span><span class="n">dflog</span><span class="o">.</span><span class="n">Gender</span><span class="o">==</span><span class="s2">&quot;Male&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">values</span><span class="p">)</span></span>
<span id="186"></span>
<span id="187"></span>
<span id="188"><span class="c1"># In[11]:</span></span>
<span id="189"></span>
<span id="190"></span>
<span id="191"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span></span>
<span id="192"><span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span></span>
<span id="193"><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xlr</span><span class="p">,</span><span class="n">ylr</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">train</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([193, 194])">highlight train/test sites</button>
<span id="194"><span class="nb">print</span><span class="p">((</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtestlr</span><span class="p">),</span><span class="n">ytestlr</span><span class="p">)))</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([193, 194])">highlight train/test sites</button>
<span id="195"></span>
<span id="196"></span>
<span id="197"><span class="c1"># In[12]:</span></span>
<span id="198"></span>
<span id="199"></span>
<span id="200"><span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span></span>
<span id="201"><span class="n">score</span> <span class="o">=</span> <span class="n">cv_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">Xlr</span><span class="p">,</span> <span class="n">ylr</span><span class="p">)</span></span>
<span id="202"><span class="nb">print</span><span class="p">(</span><span class="n">score</span><span class="p">)</span></span>
<span id="203"></span>
<span id="204"></span>
<span id="205"><span class="c1"># While this looks like a pretty great model, we would like to ensure two things:</span></span>
<span id="206"><span class="c1"># </span></span>
<span id="207"><span class="c1"># * We have found the best model (in terms of model parameters).</span></span>
<span id="208"><span class="c1"># * The model is highly likely to generalize i.e. perform well on unseen data.</span></span>
<span id="209"><span class="c1"># </span></span>
<span id="210"><span class="c1"># For tuning your model, you will use a mix of *cross-validation* and *grid search*. In Logistic Regression, the most important parameter to tune is the *regularization parameter* `C`. You will now implement some code to perform model tuning. </span></span>
<span id="211"></span>
<span id="212"><span class="c1"># **Your turn:** Implement the following search procedure to find a good model</span></span>
<span id="213"><span class="c1"># </span></span>
<span id="214"><span class="c1"># * You are given a list of possible values of `C` below</span></span>
<span id="215"><span class="c1"># * For each C:</span></span>
<span id="216"><span class="c1">#   * Create a logistic regression model with that value of C</span></span>
<span id="217"><span class="c1">#   * Find the average score for this model using the `cv_score` function **only on the training set** `(Xlr,ylr)`</span></span>
<span id="218"><span class="c1"># * Pick the C with the highest average score</span></span>
<span id="219"><span class="c1"># </span></span>
<span id="220"><span class="c1"># Your goal is to find the best model parameters based *only* on the training set, without showing the model test set at all (which is why the test set is also called a *hold-out* set).</span></span>
<span id="221"></span>
<span id="222"><span class="c1"># In[13]:</span></span>
<span id="223"></span>
<span id="224"></span>
<span id="225"><span class="c1">#the grid of parameters to search over</span></span>
<span id="226"><span class="n">Cs</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span></span>
<span id="227"></span>
<span id="228"><span class="c1">#your turn</span></span>
<span id="229"><span class="n">scores_per_C</span> <span class="o">=</span> <span class="p">{}</span></span>
<span id="230"><span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">Cs</span><span class="p">:</span></span>
<span id="231">    <span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">c</span><span class="p">)</span></span>
<span id="232">    <span class="n">scores_per_C</span><span class="o">.</span><span class="n">update</span><span class="p">({</span><span class="n">c</span><span class="p">:</span><span class="n">cv_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">Xlr</span><span class="p">,</span> <span class="n">ylr</span><span class="p">)})</span></span>
<span id="233">    </span>
<span id="234"><span class="n">v</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">scores_per_C</span><span class="o">.</span><span class="n">values</span><span class="p">())</span></span>
<span id="235"><span class="n">k</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">scores_per_C</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span></span>
<span id="236"><span class="n">C</span> <span class="o">=</span> <span class="n">k</span><span class="p">[</span><span class="n">v</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">v</span><span class="p">))]</span></span>
<span id="237"><span class="n">scores_per_C</span></span>
<span id="238"></span>
<span id="239"></span>
<span id="240"><span class="c1"># **Your turn:** Now you want to estimate how this model will predict on unseen data in the following way:</span></span>
<span id="241"><span class="c1"># </span></span>
<span id="242"><span class="c1"># * Use the C you obtained from the procedure earlier and train a Logistic Regression on the training data</span></span>
<span id="243"><span class="c1"># * Calculate the accuracy on the test data</span></span>
<span id="244"></span>
<span id="245"><span class="c1"># In[24]:</span></span>
<span id="246"></span>
<span id="247"></span>
<span id="248"><span class="c1">#your turn</span></span>
<span id="249"><span class="n">clf</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">C</span><span class="p">)</span></span>
<span id="250"><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xlr</span><span class="p">,</span><span class="n">ylr</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">train</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([250, 251])">highlight train/test sites</button>
<span id="251"><span class="nb">print</span><span class="p">((</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">Xtestlr</span><span class="p">),</span><span class="n">ytestlr</span><span class="p">)))</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([250, 251])">highlight train/test sites</button>
<span id="252"></span>
<span id="253"></span>
<span id="254"><span class="c1"># **Things to think about**</span></span>
<span id="255"><span class="c1"># </span></span>
<span id="256"><span class="c1"># You may notice that this particular value of `C` may or may not do as well as simply running the default model on a random train-test split. </span></span>
<span id="257"><span class="c1"># </span></span>
<span id="258"><span class="c1"># * Do you think that&#39;s a problem? </span></span>
<span id="259"><span class="c1"># * Why do we need to do this whole cross-validation and grid search stuff anyway?</span></span>
<span id="260"></span>
<span id="261"><span class="c1"># **Answers to above**</span></span>
<span id="262"><span class="c1"># </span></span>
<span id="263"><span class="c1"># In this instance its not a problem as when C &gt;= .1 the cv_score does not change.</span></span>
<span id="264"><span class="c1"># </span></span>
<span id="265"><span class="c1"># To get the best fit for our model, making sure not to overfit.</span></span>
<span id="266"></span>
<span id="267"><span class="c1"># ### Use scikit-learn&#39;s GridSearchCV tool</span></span>
<span id="268"></span>
<span id="269"><span class="c1"># **Your turn (extra credit):** Use scikit-learn&#39;s [GridSearchCV](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) tool to perform cross validation and grid search. </span></span>
<span id="270"><span class="c1"># </span></span>
<span id="271"><span class="c1"># * Instead of writing your own loops above to iterate over the model parameters, can you use GridSearchCV to find the best model over the training set? </span></span>
<span id="272"><span class="c1"># * Does it give you the same best value of `C`?</span></span>
<span id="273"><span class="c1"># * How does this model you&#39;ve obtained perform on the test set?</span></span>
<span id="274"></span>
<span id="275"><span class="c1"># In[15]:</span></span>
<span id="276"></span>
<span id="277"></span>
<span id="278"><span class="c1">#your turn</span></span>
<span id="279"><span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span></span>
<span id="280"><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span></span>
<span id="281"></span>
<span id="282"><span class="n">tuned_parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="n">Cs</span><span class="p">}</span></span>
<span id="283"><span class="n">clf</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">SVC</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">),</span> <span class="n">tuned_parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span></span>
<span id="284"><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xlr</span><span class="p">,</span> <span class="n">ylr</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">train</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">no independent test data</button>
<span id="285"></span>
<span id="286"><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Best parameters set found on development set:&quot;</span><span class="p">)</span></span>
<span id="287"><span class="nb">print</span><span class="p">((</span><span class="n">clf</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span></span>
<span id="288"></span>
<span id="289"></span>
<span id="290"><span class="c1"># Yes it does give the same best value of C = 1. The model will perform exactly as above results on test data, the only thing this step has done is find the best hyperparameter C, which is the same as the results above.</span></span>
<span id="291"></span>
<span id="292"><span class="c1"># ## Recap of the math behind Logistic Regression (optional, feel free to skip)</span></span>
<span id="293"></span>
<span id="294"><span class="c1"># ### Setting up some code</span></span>
<span id="295"></span>
<span id="296"><span class="c1"># Lets make a small diversion, though, and set some code up for classification using cross-validation so that we can easily run classification models in scikit-learn. We first set up a function `cv_optimize` which takes a classifier `clf`, a grid of hyperparameters (such as a complexity parameter or regularization parameter as in the last ) implemented as a dictionary `parameters`, a training set (as a samples x features array) `Xtrain`, and a set of labels `ytrain`. The code takes the traning set, splits it into `n_folds` parts, sets up `n_folds` folds, and carries out a cross-validation by splitting the training set into a training and validation section for each foldfor us. It prints the best value of the parameters, and retuens the best classifier to us.</span></span>
<span id="297"></span>
<span id="298"><span class="c1"># In[16]:</span></span>
<span id="299"></span>
<span id="300"></span>
<span id="301"><span class="k">def</span> <span class="nf">cv_optimize</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">n_folds</span><span class="o">=</span><span class="mi">5</span><span class="p">):</span></span>
<span id="302">    <span class="n">gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">parameters</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">n_folds</span><span class="p">)</span></span>
<span id="303">    <span class="n">gs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">train</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">no independent test data</button>
<span id="304">    <span class="nb">print</span><span class="p">((</span><span class="s2">&quot;BEST PARAMS&quot;</span><span class="p">,</span> <span class="n">gs</span><span class="o">.</span><span class="n">best_params_</span><span class="p">))</span></span>
<span id="305">    <span class="n">best</span> <span class="o">=</span> <span class="n">gs</span><span class="o">.</span><span class="n">best_estimator_</span></span>
<span id="306">    <span class="k">return</span> <span class="n">best</span></span>
<span id="307"></span>
<span id="308"></span>
<span id="309"><span class="c1"># We then use this best classifier to fit the entire training set. This is done inside the `do_classify` function which takes a dataframe `indf` as input. It takes the columns in the list `featurenames` as the features used to train the classifier. The column `targetname` sets the target. The classification is done by setting those samples for which `targetname` has value `target1val` to the value 1, and all others to 0. We split the dataframe into 80% training and 20% testing by default, standardizing the dataset if desired. (Standardizing a data set involves scaling the data so that it has 0 mean and is described in units of its standard deviation. We then train the model on the training set using cross-validation. Having obtained the best classifier using `cv_optimize`, we retrain on the entire training set and calculate the training and testing accuracy, which we print. We return the split data and the trained classifier.</span></span>
<span id="310"></span>
<span id="311"><span class="c1"># In[17]:</span></span>
<span id="312"></span>
<span id="313"></span>
<span id="314"><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span></span>
<span id="315"><span class="k">def</span> <span class="nf">do_classify</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">indf</span><span class="p">,</span> <span class="n">featurenames</span><span class="p">,</span> <span class="n">targetname</span><span class="p">,</span> <span class="n">target1val</span><span class="p">,</span> <span class="n">standardize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="mf">0.8</span><span class="p">):</span></span>
<span id="316">    <span class="n">subdf</span><span class="o">=</span><span class="n">indf</span><span class="p">[</span><span class="n">featurenames</span><span class="p">]</span></span>
<span id="317">    <span class="k">if</span> <span class="n">standardize</span><span class="p">:</span></span>
<span id="318">        <span class="n">subdfstd</span><span class="o">=</span><span class="p">(</span><span class="n">subdf</span> <span class="o">-</span> <span class="n">subdf</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span><span class="o">/</span><span class="n">subdf</span><span class="o">.</span><span class="n">std</span><span class="p">()</span></span>
<span id="319">    <span class="k">else</span><span class="p">:</span></span>
<span id="320">        <span class="n">subdfstd</span><span class="o">=</span><span class="n">subdf</span></span>
<span id="321">    <span class="n">X</span><span class="o">=</span><span class="n">subdfstd</span><span class="o">.</span><span class="n">values</span></span>
<span id="322">    <span class="n">y</span><span class="o">=</span><span class="p">(</span><span class="n">indf</span><span class="p">[</span><span class="n">targetname</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="o">==</span><span class="n">target1val</span><span class="p">)</span><span class="o">*</span><span class="mi">1</span></span>
<span id="323">    <span class="n">Xtrain</span><span class="p">,</span> <span class="n">Xtest</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">ytest</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="n">train_size</span><span class="p">)</span></span>
<span id="324">    <span class="n">clf</span> <span class="o">=</span> <span class="n">cv_optimize</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span></span>
<span id="325">    <span class="n">clf</span><span class="o">=</span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">train</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([325, 326, 327, 488, 72, 73, 76, 81, 82, 99])">highlight train/test sites</button>
<span id="326">    <span class="n">training_accuracy</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([325, 326, 327, 488, 72, 73, 76, 81, 82, 99])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">overlap with training data</button>
<span id="327">    <span class="n">test_accuracy</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">Xtest</span><span class="p">,</span> <span class="n">ytest</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([325, 326, 327, 488, 72, 73, 76, 81, 82, 99])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">potential preprocessing leakage</button> <a href="#318"><button type="button" style="line-height: 85%; None" onclick="mark_leak_lines([318, 318])">show and go to first leak src</button></a>
<span id="328">    <span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Accuracy on training data: </span><span class="si">%0.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">training_accuracy</span><span class="p">)))</span></span>
<span id="329">    <span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Accuracy on test data:     </span><span class="si">%0.2f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">test_accuracy</span><span class="p">)))</span></span>
<span id="330">    <span class="k">return</span> <span class="n">clf</span><span class="p">,</span> <span class="n">Xtrain</span><span class="p">,</span> <span class="n">ytrain</span><span class="p">,</span> <span class="n">Xtest</span><span class="p">,</span> <span class="n">ytest</span></span>
<span id="331"></span>
<span id="332"></span>
<span id="333"><span class="c1"># ### Logistic Regression: The Math</span></span>
<span id="334"></span>
<span id="335"><span class="c1"># We could approach classification as linear regression, there the class, 0 or 1, is the target variable $y$. But this ignores the fact that our output $y$ is discrete valued, and futhermore, the $y$ predicted by linear regression will in general take on values less than 0 and greater than 1. Thus this does not seem like a very good idea.</span></span>
<span id="336"><span class="c1"># </span></span>
<span id="337"><span class="c1"># But what if we could change the form of our hypotheses $h(x)$ instead?</span></span>
<span id="338"><span class="c1"># </span></span>
<span id="339"><span class="c1"># The idea behind logistic regression is very simple. We want to draw a line in feature space that divides the &#39;1&#39; samples from the &#39;0&#39; samples, just like in the diagram above. In other words, we wish to find the &quot;regression&quot; line which divides the samples. Now, a line has the form $w_1 x_1 + w_2 x_2 + w_0 = 0$ in 2-dimensions. On one side of this line we have </span></span>
<span id="340"><span class="c1"># </span></span>
<span id="341"><span class="c1"># $$w_1 x_1 + w_2 x_2 + w_0 \ge 0,$$</span></span>
<span id="342"><span class="c1"># </span></span>
<span id="343"><span class="c1"># and on the other side we have </span></span>
<span id="344"><span class="c1"># </span></span>
<span id="345"><span class="c1"># $$w_1 x_1 + w_2 x_2 + w_0 &lt; 0.$$ </span></span>
<span id="346"><span class="c1"># </span></span>
<span id="347"><span class="c1"># Our classification rule then becomes:</span></span>
<span id="348"><span class="c1"># </span></span>
<span id="349"><span class="c1"># \begin{eqnarray*}</span></span>
<span id="350"><span class="c1"># y = 1 &amp;if&amp; \v{w}\cdot\v{x} \ge 0\\</span></span>
<span id="351"><span class="c1"># y = 0 &amp;if&amp; \v{w}\cdot\v{x} &lt; 0</span></span>
<span id="352"><span class="c1"># \end{eqnarray*}</span></span>
<span id="353"><span class="c1"># </span></span>
<span id="354"><span class="c1"># where $\v{x}$ is the vector $\{1,x_1, x_2,...,x_n\}$ where we have also generalized to more than 2 features.</span></span>
<span id="355"><span class="c1"># </span></span>
<span id="356"><span class="c1"># What hypotheses $h$ can we use to achieve this? One way to do so is to use the **sigmoid** function:</span></span>
<span id="357"><span class="c1"># </span></span>
<span id="358"><span class="c1"># $$h(z) = \frac{1}{1 + e^{-z}}.$$</span></span>
<span id="359"><span class="c1"># </span></span>
<span id="360"><span class="c1"># Notice that at $z=0$ this function has the value 0.5. If $z &gt; 0$, $h &gt; 0.5$ and as $z \to \infty$, $h \to 1$. If $z &lt; 0$, $h &lt; 0.5$ and as $z \to -\infty$, $h \to 0$. As long as we identify any value of $y &gt; 0.5$ as 1, and any $y &lt; 0.5$ as 0, we can achieve what we wished above.</span></span>
<span id="361"><span class="c1"># </span></span>
<span id="362"><span class="c1"># This function is plotted below:</span></span>
<span id="363"></span>
<span id="364"><span class="c1"># In[18]:</span></span>
<span id="365"></span>
<span id="366"></span>
<span id="367"><span class="n">h</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">z</span><span class="p">:</span> <span class="mf">1.</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">))</span></span>
<span id="368"><span class="n">zs</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mf">0.1</span><span class="p">)</span></span>
<span id="369"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">zs</span><span class="p">,</span> <span class="n">h</span><span class="p">(</span><span class="n">zs</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">);</span></span>
<span id="370"></span>
<span id="371"></span>
<span id="372"><span class="c1"># So we then come up with our rule by identifying:</span></span>
<span id="373"><span class="c1"># </span></span>
<span id="374"><span class="c1"># $$z = \v{w}\cdot\v{x}.$$</span></span>
<span id="375"><span class="c1"># </span></span>
<span id="376"><span class="c1"># Then $h(\v{w}\cdot\v{x}) \ge 0.5$ if $\v{w}\cdot\v{x} \ge 0$ and $h(\v{w}\cdot\v{x}) \lt 0.5$ if $\v{w}\cdot\v{x} \lt 0$, and:</span></span>
<span id="377"><span class="c1"># </span></span>
<span id="378"><span class="c1"># \begin{eqnarray*}</span></span>
<span id="379"><span class="c1"># y = 1 &amp;if&amp; h(\v{w}\cdot\v{x}) \ge 0.5\\</span></span>
<span id="380"><span class="c1"># y = 0 &amp;if&amp; h(\v{w}\cdot\v{x}) \lt 0.5.</span></span>
<span id="381"><span class="c1"># \end{eqnarray*}</span></span>
<span id="382"><span class="c1"># </span></span>
<span id="383"><span class="c1"># We will show soon that this identification can be achieved by minimizing a loss in the ERM framework called the **log loss** :</span></span>
<span id="384"><span class="c1"># </span></span>
<span id="385"><span class="c1"># $$ R_{\cal{D}}(\v{w}) = - \sum_{y_i \in \cal{D}} \left ( y_i log(h(\v{w}\cdot\v{x})) + ( 1 - y_i) log(1 - h(\v{w}\cdot\v{x})) \right )$$</span></span>
<span id="386"><span class="c1"># </span></span>
<span id="387"><span class="c1"># More generally we add a regularization term (as in the ridge regression):</span></span>
<span id="388"><span class="c1"># </span></span>
<span id="389"><span class="c1"># $$ R_{\cal{D}}(\v{w}) = - \sum_{y_i \in \cal{D}} \left ( y_i log(h(\v{w}\cdot\v{x})) + ( 1 - y_i) log(1 - h(\v{w}\cdot\v{x})) \right ) + \frac{1}{C} \v{w}\cdot\v{w},$$</span></span>
<span id="390"><span class="c1"># </span></span>
<span id="391"><span class="c1"># where $C$ is the regularization strength (corresponding to $1/\alpha$ from the Ridge case), and smaller values of $C$ mean stronger regularization. As before, the regularization tries to prevent features from having terribly high weights, thus implementing a form of feature selection. </span></span>
<span id="392"><span class="c1"># </span></span>
<span id="393"><span class="c1"># How did we come up with this loss? We&#39;ll come back to that, but let us see how logistic regression works out. </span></span>
<span id="394"><span class="c1"># </span></span>
<span id="395"></span>
<span id="396"><span class="c1"># In[19]:</span></span>
<span id="397"></span>
<span id="398"></span>
<span id="399"><span class="n">dflog</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></span>
<span id="400"></span>
<span id="401"></span>
<span id="402"><span class="c1"># In[20]:</span></span>
<span id="403"></span>
<span id="404"></span>
<span id="405"><span class="n">clf_l</span><span class="p">,</span> <span class="n">Xtrain_l</span><span class="p">,</span> <span class="n">ytrain_l</span><span class="p">,</span> <span class="n">Xtest_l</span><span class="p">,</span> <span class="n">ytest_l</span>  <span class="o">=</span> <span class="n">do_classify</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(),</span> </span>
<span id="406">                                                           <span class="p">{</span><span class="s2">&quot;C&quot;</span><span class="p">:</span> <span class="p">[</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">]},</span> </span>
<span id="407">                                                           <span class="n">dflog</span><span class="p">,</span> <span class="p">[</span><span class="s1">&#39;Weight&#39;</span><span class="p">,</span> <span class="s1">&#39;Height&#39;</span><span class="p">],</span> <span class="s1">&#39;Gender&#39;</span><span class="p">,</span><span class="s1">&#39;Male&#39;</span><span class="p">)</span></span>
<span id="408"></span>
<span id="409"></span>
<span id="410"><span class="c1"># In[21]:</span></span>
<span id="411"></span>
<span id="412"></span>
<span id="413"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span></span>
<span id="414"><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span></span>
<span id="415"><span class="n">points_plot</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">Xtrain_l</span><span class="p">,</span> <span class="n">Xtest_l</span><span class="p">,</span> <span class="n">ytrain_l</span><span class="p">,</span> <span class="n">ytest_l</span><span class="p">,</span> <span class="n">clf_l</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">);</span></span>
<span id="416"></span>
<span id="417"></span>
<span id="418"><span class="c1"># In the figure here showing the results of the logistic regression, we plot the actual labels of both the training(circles) and test(squares) samples. The 0&#39;s (females) are plotted in red, the 1&#39;s (males) in blue. We also show the classification boundary, a line (to the resolution of a grid square). Every sample on the red background side of the line will be classified female, and every sample on the blue side, male. Notice that most of the samples are classified well, but there are misclassified people on both sides, as evidenced by leakage of dots or squares of one color ontothe side of the other color. Both test and traing accuracy are about 92%.</span></span>
<span id="419"></span>
<span id="420"><span class="c1"># ### The probabilistic interpretaion</span></span>
<span id="421"></span>
<span id="422"><span class="c1"># Remember we said earlier that if $h &gt; 0.5$ we ought to identify the sample with $y=1$? One way of thinking about this is to identify $h(\v{w}\cdot\v{x})$ with the probability that the sample is a &#39;1&#39; ($y=1$). Then we have the intuitive notion that lets identify a sample as 1 if we find that the probabilty of being a &#39;1&#39; is $\ge 0.5$.</span></span>
<span id="423"><span class="c1"># </span></span>
<span id="424"><span class="c1"># So suppose we say then that the probability of $y=1$ for a given $\v{x}$ is given by $h(\v{w}\cdot\v{x})$?</span></span>
<span id="425"><span class="c1"># </span></span>
<span id="426"><span class="c1"># Then, the conditional probabilities of $y=1$ or $y=0$ given a particular sample&#39;s features $\v{x}$ are:</span></span>
<span id="427"><span class="c1"># </span></span>
<span id="428"><span class="c1"># \begin{eqnarray*}</span></span>
<span id="429"><span class="c1"># P(y=1 | \v{x}) &amp;=&amp; h(\v{w}\cdot\v{x}) \\</span></span>
<span id="430"><span class="c1"># P(y=0 | \v{x}) &amp;=&amp; 1 - h(\v{w}\cdot\v{x}).</span></span>
<span id="431"><span class="c1"># \end{eqnarray*}</span></span>
<span id="432"><span class="c1"># </span></span>
<span id="433"><span class="c1"># These two can be written together as</span></span>
<span id="434"><span class="c1"># </span></span>
<span id="435"><span class="c1"># $$P(y|\v{x}, \v{w}) = h(\v{w}\cdot\v{x})^y \left(1 - h(\v{w}\cdot\v{x}) \right)^{(1-y)} $$</span></span>
<span id="436"><span class="c1"># </span></span>
<span id="437"><span class="c1"># Then multiplying over the samples we get the probability of the training $y$ given $\v{w}$ and the $\v{x}$:</span></span>
<span id="438"><span class="c1"># </span></span>
<span id="439"><span class="c1"># $$P(y|\v{x},\v{w}) = P(\{y_i\} | \{\v{x}_i\}, \v{w}) = \prod_{y_i \in \cal{D}} P(y_i|\v{x_i}, \v{w}) = \prod_{y_i \in \cal{D}} h(\v{w}\cdot\v{x_i})^{y_i} \left(1 - h(\v{w}\cdot\v{x_i}) \right)^{(1-y_i)}$$</span></span>
<span id="440"><span class="c1"># </span></span>
<span id="441"><span class="c1"># Why use probabilities? Earlier, we talked about how the regression function $f(x)$ never gives us the $y$ exactly, because of noise. This hold for classification too. Even with identical features, a different sample may be classified differently. </span></span>
<span id="442"><span class="c1"># </span></span>
<span id="443"><span class="c1"># We said that another way to think about a noisy $y$ is to imagine that our data $\dat$ was generated from  a joint probability distribution $P(x,y)$. Thus we need to model $y$ at a given $x$, written as $P(y|x)$, and since $P(x)$ is also a probability distribution, we have:</span></span>
<span id="444"><span class="c1"># </span></span>
<span id="445"><span class="c1"># $$P(x,y) = P(y | x) P(x) ,$$</span></span>
<span id="446"><span class="c1"># </span></span>
<span id="447"><span class="c1"># and can obtain our joint probability ($P(x, y))$.</span></span>
<span id="448"><span class="c1"># </span></span>
<span id="449"><span class="c1"># Indeed its important to realize that a particular training set can be thought of as a draw from some &quot;true&quot; probability distribution (just as we did when showing the hairy variance diagram). If for example the probability of classifying a test sample as a &#39;0&#39; was 0.1, and it turns out that the test sample was a &#39;0&#39;, it does not mean that this model was necessarily wrong. After all, in roughly a 10th of the draws, this new sample would be classified as a &#39;0&#39;! But, of-course its more unlikely than its likely, and having good probabilities means that we&#39;ll be likely right most of the time, which is what we want to achieve in classification. And furthermore, we can quantify this accuracy.</span></span>
<span id="450"><span class="c1"># </span></span>
<span id="451"><span class="c1"># Thus its desirable to have probabilistic, or at the very least, ranked models of classification where you can tell which sample is more likely to be classified as a &#39;1&#39;. There are business reasons for this too. Consider the example of customer &quot;churn&quot;: you are a cell-phone company and want to know, based on some of my purchasing habit and characteristic &quot;features&quot; if I am a likely defector. If so, you&#39;ll offer me an incentive not to defect. In this scenario, you might want to know which customers are most likely to defect, or even more precisely, which are most likely to respond to incentives. Based on these probabilities, you could then spend a finite marketing budget wisely.</span></span>
<span id="452"></span>
<span id="453"><span class="c1"># ### Maximizing the probability of the training set.</span></span>
<span id="454"></span>
<span id="455"><span class="c1"># Now if we maximize $$P(y|\v{x},\v{w})$$, we will maximize the chance that each point is classified correctly, which is what we want to do. While this is not exactly the same thing as maximizing the 1-0 training risk, it is a principled way of obtaining the highest probability classification. This process is called **maximum likelihood** estimation since we are maximising the **likelihood of the training data y**, </span></span>
<span id="456"><span class="c1"># </span></span>
<span id="457"><span class="c1"># $$\like = P(y|\v{x},\v{w}).$$ </span></span>
<span id="458"><span class="c1"># </span></span>
<span id="459"><span class="c1"># Maximum likelihood is one of the corenerstone methods in statistics, and is used to estimate probabilities of data. </span></span>
<span id="460"><span class="c1"># </span></span>
<span id="461"><span class="c1"># We can equivalently maximize </span></span>
<span id="462"><span class="c1"># </span></span>
<span id="463"><span class="c1"># $$\loglike = log(P(y|\v{x},\v{w}))$$ </span></span>
<span id="464"><span class="c1"># </span></span>
<span id="465"><span class="c1"># since the natural logarithm $log$ is a monotonic function. This is known as maximizing the **log-likelihood**. Thus we can equivalently *minimize* a risk that is the negative of  $log(P(y|\v{x},\v{w}))$:</span></span>
<span id="466"><span class="c1"># </span></span>
<span id="467"><span class="c1"># $$R_{\cal{D}}(h(x)) = -\loglike = -log \like = - log(P(y|\v{x},\v{w})).$$</span></span>
<span id="468"><span class="c1"># </span></span>
<span id="469"><span class="c1"># </span></span>
<span id="470"><span class="c1"># Thus</span></span>
<span id="471"><span class="c1"># </span></span>
<span id="472"><span class="c1"># \begin{eqnarray*}</span></span>
<span id="473"><span class="c1"># R_{\cal{D}}(h(x)) &amp;=&amp; -log\left(\prod_{y_i \in \cal{D}} h(\v{w}\cdot\v{x_i})^{y_i} \left(1 - h(\v{w}\cdot\v{x_i}) \right)^{(1-y_i)}\right)\\</span></span>
<span id="474"><span class="c1">#                   &amp;=&amp; -\sum_{y_i \in \cal{D}} log\left(h(\v{w}\cdot\v{x_i})^{y_i} \left(1 - h(\v{w}\cdot\v{x_i}) \right)^{(1-y_i)}\right)\\                  </span></span>
<span id="475"><span class="c1">#                   &amp;=&amp; -\sum_{y_i \in \cal{D}} log\,h(\v{w}\cdot\v{x_i})^{y_i} + log\,\left(1 - h(\v{w}\cdot\v{x_i}) \right)^{(1-y_i)}\\</span></span>
<span id="476"><span class="c1">#                   &amp;=&amp; - \sum_{y_i \in \cal{D}} \left ( y_i log(h(\v{w}\cdot\v{x})) + ( 1 - y_i) log(1 - h(\v{w}\cdot\v{x})) \right )</span></span>
<span id="477"><span class="c1"># \end{eqnarray*}</span></span>
<span id="478"><span class="c1">#                   </span></span>
<span id="479"><span class="c1"># This is exactly the risk we had above, leaving out the regularization term (which we shall return to later) and was the reason we chose it over the 1-0 risk. </span></span>
<span id="480"><span class="c1"># </span></span>
<span id="481"><span class="c1"># Notice that this little process we carried out above tells us something very interesting: **Probabilistic estimation using maximum likelihood is equivalent to Empiricial Risk Minimization using the negative log-likelihood**, since all we did was to minimize the negative log-likelihood over the training samples.</span></span>
<span id="482"><span class="c1"># </span></span>
<span id="483"><span class="c1"># `sklearn` will return the probabilities for our samples, or for that matter, for any input vector set $\{\v{x}_i\}$, i.e. $P(y_i | \v{x}_i, \v{w})$:</span></span>
<span id="484"></span>
<span id="485"><span class="c1"># In[22]:</span></span>
<span id="486"></span>
<span id="487"></span>
<span id="488"><span class="n">clf_l</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">Xtest_l</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([325, 326, 327, 488, 72, 73, 76, 81, 82, 99])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">potential preprocessing leakage</button> <a href="#318"><button type="button" style="line-height: 85%; None" onclick="mark_leak_lines([318, 318])">show and go to first leak src</button></a>
<span id="489"></span>
<span id="490"></span>
<span id="491"><span class="c1"># ###Discriminative classifier</span></span>
<span id="492"></span>
<span id="493"><span class="c1"># Logistic regression is what is known as a **discriminative classifier**. Let us plot the probabilities obtained from `predict_proba`, overlayed on the samples with their true labels:</span></span>
<span id="494"></span>
<span id="495"><span class="c1"># In[23]:</span></span>
<span id="496"></span>
<span id="497"></span>
<span id="498"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span></span>
<span id="499"><span class="n">ax</span><span class="o">=</span><span class="n">plt</span><span class="o">.</span><span class="n">gca</span><span class="p">()</span></span>
<span id="500"><span class="n">points_plot_prob</span><span class="p">(</span><span class="n">ax</span><span class="p">,</span> <span class="n">Xtrain_l</span><span class="p">,</span> <span class="n">Xtest_l</span><span class="p">,</span> <span class="n">ytrain_l</span><span class="p">,</span> <span class="n">ytest_l</span><span class="p">,</span> <span class="n">clf_l</span><span class="p">,</span> <span class="n">psize</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">);</span></span>
<span id="501"></span>
<span id="502"></span>
<span id="503"><span class="c1"># Notice that lines of equal probability, as might be expected are stright lines. What the classifier does is very intuitive: if the probability is greater than 0.5, it classifies the sample as type &#39;1&#39; (male), otherwise it classifies the sample to be class &#39;0&#39;. Thus in the diagram above, where we have plotted predicted values rather than actual labels of samples, there is a clear demarcation at the 0.5 probability line.</span></span>
<span id="504"><span class="c1"># </span></span>
<span id="505"><span class="c1"># This notion of trying to obtain the line or boundary of demarcation is what is called a **discriminative** classifier. The algorithm tries to find a decision boundary that separates the males from the females. To classify a new sample as male or female, it checks on which side of the decision boundary the sample falls, and makes a prediction. In other words we are asking, given $\v{x}$, what is the probability of a given $y$, or, what is the likelihood $P(y|\v{x},\v{w})$?</span></span>
</pre></div>
</td></tr></table></body>
</html>
