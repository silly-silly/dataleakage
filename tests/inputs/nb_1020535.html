<script>
    let highlighted = [];
    function highlight_lines(lines) {
        for (let line of highlighted) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = '';
        }
        highlighted = lines;
        for (let line of highlighted) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = 'yellow';
        }
    }
    let marked = [];
    function mark_leak_lines(lines) {
        for (let line of marked) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = '';
        }
        marked = lines;
        for (let line of marked) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = ele.style.backgroundColor = 'lightgreen';
        }
    }
    function show_infos(lines) {
        for (let line of lines) {
            let ele = document.getElementById(String(line) + "-info");
            if (ele) {
                ele.style.display = ele.style.display == 'none'? '': 'none'
            }
        }
    }
</script>
    <style type="text/css">
    .sum table {
    font-family: arial, sans-serif;
    border-collapse: collapse;
    width: 100%;
    }

    .sum td, .sum th {
    border: 1px solid #dddddd;
    text-align: left;
    padding: 8px;
    }

    .sum tr:hover {background-color: #D6EEEE;}
</style>
<center>
<table class="sum">
  <tbody><tr>
    <th>Leakage</th>
    <th>#Detected</th>
    <th>Locations</th>
  </tr>
  <tr>
    <td>Pre-processing leakage</td>
    <td>0</td>
    <td></td>
  </tr>
  <tr>
    <td>Overlap leakage</td>
    <td>0</td>
    <td></td>
  </tr>
  <tr>
    <td>No independence test data</td>
    <td>0</td>
    <td></td>
  </tr>
</tbody></table></center>

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<!--
generated by Pygments <https://pygments.org/>
Copyright 2006-2021 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
-->
<html>
<head>
  <title></title>
  <meta http-equiv="content-type" content="text/html; charset=None">
  <style type="text/css">
/*
generated by Pygments <https://pygments.org/>
Copyright 2006-2021 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
pre { line-height: 145%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
body .hll { background-color: #ffffcc }
body { background: #f8f8f8; }
body .c { color: #408080; font-style: italic } /* Comment */
body .err { border: 1px solid #FF0000 } /* Error */
body .k { color: #008000; font-weight: bold } /* Keyword */
body .o { color: #666666 } /* Operator */
body .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
body .cm { color: #408080; font-style: italic } /* Comment.Multiline */
body .cp { color: #BC7A00 } /* Comment.Preproc */
body .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
body .c1 { color: #408080; font-style: italic } /* Comment.Single */
body .cs { color: #408080; font-style: italic } /* Comment.Special */
body .gd { color: #A00000 } /* Generic.Deleted */
body .ge { font-style: italic } /* Generic.Emph */
body .gr { color: #FF0000 } /* Generic.Error */
body .gh { color: #000080; font-weight: bold } /* Generic.Heading */
body .gi { color: #00A000 } /* Generic.Inserted */
body .go { color: #888888 } /* Generic.Output */
body .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
body .gs { font-weight: bold } /* Generic.Strong */
body .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
body .gt { color: #0044DD } /* Generic.Traceback */
body .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
body .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
body .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
body .kp { color: #008000 } /* Keyword.Pseudo */
body .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
body .kt { color: #B00040 } /* Keyword.Type */
body .m { color: #666666 } /* Literal.Number */
body .s { color: #BA2121 } /* Literal.String */
body .na { color: #7D9029 } /* Name.Attribute */
body .nb { color: #008000 } /* Name.Builtin */
body .nc { color: #0000FF; font-weight: bold } /* Name.Class */
body .no { color: #880000 } /* Name.Constant */
body .nd { color: #AA22FF } /* Name.Decorator */
body .ni { color: #999999; font-weight: bold } /* Name.Entity */
body .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
body .nf { color: #0000FF } /* Name.Function */
body .nl { color: #A0A000 } /* Name.Label */
body .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
body .nt { color: #008000; font-weight: bold } /* Name.Tag */
body .nv { color: #19177C } /* Name.Variable */
body .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
body .w { color: #bbbbbb } /* Text.Whitespace */
body .mb { color: #666666 } /* Literal.Number.Bin */
body .mf { color: #666666 } /* Literal.Number.Float */
body .mh { color: #666666 } /* Literal.Number.Hex */
body .mi { color: #666666 } /* Literal.Number.Integer */
body .mo { color: #666666 } /* Literal.Number.Oct */
body .sa { color: #BA2121 } /* Literal.String.Affix */
body .sb { color: #BA2121 } /* Literal.String.Backtick */
body .sc { color: #BA2121 } /* Literal.String.Char */
body .dl { color: #BA2121 } /* Literal.String.Delimiter */
body .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
body .s2 { color: #BA2121 } /* Literal.String.Double */
body .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
body .sh { color: #BA2121 } /* Literal.String.Heredoc */
body .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
body .sx { color: #008000 } /* Literal.String.Other */
body .sr { color: #BB6688 } /* Literal.String.Regex */
body .s1 { color: #BA2121 } /* Literal.String.Single */
body .ss { color: #19177C } /* Literal.String.Symbol */
body .bp { color: #008000 } /* Name.Builtin.Pseudo */
body .fm { color: #0000FF } /* Name.Function.Magic */
body .vc { color: #19177C } /* Name.Variable.Class */
body .vg { color: #19177C } /* Name.Variable.Global */
body .vi { color: #19177C } /* Name.Variable.Instance */
body .vm { color: #19177C } /* Name.Variable.Magic */
body .il { color: #666666 } /* Literal.Number.Integer.Long */

  </style>
</head>
<body>
<h2></h2>

<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span>
<span class="normal">713</span>
<span class="normal">714</span>
<span class="normal">715</span>
<span class="normal">716</span>
<span class="normal">717</span>
<span class="normal">718</span>
<span class="normal">719</span>
<span class="normal">720</span>
<span class="normal">721</span>
<span class="normal">722</span>
<span class="normal">723</span>
<span class="normal">724</span>
<span class="normal">725</span>
<span class="normal">726</span>
<span class="normal">727</span>
<span class="normal">728</span>
<span class="normal">729</span>
<span class="normal">730</span>
<span class="normal">731</span>
<span class="normal">732</span>
<span class="normal">733</span>
<span class="normal">734</span>
<span class="normal">735</span>
<span class="normal">736</span>
<span class="normal">737</span>
<span class="normal">738</span>
<span class="normal">739</span>
<span class="normal">740</span>
<span class="normal">741</span>
<span class="normal">742</span>
<span class="normal">743</span>
<span class="normal">744</span>
<span class="normal">745</span>
<span class="normal">746</span>
<span class="normal">747</span>
<span class="normal">748</span>
<span class="normal">749</span>
<span class="normal">750</span>
<span class="normal">751</span>
<span class="normal">752</span>
<span class="normal">753</span>
<span class="normal">754</span>
<span class="normal">755</span>
<span class="normal">756</span>
<span class="normal">757</span>
<span class="normal">758</span>
<span class="normal">759</span>
<span class="normal">760</span>
<span class="normal">761</span>
<span class="normal">762</span>
<span class="normal">763</span>
<span class="normal">764</span>
<span class="normal">765</span>
<span class="normal">766</span>
<span class="normal">767</span>
<span class="normal">768</span>
<span class="normal">769</span>
<span class="normal">770</span>
<span class="normal">771</span>
<span class="normal">772</span>
<span class="normal">773</span>
<span class="normal">774</span>
<span class="normal">775</span>
<span class="normal">776</span>
<span class="normal">777</span>
<span class="normal">778</span>
<span class="normal">779</span>
<span class="normal">780</span>
<span class="normal">781</span>
<span class="normal">782</span>
<span class="normal">783</span>
<span class="normal">784</span>
<span class="normal">785</span>
<span class="normal">786</span>
<span class="normal">787</span>
<span class="normal">788</span>
<span class="normal">789</span>
<span class="normal">790</span>
<span class="normal">791</span>
<span class="normal">792</span>
<span class="normal">793</span>
<span class="normal">794</span>
<span class="normal">795</span>
<span class="normal">796</span>
<span class="normal">797</span>
<span class="normal">798</span>
<span class="normal">799</span>
<span class="normal">800</span>
<span class="normal">801</span>
<span class="normal">802</span>
<span class="normal">803</span>
<span class="normal">804</span>
<span class="normal">805</span>
<span class="normal">806</span>
<span class="normal">807</span>
<span class="normal">808</span>
<span class="normal">809</span>
<span class="normal">810</span>
<span class="normal">811</span>
<span class="normal">812</span>
<span class="normal">813</span>
<span class="normal">814</span>
<span class="normal">815</span>
<span class="normal">816</span>
<span class="normal">817</span>
<span class="normal">818</span>
<span class="normal">819</span>
<span class="normal">820</span>
<span class="normal">821</span>
<span class="normal">822</span>
<span class="normal">823</span>
<span class="normal">824</span>
<span class="normal">825</span>
<span class="normal">826</span>
<span class="normal">827</span>
<span class="normal">828</span>
<span class="normal">829</span>
<span class="normal">830</span>
<span class="normal">831</span>
<span class="normal">832</span>
<span class="normal">833</span>
<span class="normal">834</span>
<span class="normal">835</span>
<span class="normal">836</span>
<span class="normal">837</span>
<span class="normal">838</span>
<span class="normal">839</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span id="2"><span class="c1"># coding: utf-8</span></span>
<span id="3"></span>
<span id="4"><span class="c1"># ## Our Mission ##</span></span>
<span id="5"><span class="c1"># </span></span>
<span id="6"><span class="c1"># Spam detection is one of the major applications of Machine Learning in the interwebs today. Pretty much all of the major email service providers have spam detection systems built in and automatically classify such mail as &#39;Junk Mail&#39;. </span></span>
<span id="7"><span class="c1"># </span></span>
<span id="8"><span class="c1"># In this mission we will be using the Naive Bayes algorithm to create a model that can classify [dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) SMS messages as spam or not spam, based on the training we give to the model. It is important to have some level of intuition as to what a spammy text message might look like. Usually they have words like &#39;free&#39;, &#39;win&#39;, &#39;winner&#39;, &#39;cash&#39;, &#39;prize&#39; and the like in them as these texts are designed to catch your eye and in some sense tempt you to open them. Also, spam messages tend to have words written in all capitals and also tend to use a lot of exclamation marks. To the recipient, it is usually pretty straightforward to identify a spam text and our objective here is to train a model to do that for us!</span></span>
<span id="9"><span class="c1"># </span></span>
<span id="10"><span class="c1"># Being able to identify spam messages is a binary classification problem as messages are classified as either &#39;Spam&#39; or &#39;Not Spam&#39; and nothing else. Also, this is a supervised learning problem, as we will be feeding a labelled dataset into the model, that it can learn from, to make future predictions. </span></span>
<span id="11"></span>
<span id="12"><span class="c1"># ### Step 0: Introduction to the Naive Bayes Theorem ###</span></span>
<span id="13"><span class="c1"># </span></span>
<span id="14"><span class="c1"># Bayes theorem is one of the earliest probabilistic inference algorithms developed by Reverend Bayes (which he used to try and infer the existence of God no less) and still performs extremely well for certain use cases. </span></span>
<span id="15"><span class="c1"># </span></span>
<span id="16"><span class="c1"># It&#39;s best to understand this theorem using an example. Let&#39;s say you are a member of the Secret Service and you have been deployed to protect the Democratic presidential nominee during one of his/her campaign speeches. Being a public event that is open to all, your job is not easy and you have to be on the constant lookout for threats. So one place to start is to put a certain threat-factor for each person. So based on the features of an individual, like the age, sex, and other smaller factors like is the person carrying a bag?, does the person look nervous? etc. you can make a judgement call as to if that person is viable threat. </span></span>
<span id="17"><span class="c1"># </span></span>
<span id="18"><span class="c1"># If an individual ticks all the boxes up to a level where it crosses a threshold of doubt in your mind, you can take action and remove that person from the vicinity. The Bayes theorem works in the same way as we are computing the probability of an event(a person being a threat) based on the probabilities of certain related events(age, sex, presence of bag or not, nervousness etc. of the person). </span></span>
<span id="19"><span class="c1"># </span></span>
<span id="20"><span class="c1"># One thing to consider is the independence of these features amongst each other. For example if a child looks nervous at the event then the likelihood of that person being a threat is not as much as say if it was a grown man who was nervous. To break this down a bit further, here there are two features we are considering, age AND nervousness. Say we look at these features individually, we could design a model that flags ALL persons that are nervous as potential threats. However, it is likely that we will have a lot of false positives as there is a strong chance that minors present at the event will be nervous. Hence by considering the age of a person along with the &#39;nervousness&#39; feature we would definitely get a more accurate result as to who are potential threats and who aren&#39;t. </span></span>
<span id="21"><span class="c1"># </span></span>
<span id="22"><span class="c1"># This is the &#39;Naive&#39; bit of the theorem where it considers each feature to be independant of each other which may not always be the case and hence that can affect the final judgement.</span></span>
<span id="23"><span class="c1"># </span></span>
<span id="24"><span class="c1"># In short, the Bayes theorem calculates the probability of a certain event happening(in our case, a message being  spam) based on the joint probabilistic distributions of certain other events(in our case, a message being classified as spam). We will dive into the workings of the Bayes theorem later in the mission, but first, let us understand the data we are going to work with.</span></span>
<span id="25"></span>
<span id="26"><span class="c1"># ### Step 1.1: Understanding our dataset ### </span></span>
<span id="27"><span class="c1"># </span></span>
<span id="28"><span class="c1"># </span></span>
<span id="29"><span class="c1"># We will be using a [dataset](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) from the UCI Machine Learning repository which has a very good collection of datasets for experimental research purposes. The direct data link is [here](https://archive.ics.uci.edu/ml/machine-learning-databases/00228/).</span></span>
<span id="30"><span class="c1"># </span></span>
<span id="31"><span class="c1"># </span></span>
<span id="32"><span class="c1">#  ** Here&#39;s a preview of the data: ** </span></span>
<span id="33"><span class="c1"># </span></span>
<span id="34"><span class="c1"># &lt;img src=&quot;images/dqnb.png&quot; height=&quot;1242&quot; width=&quot;1242&quot;&gt;</span></span>
<span id="35"><span class="c1"># </span></span>
<span id="36"><span class="c1"># The columns in the data set are currently not named and as you can see, there are 2 columns. </span></span>
<span id="37"><span class="c1"># </span></span>
<span id="38"><span class="c1"># The first column takes two values, &#39;ham&#39; which signifies that the message is not spam, and &#39;spam&#39; which signifies that the message is spam. </span></span>
<span id="39"><span class="c1"># </span></span>
<span id="40"><span class="c1"># The second column is the text content of the SMS message that is being classified.</span></span>
<span id="41"></span>
<span id="42"><span class="c1"># &gt;** Instructions: **</span></span>
<span id="43"><span class="c1"># * Import the dataset into a pandas dataframe using the read_table method. Because this is a tab separated dataset we will be using &#39;\t&#39; as the value for the &#39;sep&#39; argument which specifies this format. </span></span>
<span id="44"><span class="c1"># * Also, rename the column names by specifying a list [&#39;label, &#39;sms_message&#39;] to the &#39;names&#39; argument of read_table().</span></span>
<span id="45"><span class="c1"># * Print the first five values of the dataframe with the new column names.</span></span>
<span id="46"></span>
<span id="47"><span class="c1"># In[1]:</span></span>
<span id="48"></span>
<span id="49"></span>
<span id="50"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="51"><span class="sd">Solution</span></span>
<span id="52"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="53"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span></span>
<span id="54"><span class="c1"># Dataset from - https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection</span></span>
<span id="55"><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_table</span><span class="p">(</span><span class="s1">&#39;smsspamcollection/SMSSpamCollection&#39;</span><span class="p">,</span></span>
<span id="56">                   <span class="n">sep</span><span class="o">=</span><span class="s1">&#39;</span><span class="se">\t</span><span class="s1">&#39;</span><span class="p">,</span> </span>
<span id="57">                   <span class="n">header</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> </span>
<span id="58">                   <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;sms_message&#39;</span><span class="p">])</span></span>
<span id="59"></span>
<span id="60"><span class="c1"># Output printing out first 5 columns</span></span>
<span id="61"><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></span>
<span id="62"></span>
<span id="63"></span>
<span id="64"><span class="c1"># ### Step 1.2: Data Preprocessing ###</span></span>
<span id="65"><span class="c1"># </span></span>
<span id="66"><span class="c1"># Now that we have a basic understanding of what our dataset looks like, lets convert our labels to binary variables, 0 to represent &#39;ham&#39;(i.e. not spam) and 1 to represent &#39;spam&#39; for ease of computation. </span></span>
<span id="67"><span class="c1"># </span></span>
<span id="68"><span class="c1"># You might be wondering why do we need to do this step? The answer to this lies in how scikit-learn handles inputs. Scikit-learn only deals with numerical values and hence if we were to leave our label values as strings, scikit-learn would do the conversion internally(more specifically, the string labels will be cast to unknown float values). </span></span>
<span id="69"><span class="c1"># </span></span>
<span id="70"><span class="c1"># Our model would still be able to make predictions if we left our labels as strings but we could have issues later when calculating performance metrics, for example when calculating our precision and recall scores. Hence, to avoid unexpected &#39;gotchas&#39; later, it is good practice to have our categorical values be fed into our model as integers. </span></span>
<span id="71"></span>
<span id="72"><span class="c1"># &gt;**Instructions: **</span></span>
<span id="73"><span class="c1"># * Convert the values in the &#39;label&#39; colum to numerical values using map method as follows:</span></span>
<span id="74"><span class="c1"># {&#39;ham&#39;:0, &#39;spam&#39;:1} This maps the &#39;ham&#39; value to 0 and the &#39;spam&#39; value to 1.</span></span>
<span id="75"><span class="c1"># * Also, to get an idea of the size of the dataset we are dealing with, print out number of rows and columns using </span></span>
<span id="76"><span class="c1"># &#39;shape&#39;.</span></span>
<span id="77"></span>
<span id="78"><span class="c1"># In[2]:</span></span>
<span id="79"></span>
<span id="80"></span>
<span id="81"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="82"><span class="sd">Solution</span></span>
<span id="83"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="84"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">map</span><span class="p">({</span><span class="s1">&#39;ham&#39;</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;spam&#39;</span><span class="p">:</span><span class="mi">1</span><span class="p">})</span></span>
<span id="85"><span class="nb">print</span><span class="p">((</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span></span>
<span id="86"><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span> <span class="c1"># returns (rows, columns)</span></span>
<span id="87"></span>
<span id="88"></span>
<span id="89"><span class="c1"># ### Step 2.1: Bag of words ###</span></span>
<span id="90"><span class="c1"># </span></span>
<span id="91"><span class="c1"># What we have here in our data set is a large collection of text data (5,572 rows of data). Most ML algorithms rely on numerical data to be fed into them as input, and email/sms messages are usually text heavy. </span></span>
<span id="92"><span class="c1"># </span></span>
<span id="93"><span class="c1"># Here we&#39;d like to introduce the Bag of Words(BoW) concept which is a term used to specify the problems that have a &#39;bag of words&#39; or a collection of text data that needs to be worked with. The basic idea of BoW is to take a piece of text and count the frequency of the words in that text. It is important to note that the BoW concept treats each word individually and the order in which the words occur does not matter. </span></span>
<span id="94"><span class="c1"># </span></span>
<span id="95"><span class="c1"># Using a process which we will go through now, we can covert a collection of documents to a matrix, with each document being a row and each word(token) being the column, and the corresponding (row,column) values being the frequency of occurrance of each word or token in that document.</span></span>
<span id="96"><span class="c1"># </span></span>
<span id="97"><span class="c1"># For example: </span></span>
<span id="98"><span class="c1"># </span></span>
<span id="99"><span class="c1"># Lets say we have 4 documents as follows:</span></span>
<span id="100"><span class="c1"># </span></span>
<span id="101"><span class="c1"># `[&#39;Hello, how are you!&#39;,</span></span>
<span id="102"><span class="c1"># &#39;Win money, win from home.&#39;,</span></span>
<span id="103"><span class="c1"># &#39;Call me now&#39;,</span></span>
<span id="104"><span class="c1"># &#39;Hello, Call you tomorrow?&#39;]`</span></span>
<span id="105"><span class="c1"># </span></span>
<span id="106"><span class="c1"># Our objective here is to convert this set of text to a frequency distribution matrix, as follows:</span></span>
<span id="107"><span class="c1"># </span></span>
<span id="108"><span class="c1"># &lt;img src=&quot;images/countvectorizer.png&quot; height=&quot;542&quot; width=&quot;542&quot;&gt;</span></span>
<span id="109"><span class="c1"># </span></span>
<span id="110"><span class="c1"># Here as we can see, the documents are numbered in the rows, and each word is a column name, with the corresponding value being the frequency of that word in the document.</span></span>
<span id="111"><span class="c1"># </span></span>
<span id="112"><span class="c1"># Lets break this down and see how we can do this conversion using a small set of documents.</span></span>
<span id="113"><span class="c1"># </span></span>
<span id="114"><span class="c1"># To handle this, we will be using sklearns </span></span>
<span id="115"><span class="c1"># [count vectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer) method which does the following:</span></span>
<span id="116"><span class="c1"># </span></span>
<span id="117"><span class="c1"># * It tokenizes the string(separates the string into individual words) and gives an integer ID to each token.</span></span>
<span id="118"><span class="c1"># * It counts the occurrance of each of those tokens.</span></span>
<span id="119"><span class="c1"># </span></span>
<span id="120"><span class="c1"># ** Please Note: ** </span></span>
<span id="121"><span class="c1"># </span></span>
<span id="122"><span class="c1"># * The CountVectorizer method automatically converts all tokenized words to their lower case form so that it does not treat words like &#39;He&#39; and &#39;he&#39; differently. It does this using the `lowercase` parameter which is by default set to `True`.</span></span>
<span id="123"><span class="c1"># </span></span>
<span id="124"><span class="c1"># * It also ignores all punctuation so that words followed by a punctuation mark (for example: &#39;hello!&#39;) are not treated differently than the same words not prefixed or suffixed by a punctuation mark (for example: &#39;hello&#39;). It does this using the `token_pattern` parameter which has a default regular expression which selects tokens of 2 or more alphanumeric characters.</span></span>
<span id="125"><span class="c1"># </span></span>
<span id="126"><span class="c1"># * The third parameter to take note of is the `stop_words` parameter. Stop words refer to the most commonly used words in a language. They include words like &#39;am&#39;, &#39;an&#39;, &#39;and&#39;, &#39;the&#39; etc. By setting this parameter value to `english`, CountVectorizer will automatically ignore all words(from our input text) that are found in the built in list of english stop words in scikit-learn. This is extremely helpful as stop words can skew our calculations when we are trying to find certain key words that are indicative of spam.</span></span>
<span id="127"><span class="c1"># </span></span>
<span id="128"><span class="c1"># We will dive into the application of each of these into our model in a later step, but for now it is important to be aware of such preprocessing techniques available to us when dealing with textual data.</span></span>
<span id="129"></span>
<span id="130"><span class="c1"># ### Step 2.2: Implementing Bag of Words from scratch ###</span></span>
<span id="131"><span class="c1"># </span></span>
<span id="132"><span class="c1"># Before we dive into scikit-learn&#39;s Bag of Words(BoW) library to do the dirty work for us, let&#39;s implement it ourselves first so that we can understand what&#39;s happening behind the scenes. </span></span>
<span id="133"><span class="c1"># </span></span>
<span id="134"><span class="c1"># ** Step 1: Convert all strings to their lower case form. **</span></span>
<span id="135"><span class="c1"># </span></span>
<span id="136"><span class="c1"># Let&#39;s say we have a document set:</span></span>
<span id="137"><span class="c1"># </span></span>
<span id="138"><span class="c1"># ```</span></span>
<span id="139"><span class="c1"># documents = [&#39;Hello, how are you!&#39;,</span></span>
<span id="140"><span class="c1">#              &#39;Win money, win from home.&#39;,</span></span>
<span id="141"><span class="c1">#              &#39;Call me now.&#39;,</span></span>
<span id="142"><span class="c1">#              &#39;Hello, Call hello you tomorrow?&#39;]</span></span>
<span id="143"><span class="c1"># ```</span></span>
<span id="144"><span class="c1"># &gt;&gt;** Instructions: **</span></span>
<span id="145"><span class="c1"># * Convert all the strings in the documents set to their lower case. Save them into a list called &#39;lower_case_documents&#39;. You can convert strings to their lower case in python by using the lower() method.</span></span>
<span id="146"><span class="c1"># </span></span>
<span id="147"></span>
<span id="148"><span class="c1"># In[3]:</span></span>
<span id="149"></span>
<span id="150"></span>
<span id="151"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="152"><span class="sd">Solution:</span></span>
<span id="153"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="154"><span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Hello, how are you!&#39;</span><span class="p">,</span></span>
<span id="155">             <span class="s1">&#39;Win money, win from home.&#39;</span><span class="p">,</span></span>
<span id="156">             <span class="s1">&#39;Call me now.&#39;</span><span class="p">,</span></span>
<span id="157">             <span class="s1">&#39;Hello, Call hello you tomorrow?&#39;</span><span class="p">]</span></span>
<span id="158"></span>
<span id="159"><span class="n">lower_case_documents</span> <span class="o">=</span> <span class="p">[]</span></span>
<span id="160"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">documents</span><span class="p">:</span></span>
<span id="161">    <span class="n">lower_case_documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span></span>
<span id="162"><span class="nb">print</span><span class="p">(</span><span class="n">lower_case_documents</span><span class="p">)</span></span>
<span id="163"></span>
<span id="164"></span>
<span id="165"><span class="c1"># ** Step 2: Removing all punctuations **</span></span>
<span id="166"><span class="c1"># </span></span>
<span id="167"><span class="c1"># &gt;&gt;**Instructions: **</span></span>
<span id="168"><span class="c1"># Remove all punctuation from the strings in the document set. Save them into a list called </span></span>
<span id="169"><span class="c1"># &#39;sans_punctuation_documents&#39;. </span></span>
<span id="170"></span>
<span id="171"><span class="c1"># In[4]:</span></span>
<span id="172"></span>
<span id="173"></span>
<span id="174"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="175"><span class="sd">Solution:</span></span>
<span id="176"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="177"><span class="n">sans_punctuation_documents</span> <span class="o">=</span> <span class="p">[]</span></span>
<span id="178"><span class="kn">import</span> <span class="nn">string</span></span>
<span id="179"></span>
<span id="180"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">lower_case_documents</span><span class="p">:</span></span>
<span id="181">    <span class="n">sans_punctuation_documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">translate</span><span class="p">(</span><span class="nb">str</span><span class="o">.</span><span class="n">maketrans</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">string</span><span class="o">.</span><span class="n">punctuation</span><span class="p">)))</span></span>
<span id="182"><span class="nb">print</span><span class="p">(</span><span class="n">sans_punctuation_documents</span><span class="p">)</span></span>
<span id="183"></span>
<span id="184"></span>
<span id="185"><span class="c1"># ** Step 3: Tokenization **</span></span>
<span id="186"><span class="c1"># </span></span>
<span id="187"><span class="c1"># Tokenizing a sentence in a document set means splitting up a sentence into individual words using a delimiter. The delimiter specifies what character we will use to identify the beginning and the end of a word(for example we could use a single space as the delimiter for identifying words in our document set.)</span></span>
<span id="188"></span>
<span id="189"><span class="c1"># &gt;&gt;**Instructions:**</span></span>
<span id="190"><span class="c1"># Tokenize the strings stored in &#39;sans_punctuation_documents&#39; using the split() method. and store the final document set </span></span>
<span id="191"><span class="c1"># in a list called &#39;preprocessed_documents&#39;.</span></span>
<span id="192"><span class="c1"># </span></span>
<span id="193"></span>
<span id="194"><span class="c1"># In[6]:</span></span>
<span id="195"></span>
<span id="196"></span>
<span id="197"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="198"><span class="sd">Solution:</span></span>
<span id="199"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="200"><span class="n">preprocessed_documents</span> <span class="o">=</span> <span class="p">[]</span></span>
<span id="201"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sans_punctuation_documents</span><span class="p">:</span></span>
<span id="202">    <span class="n">preprocessed_documents</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">))</span></span>
<span id="203"><span class="nb">print</span><span class="p">(</span><span class="n">preprocessed_documents</span><span class="p">)</span></span>
<span id="204"></span>
<span id="205"></span>
<span id="206"><span class="c1"># ** Step 4: Count frequencies **</span></span>
<span id="207"><span class="c1"># </span></span>
<span id="208"><span class="c1"># Now that we have our document set in the required format, we can proceed to counting the occurrence of each word in each document of the document set. We will use the `Counter` method from the Python `collections` library for this purpose. </span></span>
<span id="209"><span class="c1"># </span></span>
<span id="210"><span class="c1"># `Counter` counts the occurrence of each item in the list and returns a dictionary with the key as the item being counted and the corresponding value being the count of that item in the list. </span></span>
<span id="211"></span>
<span id="212"><span class="c1"># &gt;&gt;**Instructions:**</span></span>
<span id="213"><span class="c1"># Using the Counter() method and preprocessed_documents as the input, create a dictionary with the keys being each word in each document and the corresponding values being the frequncy of occurrence of that word. Save each Counter dictionary as an item in a list called &#39;frequency_list&#39;.</span></span>
<span id="214"><span class="c1"># </span></span>
<span id="215"></span>
<span id="216"><span class="c1"># In[7]:</span></span>
<span id="217"></span>
<span id="218"></span>
<span id="219"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="220"><span class="sd">Solution</span></span>
<span id="221"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="222"><span class="n">frequency_list</span> <span class="o">=</span> <span class="p">[]</span></span>
<span id="223"><span class="kn">import</span> <span class="nn">pprint</span></span>
<span id="224"><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span></span>
<span id="225"></span>
<span id="226"><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">preprocessed_documents</span><span class="p">:</span></span>
<span id="227">    <span class="n">frequency_counts</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">i</span><span class="p">)</span></span>
<span id="228">    <span class="n">frequency_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">frequency_counts</span><span class="p">)</span></span>
<span id="229"><span class="n">pprint</span><span class="o">.</span><span class="n">pprint</span><span class="p">(</span><span class="n">frequency_list</span><span class="p">)</span></span>
<span id="230"></span>
<span id="231"></span>
<span id="232"><span class="c1"># Congratulations! You have implemented the Bag of Words process from scratch! As we can see in our previous output, we have a frequency distribution dictionary which gives a clear view of the text that we are dealing with.</span></span>
<span id="233"><span class="c1"># </span></span>
<span id="234"><span class="c1"># We should now have a solid understanding of what is happening behind the scenes in the `sklearn.feature_extraction.text.CountVectorizer` method of scikit-learn. </span></span>
<span id="235"><span class="c1"># </span></span>
<span id="236"><span class="c1"># We will now implement `sklearn.feature_extraction.text.CountVectorizer` method in the next step.</span></span>
<span id="237"></span>
<span id="238"><span class="c1"># ### Step 2.3: Implementing Bag of Words in scikit-learn ###</span></span>
<span id="239"><span class="c1"># </span></span>
<span id="240"><span class="c1"># Now that we have implemented the BoW concept from scratch, let&#39;s go ahead and use scikit-learn to do this process in a clean and succinct way. We will use the same document set as we used in the previous step. </span></span>
<span id="241"></span>
<span id="242"><span class="c1"># In[167]:</span></span>
<span id="243"></span>
<span id="244"></span>
<span id="245"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="246"><span class="sd">Here we will look to create a frequency matrix on a smaller document set to make sure we understand how the </span></span>
<span id="247"><span class="sd">document-term matrix generation happens. We have created a sample document set &#39;documents&#39;.</span></span>
<span id="248"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="249"><span class="n">documents</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Hello, how are you!&#39;</span><span class="p">,</span></span>
<span id="250">                <span class="s1">&#39;Win money, win from home.&#39;</span><span class="p">,</span></span>
<span id="251">                <span class="s1">&#39;Call me now.&#39;</span><span class="p">,</span></span>
<span id="252">                <span class="s1">&#39;Hello, Call hello you tomorrow?&#39;</span><span class="p">]</span></span>
<span id="253"></span>
<span id="254"></span>
<span id="255"><span class="c1"># &gt;&gt;**Instructions:**</span></span>
<span id="256"><span class="c1"># Import the sklearn.feature_extraction.text.CountVectorizer method and create an instance of it called &#39;count_vector&#39;. </span></span>
<span id="257"></span>
<span id="258"><span class="c1"># In[9]:</span></span>
<span id="259"></span>
<span id="260"></span>
<span id="261"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="262"><span class="sd">Solution</span></span>
<span id="263"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="264"><span class="kn">from</span> <span class="nn">sklearn.feature_extraction.text</span> <span class="kn">import</span> <span class="n">CountVectorizer</span></span>
<span id="265"><span class="n">count_vector</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span></span>
<span id="266"></span>
<span id="267"></span>
<span id="268"><span class="c1"># ** Data preprocessing with CountVectorizer() **</span></span>
<span id="269"><span class="c1"># </span></span>
<span id="270"><span class="c1"># In Step 2.2, we implemented a version of the CountVectorizer() method from scratch that entailed cleaning our data first. This cleaning involved converting all of our data to lower case and removing all punctuation marks. CountVectorizer() has certain parameters which take care of these steps for us. They are:</span></span>
<span id="271"><span class="c1"># </span></span>
<span id="272"><span class="c1"># * `lowercase = True`</span></span>
<span id="273"><span class="c1">#     </span></span>
<span id="274"><span class="c1">#     The `lowercase` parameter has a default value of `True` which converts all of our text to its lower case form.</span></span>
<span id="275"><span class="c1"># </span></span>
<span id="276"><span class="c1"># </span></span>
<span id="277"><span class="c1"># * `token_pattern = (?u)\\b\\w\\w+\\b`</span></span>
<span id="278"><span class="c1">#     </span></span>
<span id="279"><span class="c1">#     The `token_pattern` parameter has a default regular expression value of `(?u)\\b\\w\\w+\\b` which ignores all punctuation marks and treats them as delimiters, while accepting alphanumeric strings of length greater than or equal to 2, as individual tokens or words.</span></span>
<span id="280"><span class="c1"># </span></span>
<span id="281"><span class="c1"># </span></span>
<span id="282"><span class="c1"># * `stop_words`</span></span>
<span id="283"><span class="c1"># </span></span>
<span id="284"><span class="c1">#     The `stop_words` parameter, if set to `english` will remove all words from our document set that match a list of English stop words which is defined in scikit-learn. Considering the size of our dataset and the fact that we are dealing with SMS messages and not larger text sources like e-mail, we will not be setting this parameter value.</span></span>
<span id="285"><span class="c1"># </span></span>
<span id="286"><span class="c1"># You can take a look at all the parameter values of your `count_vector` object by simply printing out the object as follows:</span></span>
<span id="287"></span>
<span id="288"><span class="c1"># In[10]:</span></span>
<span id="289"></span>
<span id="290"></span>
<span id="291"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="292"><span class="sd">Practice node:</span></span>
<span id="293"><span class="sd">Print the &#39;count_vector&#39; object which is an instance of &#39;CountVectorizer()&#39;</span></span>
<span id="294"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="295"><span class="nb">print</span><span class="p">(</span><span class="n">count_vector</span><span class="p">)</span></span>
<span id="296"></span>
<span id="297"></span>
<span id="298"><span class="c1"># &gt;&gt;**Instructions:**</span></span>
<span id="299"><span class="c1"># Fit your document dataset to the CountVectorizer object you have created using fit(), and get the list of words </span></span>
<span id="300"><span class="c1"># which have been categorized as features using the get_feature_names() method.</span></span>
<span id="301"></span>
<span id="302"><span class="c1"># In[12]:</span></span>
<span id="303"></span>
<span id="304"></span>
<span id="305"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="306"><span class="sd">Solution:</span></span>
<span id="307"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="308"><span class="n">count_vector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span></span>
<span id="309"><span class="n">count_vector</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">()</span></span>
<span id="310"></span>
<span id="311"></span>
<span id="312"><span class="c1"># The `get_feature_names()` method returns our feature names for this dataset, which is the set of words that make up our vocabulary for &#39;documents&#39;.</span></span>
<span id="313"></span>
<span id="314"><span class="c1"># &gt;&gt;**</span></span>
<span id="315"><span class="c1"># Instructions:**</span></span>
<span id="316"><span class="c1"># Create a matrix with the rows being each of the 4 documents, and the columns being each word. </span></span>
<span id="317"><span class="c1"># The corresponding (row, column) value is the frequency of occurrance of that word(in the column) in a particular</span></span>
<span id="318"><span class="c1"># document(in the row). You can do this using the transform() method and passing in the document data set as the </span></span>
<span id="319"><span class="c1"># argument. The transform() method returns a matrix of numpy integers, you can convert this to an array using</span></span>
<span id="320"><span class="c1"># toarray(). Call the array &#39;doc_array&#39;</span></span>
<span id="321"><span class="c1"># </span></span>
<span id="322"></span>
<span id="323"><span class="c1"># In[13]:</span></span>
<span id="324"></span>
<span id="325"></span>
<span id="326"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="327"><span class="sd">Solution</span></span>
<span id="328"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="329"><span class="n">doc_array</span> <span class="o">=</span> <span class="n">count_vector</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">documents</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span></span>
<span id="330"><span class="n">doc_array</span></span>
<span id="331"></span>
<span id="332"></span>
<span id="333"><span class="c1"># Now we have a clean representation of the documents in terms of the frequency distribution of the words in them. To make it easier to understand our next step is to convert this array into a dataframe and name the columns appropriately.</span></span>
<span id="334"></span>
<span id="335"><span class="c1"># &gt;&gt;**Instructions:**</span></span>
<span id="336"><span class="c1"># Convert the array we obtained, loaded into &#39;doc_array&#39;, into a dataframe and set the column names to </span></span>
<span id="337"><span class="c1"># the word names(which you computed earlier using get_feature_names(). Call the dataframe &#39;frequency_matrix&#39;.</span></span>
<span id="338"><span class="c1"># </span></span>
<span id="339"></span>
<span id="340"><span class="c1"># In[14]:</span></span>
<span id="341"></span>
<span id="342"></span>
<span id="343"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="344"><span class="sd">Solution</span></span>
<span id="345"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="346"><span class="n">frequency_matrix</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">doc_array</span><span class="p">,</span> </span>
<span id="347">                                <span class="n">columns</span> <span class="o">=</span> <span class="n">count_vector</span><span class="o">.</span><span class="n">get_feature_names</span><span class="p">())</span></span>
<span id="348"><span class="n">frequency_matrix</span></span>
<span id="349"></span>
<span id="350"></span>
<span id="351"><span class="c1"># Congratulations! You have successfully implemented a Bag of Words problem for a document dataset that we created. </span></span>
<span id="352"><span class="c1"># </span></span>
<span id="353"><span class="c1"># One potential issue that can arise from using this method out of the box is the fact that if our dataset of text is extremely large(say if we have a large collection of news articles or email data), there will be certain values that are more common that others simply due to the structure of the language itself. So for example words like &#39;is&#39;, &#39;the&#39;, &#39;an&#39;, pronouns, grammatical contructs etc could skew our matrix and affect our analyis. </span></span>
<span id="354"><span class="c1"># </span></span>
<span id="355"><span class="c1"># There are a couple of ways to mitigate this. One way is to use the `stop_words` parameter and set its value to `english`. This will automatically ignore all words(from our input text) that are found in a built in list of English stop words in scikit-learn.</span></span>
<span id="356"><span class="c1"># </span></span>
<span id="357"><span class="c1"># Another way of mitigating this is by using the [tfidf](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer) method. This method is out of scope for the context of this lesson.</span></span>
<span id="358"></span>
<span id="359"><span class="c1"># ### Step 3.1: Training and testing sets ###</span></span>
<span id="360"><span class="c1"># </span></span>
<span id="361"><span class="c1"># Now that we have understood how to deal with the Bag of Words problem we can get back to our dataset and proceed with our analysis. Our first step in this regard would be to split our dataset into a training and testing set so we can test our model later. </span></span>
<span id="362"></span>
<span id="363"><span class="c1"># </span></span>
<span id="364"><span class="c1"># &gt;&gt;**Instructions:**</span></span>
<span id="365"><span class="c1"># Split the dataset into a training and testing set by using the train_test_split method in sklearn. Split the data</span></span>
<span id="366"><span class="c1"># using the following variables:</span></span>
<span id="367"><span class="c1"># * `X_train` is our training data for the &#39;sms_message&#39; column.</span></span>
<span id="368"><span class="c1"># * `y_train` is our training data for the &#39;label&#39; column</span></span>
<span id="369"><span class="c1"># * `X_test` is our testing data for the &#39;sms_message&#39; column.</span></span>
<span id="370"><span class="c1"># * `y_test` is our testing data for the &#39;label&#39; column</span></span>
<span id="371"><span class="c1"># Print out the number of rows we have in each our training and testing data.</span></span>
<span id="372"><span class="c1"># </span></span>
<span id="373"></span>
<span id="374"><span class="c1"># In[51]:</span></span>
<span id="375"></span>
<span id="376"></span>
<span id="377"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="378"><span class="sd">Solution</span></span>
<span id="379"></span>
<span id="380"><span class="sd">NOTE: sklearn.cross_validation will be deprecated soon to sklearn.model_selection </span></span>
<span id="381"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="382"><span class="c1"># split into training and testing sets</span></span>
<span id="383"><span class="c1"># USE from sklearn.model_selection import train_test_split to avoid seeing deprecation warning.</span></span>
<span id="384"><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span></span>
<span id="385"></span>
<span id="386"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;sms_message&#39;</span><span class="p">],</span> </span>
<span id="387">                                                    <span class="n">df</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">],</span> </span>
<span id="388">                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></span>
<span id="389"></span>
<span id="390"><span class="nb">print</span><span class="p">((</span><span class="s1">&#39;Number of rows in the total set: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span></span>
<span id="391"><span class="nb">print</span><span class="p">((</span><span class="s1">&#39;Number of rows in the training set: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span></span>
<span id="392"><span class="nb">print</span><span class="p">((</span><span class="s1">&#39;Number of rows in the test set: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span></span>
<span id="393"></span>
<span id="394"></span>
<span id="395"><span class="c1"># ### Step 3.2: Applying Bag of Words processing to our dataset. ###</span></span>
<span id="396"><span class="c1"># </span></span>
<span id="397"><span class="c1"># Now that we have split the data, our next objective is to follow the steps from Step 2: Bag of words and convert our data into the desired matrix format. To do this we will be using CountVectorizer() as we did before. There are two  steps to consider here:</span></span>
<span id="398"><span class="c1"># </span></span>
<span id="399"><span class="c1"># * Firstly, we have to fit our training data (`X_train`) into `CountVectorizer()` and return the matrix.</span></span>
<span id="400"><span class="c1"># * Secondly, we have to transform our testing data (`X_test`) to return the matrix. </span></span>
<span id="401"><span class="c1"># </span></span>
<span id="402"><span class="c1"># Note that `X_train` is our training data for the &#39;sms_message&#39; column in our dataset and we will be using this to train our model. </span></span>
<span id="403"><span class="c1"># </span></span>
<span id="404"><span class="c1"># `X_test` is our testing data for the &#39;sms_message&#39; column and this is the data we will be using(after transformation to a matrix) to make predictions on. We will then compare those predictions with `y_test` in a later step. </span></span>
<span id="405"><span class="c1"># </span></span>
<span id="406"><span class="c1"># For now, we have provided the code that does the matrix transformations for you!</span></span>
<span id="407"></span>
<span id="408"><span class="c1"># In[ ]:</span></span>
<span id="409"></span>
<span id="410"></span>
<span id="411"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="412"><span class="sd">[Practice Node]</span></span>
<span id="413"></span>
<span id="414"><span class="sd">The code for this segment is in 2 parts. Firstly, we are learning a vocabulary dictionary for the training data </span></span>
<span id="415"><span class="sd">and then transforming the data into a document-term matrix; secondly, for the testing data we are only </span></span>
<span id="416"><span class="sd">transforming the data into a document-term matrix.</span></span>
<span id="417"></span>
<span id="418"><span class="sd">This is similar to the process we followed in Step 2.3</span></span>
<span id="419"></span>
<span id="420"><span class="sd">We will provide the transformed data to students in the variables &#39;training_data&#39; and &#39;testing_data&#39;.</span></span>
<span id="421"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="422"></span>
<span id="423"></span>
<span id="424"><span class="c1"># In[52]:</span></span>
<span id="425"></span>
<span id="426"></span>
<span id="427"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="428"><span class="sd">Solution</span></span>
<span id="429"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="430"><span class="c1"># Instantiate the CountVectorizer method</span></span>
<span id="431"><span class="n">count_vector</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">()</span></span>
<span id="432"></span>
<span id="433"><span class="c1"># Fit the training data and then return the matrix</span></span>
<span id="434"><span class="n">training_data</span> <span class="o">=</span> <span class="n">count_vector</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">)</span></span>
<span id="435"></span>
<span id="436"><span class="c1"># Transform testing data and return the matrix. Note we are not fitting the testing data into the CountVectorizer()</span></span>
<span id="437"><span class="n">testing_data</span> <span class="o">=</span> <span class="n">count_vector</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></span>
<span id="438"></span>
<span id="439"></span>
<span id="440"><span class="c1"># ### Step 4.1: Bayes Theorem implementation from scratch ###</span></span>
<span id="441"><span class="c1"># </span></span>
<span id="442"><span class="c1"># Now that we have our dataset in the format that we need, we can move onto the next portion of our mission which is the  algorithm we will use to make our predictions to classify a message as spam or not spam. Remember that at the start of the mission we briefly discussed the Bayes theorem but now we shall go into a little more detail. In layman&#39;s terms, the Bayes theorem calculates the probability of an event occurring, based on certain other probabilities that are related to the event in question. It is  composed of a  prior(the probabilities that we are aware of or that is given to us) and the posterior(the probabilities we are looking to compute using the priors). </span></span>
<span id="443"><span class="c1"># </span></span>
<span id="444"><span class="c1"># Let us implement the Bayes Theorem from scratch using a simple example. Let&#39;s say we are trying to find the odds of an individual having diabetes, given that he or she was tested for it and got a positive result. </span></span>
<span id="445"><span class="c1"># In the medical field, such probabilies play a very important role as it usually deals with life and death situatuations. </span></span>
<span id="446"><span class="c1"># </span></span>
<span id="447"><span class="c1"># We assume the following:</span></span>
<span id="448"><span class="c1"># </span></span>
<span id="449"><span class="c1"># `P(D)` is the probability of a person having Diabetes. It&#39;s value is `0.01` or in other words, 1% of the general population has diabetes(Disclaimer: these values are assumptions and are not reflective of any medical study).</span></span>
<span id="450"><span class="c1"># </span></span>
<span id="451"><span class="c1"># `P(Pos)` is the probability of getting a positive test result.</span></span>
<span id="452"><span class="c1"># </span></span>
<span id="453"><span class="c1"># `P(Neg)` is the probability of getting a negative test result.</span></span>
<span id="454"><span class="c1"># </span></span>
<span id="455"><span class="c1"># `P(Pos|D)` is the probability of getting a positive result on a test done for detecting diabetes, given that you have diabetes. This has a value `0.9`. In other words the test is correct 90% of the time. This is also called the Sensitivity or True Positive Rate.</span></span>
<span id="456"><span class="c1"># </span></span>
<span id="457"><span class="c1"># `P(Neg|~D)` is the probability of getting a negative result on a test done for detecting diabetes, given that you do not have diabetes. This also has a value of `0.9` and is therefore correct, 90% of the time. This is also called the Specificity or True Negative Rate.</span></span>
<span id="458"><span class="c1"># </span></span>
<span id="459"><span class="c1"># The Bayes formula is as follows:</span></span>
<span id="460"><span class="c1"># </span></span>
<span id="461"><span class="c1"># &lt;img src=&quot;images/bayes_formula.png&quot; height=&quot;242&quot; width=&quot;242&quot;&gt;</span></span>
<span id="462"><span class="c1"># </span></span>
<span id="463"><span class="c1"># * `P(A)` is the prior probability of A occuring independantly. In our example this is `P(D)`. This value is given to us.</span></span>
<span id="464"><span class="c1"># </span></span>
<span id="465"><span class="c1"># * `P(B)` is the prior probability of B occuring independantly. In our example this is `P(Pos)`.</span></span>
<span id="466"><span class="c1"># </span></span>
<span id="467"><span class="c1"># * `P(A|B)` is the posterior probability that A occurs given B. In our example this is `P(D|Pos)`. That is, **the probability of an individual having diabetes, given that, that individual got a positive test result. This is the value that we are looking to calculate.**</span></span>
<span id="468"><span class="c1"># </span></span>
<span id="469"><span class="c1"># * `P(B|A)` is the likelihood probability of B occuring, given A. In our example this is `P(Pos|D)`. This value is given to us.</span></span>
<span id="470"></span>
<span id="471"><span class="c1"># Putting our values into the formula for Bayes theorem we get:</span></span>
<span id="472"><span class="c1"># </span></span>
<span id="473"><span class="c1"># `P(D|Pos) = (P(D) * P(Pos|D) / P(Pos)`</span></span>
<span id="474"><span class="c1"># </span></span>
<span id="475"><span class="c1"># The probability of getting a positive test result `P(Pos)` can be calulated using the Sensitivity and Specificity as follows:</span></span>
<span id="476"><span class="c1"># </span></span>
<span id="477"><span class="c1"># `P(Pos) = [P(D) * Sensitivity] + [P(~D) * (1-Specificity))]`</span></span>
<span id="478"></span>
<span id="479"><span class="c1"># In[ ]:</span></span>
<span id="480"></span>
<span id="481"></span>
<span id="482"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="483"><span class="sd">Instructions:</span></span>
<span id="484"><span class="sd">Calculate probability of getting a positive test result, P(Pos)</span></span>
<span id="485"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="486"></span>
<span id="487"></span>
<span id="488"><span class="c1"># In[27]:</span></span>
<span id="489"></span>
<span id="490"></span>
<span id="491"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="492"><span class="sd">Solution (skeleton code will be provided)</span></span>
<span id="493"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="494"><span class="c1"># P(D)</span></span>
<span id="495"><span class="n">p_diabetes</span> <span class="o">=</span> <span class="mf">0.01</span></span>
<span id="496"></span>
<span id="497"><span class="c1"># P(~D)</span></span>
<span id="498"><span class="n">p_no_diabetes</span> <span class="o">=</span> <span class="mf">0.99</span></span>
<span id="499"></span>
<span id="500"><span class="c1"># Sensitivity or P(Pos|D)</span></span>
<span id="501"><span class="n">p_pos_diabetes</span> <span class="o">=</span> <span class="mf">0.9</span></span>
<span id="502"></span>
<span id="503"><span class="c1"># Specificity or P(Neg/~D)</span></span>
<span id="504"><span class="n">p_neg_no_diabetes</span> <span class="o">=</span> <span class="mf">0.9</span></span>
<span id="505"></span>
<span id="506"><span class="c1"># P(Pos)</span></span>
<span id="507"><span class="n">p_pos</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_diabetes</span> <span class="o">*</span> <span class="n">p_pos_diabetes</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">p_no_diabetes</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">p_neg_no_diabetes</span><span class="p">))</span></span>
<span id="508"><span class="nb">print</span><span class="p">((</span><span class="s1">&#39;The probability of getting a positive test result P(Pos) is: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="p">,</span><span class="nb">format</span><span class="p">(</span><span class="n">p_pos</span><span class="p">)))</span></span>
<span id="509"></span>
<span id="510"></span>
<span id="511"><span class="c1"># ** Using all of this information we can calculate our posteriors as follows: **</span></span>
<span id="512"><span class="c1">#     </span></span>
<span id="513"><span class="c1"># The probability of an individual having diabetes, given that, that individual got a positive test result:</span></span>
<span id="514"><span class="c1"># </span></span>
<span id="515"><span class="c1"># `P(D/Pos) = (P(D) * Sensitivity)) / P(Pos)`</span></span>
<span id="516"><span class="c1"># </span></span>
<span id="517"><span class="c1"># The probability of an individual not having diabetes, given that, that individual got a positive test result:</span></span>
<span id="518"><span class="c1"># </span></span>
<span id="519"><span class="c1"># `P(~D/Pos) = (P(~D) * (1-Specificity)) / P(Pos)`</span></span>
<span id="520"><span class="c1"># </span></span>
<span id="521"><span class="c1"># The sum of our posteriors will always equal `1`. </span></span>
<span id="522"></span>
<span id="523"><span class="c1"># In[ ]:</span></span>
<span id="524"></span>
<span id="525"></span>
<span id="526"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="527"><span class="sd">Instructions:</span></span>
<span id="528"><span class="sd">Compute the probability of an individual having diabetes, given that, that individual got a positive test result.</span></span>
<span id="529"><span class="sd">In other words, compute P(D|Pos).</span></span>
<span id="530"></span>
<span id="531"><span class="sd">The formula is: P(D|Pos) = (P(D) * P(Pos|D) / P(Pos)</span></span>
<span id="532"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="533"></span>
<span id="534"></span>
<span id="535"><span class="c1"># In[28]:</span></span>
<span id="536"></span>
<span id="537"></span>
<span id="538"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="539"><span class="sd">Solution</span></span>
<span id="540"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="541"><span class="c1"># P(D|Pos)</span></span>
<span id="542"><span class="n">p_diabetes_pos</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_diabetes</span> <span class="o">*</span> <span class="n">p_pos_diabetes</span><span class="p">)</span> <span class="o">/</span> <span class="n">p_pos</span></span>
<span id="543"><span class="nb">print</span><span class="p">((</span><span class="s1">&#39;Probability of an individual having diabetes, given that that individual got a positive test result is:&#39;</span><span class="p">,</span><span class="nb">format</span><span class="p">(</span><span class="n">p_diabetes_pos</span><span class="p">)))</span> </span>
<span id="544"></span>
<span id="545"></span>
<span id="546"><span class="c1"># In[ ]:</span></span>
<span id="547"></span>
<span id="548"></span>
<span id="549"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="550"><span class="sd">Instructions:</span></span>
<span id="551"><span class="sd">Compute the probability of an individual not having diabetes, given that, that individual got a positive test result.</span></span>
<span id="552"><span class="sd">In other words, compute P(~D|Pos).</span></span>
<span id="553"></span>
<span id="554"><span class="sd">The formula is: P(~D|Pos) = (P(~D) * P(Pos|~D) / P(Pos)</span></span>
<span id="555"></span>
<span id="556"><span class="sd">Note that P(Pos/~D) can be computed as 1 - P(Neg/~D). </span></span>
<span id="557"></span>
<span id="558"><span class="sd">Therefore:</span></span>
<span id="559"><span class="sd">P(Pos/~D) = p_pos_no_diabetes = 1 - 0.9 = 0.1</span></span>
<span id="560"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="561"></span>
<span id="562"></span>
<span id="563"><span class="c1"># In[203]:</span></span>
<span id="564"></span>
<span id="565"></span>
<span id="566"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="567"><span class="sd">Solution</span></span>
<span id="568"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="569"><span class="c1"># P(Pos/~D)</span></span>
<span id="570"><span class="n">p_pos_no_diabetes</span> <span class="o">=</span> <span class="mf">0.1</span></span>
<span id="571"></span>
<span id="572"><span class="c1"># P(~D|Pos)</span></span>
<span id="573"><span class="n">p_no_diabetes_pos</span> <span class="o">=</span> <span class="p">(</span><span class="n">p_no_diabetes</span> <span class="o">*</span> <span class="n">p_pos_no_diabetes</span><span class="p">)</span> <span class="o">/</span> <span class="n">p_pos</span></span>
<span id="574"><span class="nb">print</span><span class="p">((</span><span class="s1">&#39;Probability of an individual not having diabetes, given that that individual got a positive test result is:&#39;</span><span class="p">,</span><span class="n">p_no_diabetes_pos</span><span class="p">))</span></span>
<span id="575"></span>
<span id="576"></span>
<span id="577"><span class="c1"># Congratulations! You have implemented Bayes theorem from scratch. Your analysis shows that even if you get a positive test result, there is only a 8.3% chance that you actually have diabetes and a 91.67% chance that you do not have diabetes. This is of course assuming that only 1% of the entire population has diabetes which of course is only an assumption.</span></span>
<span id="578"></span>
<span id="579"><span class="c1"># ** What does the term &#39;Naive&#39; in &#39;Naive Bayes&#39; mean ? ** </span></span>
<span id="580"><span class="c1"># </span></span>
<span id="581"><span class="c1"># The term &#39;Naive&#39; in Naive Bayes comes from the fact that the algorithm considers the features that it is using to make the predictions to be independent of each other, which may not always be the case. So in our Diabetes example, we are considering only one feature, that is the test result. Say we added another feature, &#39;exercise&#39;. Let&#39;s say this feature has a binary value of `0` and `1`, where the former signifies that the individual exercises less than or equal to 2 days a week and the latter signifies that the individual exercises greater than or equal to 3 days a week. If we had to use both of these features, namely the test result and the value of the &#39;exercise&#39; feature, to compute our final probabilities, Bayes&#39; theorem would fail. Naive Bayes&#39; is an extension of Bayes&#39; theorem that assumes that all the features are independent of each other. </span></span>
<span id="582"></span>
<span id="583"><span class="c1"># ### Step 4.2: Naive Bayes implementation from scratch ###</span></span>
<span id="584"><span class="c1"># </span></span>
<span id="585"><span class="c1"># </span></span>
<span id="586"></span>
<span id="587"><span class="c1"># Now that you have understood the ins and outs of Bayes Theorem, we will extend it to consider cases where we have more than feature. </span></span>
<span id="588"><span class="c1"># </span></span>
<span id="589"><span class="c1"># Let&#39;s say that we have two political parties&#39; candidates, &#39;Jill Stein&#39; of the Green Party and &#39;Gary Johnson&#39; of the Libertarian Party and we have the probabilities of each of these candidates saying the words &#39;freedom&#39;, &#39;immigration&#39; and &#39;environment&#39; when they give a speech:</span></span>
<span id="590"><span class="c1"># </span></span>
<span id="591"><span class="c1"># * Probability that Jill Stein says &#39;freedom&#39;: 0.1 ---------&gt; `P(F|J)`</span></span>
<span id="592"><span class="c1"># * Probability that Jill Stein says &#39;immigration&#39;: 0.1 -----&gt; `P(I|J)`</span></span>
<span id="593"><span class="c1"># * Probability that Jill Stein says &#39;environment&#39;: 0.8 -----&gt; `P(E|J)`</span></span>
<span id="594"><span class="c1"># </span></span>
<span id="595"><span class="c1"># </span></span>
<span id="596"><span class="c1"># * Probability that Gary Johnson says &#39;freedom&#39;: 0.7 -------&gt; `P(F|G)`</span></span>
<span id="597"><span class="c1"># * Probability that Gary Johnson says &#39;immigration&#39;: 0.2 ---&gt; `P(I|G)`</span></span>
<span id="598"><span class="c1"># * Probability that Gary Johnson says &#39;environment&#39;: 0.1 ---&gt; `P(E|G)`</span></span>
<span id="599"><span class="c1"># </span></span>
<span id="600"><span class="c1"># </span></span>
<span id="601"><span class="c1"># And let us also assume that the probablility of Jill Stein giving a speech, `P(J)` is `0.5` and the same for Gary Johnson, `P(G) = 0.5`. </span></span>
<span id="602"><span class="c1"># </span></span>
<span id="603"><span class="c1"># </span></span>
<span id="604"><span class="c1"># Given this, what if we had to find the probabilities of Jill Stein saying the words &#39;freedom&#39; and &#39;immigration&#39;? This is where the Naive Bayes&#39;theorem comes into play as we are considering two features, &#39;freedom&#39; and &#39;immigration&#39;.</span></span>
<span id="605"><span class="c1"># </span></span>
<span id="606"><span class="c1"># Now we are at a place where we can define the formula for the Naive Bayes&#39; theorem:</span></span>
<span id="607"><span class="c1"># </span></span>
<span id="608"><span class="c1"># &lt;img src=&quot;images/naivebayes.png&quot; height=&quot;342&quot; width=&quot;342&quot;&gt;</span></span>
<span id="609"><span class="c1"># </span></span>
<span id="610"><span class="c1"># Here, `y` is the class variable or in our case the name of the candidate and `x1` through `xn` are the feature vectors or in our case the individual words. The theorem makes the assumption that each of the feature vectors or words (`xi`) are independent of each other.</span></span>
<span id="611"></span>
<span id="612"><span class="c1"># To break this down, we have to compute the following posterior probabilities:</span></span>
<span id="613"><span class="c1"># </span></span>
<span id="614"><span class="c1"># * `P(J|F,I)`: Probability of Jill Stein saying the words Freedom and Immigration. </span></span>
<span id="615"><span class="c1"># </span></span>
<span id="616"><span class="c1">#     Using the formula and our knowledge of Bayes&#39; theorem, we can compute this as follows: `P(J|F,I)` = `(P(J) * P(F|J) * P(I|J)) / P(F,I)`. Here `P(F,I)` is the probability of the words &#39;freedom&#39; and &#39;immigration&#39; being said in a speech.</span></span>
<span id="617"><span class="c1">#     </span></span>
<span id="618"><span class="c1"># </span></span>
<span id="619"><span class="c1"># * `P(G|F,I)`: Probability of Gary Johnson saying the words Freedom and Immigration.  </span></span>
<span id="620"><span class="c1">#     </span></span>
<span id="621"><span class="c1">#     Using the formula, we can compute this as follows: `P(G|F,I)` = `(P(G) * P(F|G) * P(I|G)) / P(F,I)`</span></span>
<span id="622"></span>
<span id="623"><span class="c1"># In[ ]:</span></span>
<span id="624"></span>
<span id="625"></span>
<span id="626"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="627"><span class="sd">Instructions: Compute the probability of the words &#39;freedom&#39; and &#39;immigration&#39; being said in a speech, or</span></span>
<span id="628"><span class="sd">P(F,I).</span></span>
<span id="629"></span>
<span id="630"><span class="sd">The first step is multiplying the probabilities of Jill Stein giving a speech with her individual </span></span>
<span id="631"><span class="sd">probabilities of saying the words &#39;freedom&#39; and &#39;immigration&#39;. Store this in a variable called p_j_text</span></span>
<span id="632"></span>
<span id="633"><span class="sd">The second step is multiplying the probabilities of Gary Johnson giving a speech with his individual </span></span>
<span id="634"><span class="sd">probabilities of saying the words &#39;freedom&#39; and &#39;immigration&#39;. Store this in a variable called p_g_text</span></span>
<span id="635"></span>
<span id="636"><span class="sd">The third step is to add both of these probabilities and you will get P(F,I).</span></span>
<span id="637"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="638"></span>
<span id="639"></span>
<span id="640"><span class="c1"># In[29]:</span></span>
<span id="641"></span>
<span id="642"></span>
<span id="643"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="644"><span class="sd">Solution: Step 1</span></span>
<span id="645"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="646"><span class="c1"># P(J)</span></span>
<span id="647"><span class="n">p_j</span> <span class="o">=</span> <span class="mf">0.5</span></span>
<span id="648"></span>
<span id="649"><span class="c1"># P(F/J)</span></span>
<span id="650"><span class="n">p_j_f</span> <span class="o">=</span> <span class="mf">0.1</span></span>
<span id="651"></span>
<span id="652"><span class="c1"># P(I/J)</span></span>
<span id="653"><span class="n">p_j_i</span> <span class="o">=</span> <span class="mf">0.1</span></span>
<span id="654"></span>
<span id="655"><span class="n">p_j_text</span> <span class="o">=</span> <span class="n">p_j</span> <span class="o">*</span> <span class="n">p_j_f</span> <span class="o">*</span> <span class="n">p_j_i</span></span>
<span id="656"><span class="nb">print</span><span class="p">(</span><span class="n">p_j_text</span><span class="p">)</span></span>
<span id="657"></span>
<span id="658"></span>
<span id="659"><span class="c1"># In[30]:</span></span>
<span id="660"></span>
<span id="661"></span>
<span id="662"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="663"><span class="sd">Solution: Step 2</span></span>
<span id="664"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="665"><span class="c1"># P(G)</span></span>
<span id="666"><span class="n">p_g</span> <span class="o">=</span> <span class="mf">0.5</span></span>
<span id="667"></span>
<span id="668"><span class="c1"># P(F/G)</span></span>
<span id="669"><span class="n">p_g_f</span> <span class="o">=</span> <span class="mf">0.7</span></span>
<span id="670"></span>
<span id="671"><span class="c1"># P(I/G)</span></span>
<span id="672"><span class="n">p_g_i</span> <span class="o">=</span> <span class="mf">0.2</span></span>
<span id="673"></span>
<span id="674"><span class="n">p_g_text</span> <span class="o">=</span> <span class="n">p_g</span> <span class="o">*</span> <span class="n">p_g_f</span> <span class="o">*</span> <span class="n">p_g_i</span></span>
<span id="675"><span class="nb">print</span><span class="p">(</span><span class="n">p_g_text</span><span class="p">)</span></span>
<span id="676"></span>
<span id="677"></span>
<span id="678"><span class="c1"># In[31]:</span></span>
<span id="679"></span>
<span id="680"></span>
<span id="681"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="682"><span class="sd">Solution: Step 3: Compute P(F,I) and store in p_f_i</span></span>
<span id="683"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="684"><span class="n">p_f_i</span> <span class="o">=</span> <span class="n">p_j_text</span> <span class="o">+</span> <span class="n">p_g_text</span></span>
<span id="685"><span class="nb">print</span><span class="p">((</span><span class="s1">&#39;Probability of words freedom and immigration being said are: &#39;</span><span class="p">,</span> <span class="nb">format</span><span class="p">(</span><span class="n">p_f_i</span><span class="p">)))</span></span>
<span id="686"></span>
<span id="687"></span>
<span id="688"><span class="c1"># Now we can compute the probability of `P(J|F,I)`, that is the probability of Jill Stein saying the words Freedom and Immigration and `P(G|F,I)`, that is the probability of Gary Johnson saying the words Freedom and Immigration.</span></span>
<span id="689"></span>
<span id="690"><span class="c1"># In[ ]:</span></span>
<span id="691"></span>
<span id="692"></span>
<span id="693"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="694"><span class="sd">Instructions:</span></span>
<span id="695"><span class="sd">Compute P(J|F,I) using the formula P(J|F,I) = (P(J) * P(F|J) * P(I|J)) / P(F,I) and store it in a variable p_j_fi</span></span>
<span id="696"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="697"></span>
<span id="698"></span>
<span id="699"><span class="c1"># In[32]:</span></span>
<span id="700"></span>
<span id="701"></span>
<span id="702"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="703"><span class="sd">Solution</span></span>
<span id="704"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="705"><span class="n">p_j_fi</span> <span class="o">=</span> <span class="n">p_j_text</span> <span class="o">/</span> <span class="n">p_f_i</span></span>
<span id="706"><span class="nb">print</span><span class="p">((</span><span class="s1">&#39;The probability of Jill Stein saying the words Freedom and Immigration: &#39;</span><span class="p">,</span> <span class="nb">format</span><span class="p">(</span><span class="n">p_j_fi</span><span class="p">)))</span></span>
<span id="707"></span>
<span id="708"></span>
<span id="709"><span class="c1"># In[ ]:</span></span>
<span id="710"></span>
<span id="711"></span>
<span id="712"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="713"><span class="sd">Instructions:</span></span>
<span id="714"><span class="sd">Compute P(G|F,I) using the formula P(G|F,I) = (P(G) * P(F|G) * P(I|G)) / P(F,I) and store it in a variable p_g_fi</span></span>
<span id="715"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="716"></span>
<span id="717"></span>
<span id="718"><span class="c1"># In[33]:</span></span>
<span id="719"></span>
<span id="720"></span>
<span id="721"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="722"><span class="sd">Solution</span></span>
<span id="723"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="724"><span class="n">p_g_fi</span> <span class="o">=</span> <span class="n">p_g_text</span> <span class="o">/</span> <span class="n">p_f_i</span></span>
<span id="725"><span class="nb">print</span><span class="p">((</span><span class="s1">&#39;The probability of Gary Johnson saying the words Freedom and Immigration: &#39;</span><span class="p">,</span> <span class="nb">format</span><span class="p">(</span><span class="n">p_g_fi</span><span class="p">)))</span></span>
<span id="726"></span>
<span id="727"></span>
<span id="728"><span class="c1"># And as we can see, just like in the Bayes&#39; theorem case, the sum of our posteriors is equal to 1. Congratulations! You have implemented the Naive Bayes&#39; theorem from scratch. Our analysis shows that there is only a 6.6% chance that Jill Stein of the Green Party uses the words &#39;freedom&#39; and &#39;immigration&#39; in her speech as compard the the 93.3% chance for Gary Johnson of the Libertarian party.</span></span>
<span id="729"></span>
<span id="730"><span class="c1"># Another more generic example of Naive Bayes&#39; in action is as when we search for the term &#39;Sacramento Kings&#39; in a search engine. In order for us to get the results pertaining to the Scramento Kings NBA basketball team, the search engine needs to be able to associate the two words together and not treat them individually, in which case we would get results of images tagged with &#39;Sacramento&#39; like pictures of city landscapes and images of &#39;Kings&#39; which could be pictures of crowns or kings from history when what we are looking to get are images of the basketball team. This is a classic case of the search engine treating the words as independant entities and hence being &#39;naive&#39; in its approach. </span></span>
<span id="731"><span class="c1"># </span></span>
<span id="732"><span class="c1"># </span></span>
<span id="733"><span class="c1"># Applying this to our problem of classifying messages as spam, the Naive Bayes algorithm *looks at each word individually and not as associated entities* with any kind of link between them. In the case of spam detectors, this usually works as there are certain red flag words which can almost guarantee its classification as spam, for example emails with words like &#39;viagra&#39; are usually classified as spam.</span></span>
<span id="734"></span>
<span id="735"><span class="c1"># ### Step 5: Naive Bayes implementation using scikit-learn ###</span></span>
<span id="736"><span class="c1"># </span></span>
<span id="737"><span class="c1"># Thankfully, sklearn has several Naive Bayes implementations that we can use and so we do not have to do the math from scratch. We will be using sklearns `sklearn.naive_bayes` method to make predictions on our dataset. </span></span>
<span id="738"><span class="c1"># </span></span>
<span id="739"><span class="c1"># Specifically, we will be using the multinomial Naive Bayes implementation. This particular classifier is suitable for classification with discrete features (such as in our case, word counts for text classification). It takes in integer word counts as its input. On the other hand Gaussian Naive Bayes is better suited for continuous data as it assumes that the input data has a Gaussian(normal) distribution.</span></span>
<span id="740"></span>
<span id="741"><span class="c1"># In[ ]:</span></span>
<span id="742"></span>
<span id="743"></span>
<span id="744"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="745"><span class="sd">Instructions:</span></span>
<span id="746"></span>
<span id="747"><span class="sd">We have loaded the training data into the variable &#39;training_data&#39; and the testing data into the </span></span>
<span id="748"><span class="sd">variable &#39;testing_data&#39;.</span></span>
<span id="749"></span>
<span id="750"><span class="sd">Import the MultinomialNB classifier and fit the training data into the classifier using fit(). Name your classifier</span></span>
<span id="751"><span class="sd">&#39;naive_bayes&#39;. You will be training the classifier using &#39;training_data&#39; and y_train&#39; from our split earlier. </span></span>
<span id="752"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="753"></span>
<span id="754"></span>
<span id="755"><span class="c1"># In[53]:</span></span>
<span id="756"></span>
<span id="757"></span>
<span id="758"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="759"><span class="sd">Solution</span></span>
<span id="760"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="761"><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">MultinomialNB</span></span>
<span id="762"><span class="n">naive_bayes</span> <span class="o">=</span> <span class="n">MultinomialNB</span><span class="p">()</span></span>
<span id="763"><span class="n">naive_bayes</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">training_data</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">train</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([763, 782])">highlight train/test sites</button>
<span id="764"></span>
<span id="765"></span>
<span id="766"><span class="c1"># In[ ]:</span></span>
<span id="767"></span>
<span id="768"></span>
<span id="769"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="770"><span class="sd">Instructions:</span></span>
<span id="771"><span class="sd">Now that our algorithm has been trained using the training data set we can now make some predictions on the test data</span></span>
<span id="772"><span class="sd">stored in &#39;testing_data&#39; using predict(). Save your predictions into the &#39;predictions&#39; variable.</span></span>
<span id="773"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="774"></span>
<span id="775"></span>
<span id="776"><span class="c1"># In[54]:</span></span>
<span id="777"></span>
<span id="778"></span>
<span id="779"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="780"><span class="sd">Solution</span></span>
<span id="781"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="782"><span class="n">predictions</span> <span class="o">=</span> <span class="n">naive_bayes</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">testing_data</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([763, 782])">highlight train/test sites</button>
<span id="783"></span>
<span id="784"></span>
<span id="785"><span class="c1"># Now that predictions have been made on our test set, we need to check the accuracy of our predictions.</span></span>
<span id="786"></span>
<span id="787"><span class="c1"># ### Step 6: Evaluating our model ###</span></span>
<span id="788"><span class="c1"># </span></span>
<span id="789"><span class="c1"># Now that we have made predictions on our test set, our next goal is to evaluate how well our model is doing. There are various mechanisms for doing so, but first let&#39;s do quick recap of them.</span></span>
<span id="790"><span class="c1"># </span></span>
<span id="791"><span class="c1"># ** Accuracy ** measures how often the classifier makes the correct prediction. Its the ratio of the number of correct predictions to the total number of predictions (the number of test data points).</span></span>
<span id="792"><span class="c1"># </span></span>
<span id="793"><span class="c1"># ** Precision ** tells us what proportion of messages we classified as spam, actually were spam.</span></span>
<span id="794"><span class="c1"># It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classification), in other words it is the ratio of</span></span>
<span id="795"><span class="c1"># </span></span>
<span id="796"><span class="c1"># `[True Positives/(True Positives + False Positives)]`</span></span>
<span id="797"><span class="c1"># </span></span>
<span id="798"><span class="c1"># ** Recall(sensitivity)** tells us what proportion of messages that actually were spam were classified by us as spam.</span></span>
<span id="799"><span class="c1"># It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of</span></span>
<span id="800"><span class="c1"># </span></span>
<span id="801"><span class="c1"># `[True Positives/(True Positives + False Negatives)]`</span></span>
<span id="802"><span class="c1"># </span></span>
<span id="803"><span class="c1"># For classification problems that are skewed in their classification distributions like in our case, for example if we had a 100 text messages and only 2 were spam and the rest 98 weren&#39;t, accuracy by itself is not a very good metric. We could classify 90 messages as not spam(including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam(all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the F1 score, which is weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score.</span></span>
<span id="804"></span>
<span id="805"><span class="c1"># We will be using all 4 metrics to make sure our model does well. For all 4 metrics whose values can range from 0 to 1, having a score as close to 1 as possible is a good indicator of how well our model is doing.</span></span>
<span id="806"></span>
<span id="807"><span class="c1"># In[ ]:</span></span>
<span id="808"></span>
<span id="809"></span>
<span id="810"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="811"><span class="sd">Instructions:</span></span>
<span id="812"><span class="sd">Compute the accuracy, precision, recall and F1 scores of your model using your test data &#39;y_test&#39; and the predictions</span></span>
<span id="813"><span class="sd">you made earlier stored in the &#39;predictions&#39; variable.</span></span>
<span id="814"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="815"></span>
<span id="816"></span>
<span id="817"><span class="c1"># In[56]:</span></span>
<span id="818"></span>
<span id="819"></span>
<span id="820"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="821"><span class="sd">Solution</span></span>
<span id="822"><span class="sd">&#39;&#39;&#39;</span></span>
<span id="823"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span></span>
<span id="824"><span class="nb">print</span><span class="p">((</span><span class="s1">&#39;Accuracy score: &#39;</span><span class="p">,</span> <span class="nb">format</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">))))</span></span>
<span id="825"><span class="nb">print</span><span class="p">((</span><span class="s1">&#39;Precision score: &#39;</span><span class="p">,</span> <span class="nb">format</span><span class="p">(</span><span class="n">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">))))</span></span>
<span id="826"><span class="nb">print</span><span class="p">((</span><span class="s1">&#39;Recall score: &#39;</span><span class="p">,</span> <span class="nb">format</span><span class="p">(</span><span class="n">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">))))</span></span>
<span id="827"><span class="nb">print</span><span class="p">((</span><span class="s1">&#39;F1 score: &#39;</span><span class="p">,</span> <span class="nb">format</span><span class="p">(</span><span class="n">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">predictions</span><span class="p">))))</span></span>
<span id="828"></span>
<span id="829"></span>
<span id="830"><span class="c1"># ### Step 7: Conclusion ###</span></span>
<span id="831"><span class="c1"># </span></span>
<span id="832"><span class="c1"># One of the major advantages that Naive Bayes has over other classification algorithms is its ability to handle an extremely large number of features. In our case, each word is treated as a feature and there are thousands of different words. Also, it performs well even with the presence of irrelevant features and is relatively unaffected by them. The other major advantage it has is its relative simplicity. Naive Bayes&#39; works well right out of the box and tuning it&#39;s parameters is rarely ever necessary, except usually in cases where the distribution of the data is known. </span></span>
<span id="833"><span class="c1"># It rarely ever overfits the data. Another important advantage is that its model training and prediction times are very fast for the amount of data it can handle. All in all, Naive Bayes&#39; really is a gem of an algorithm!</span></span>
<span id="834"><span class="c1"># </span></span>
<span id="835"><span class="c1"># Congratulations! You have succesfully designed a model that can efficiently predict if an SMS message is spam or not!</span></span>
<span id="836"><span class="c1"># </span></span>
<span id="837"><span class="c1"># Thank you for learning with us!</span></span>
<span id="838"></span>
<span id="839"><span class="c1"># In[ ]:</span></span>
</pre></div>
</td></tr></table></body>
</html>
