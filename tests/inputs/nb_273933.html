<script>
    let highlighted = [];
    function highlight_lines(lines) {
        for (let line of highlighted) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = '';
        }
        highlighted = lines;
        for (let line of highlighted) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = 'yellow';
        }
    }
    let marked = [];
    function mark_leak_lines(lines) {
        for (let line of marked) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = '';
        }
        marked = lines;
        for (let line of marked) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = ele.style.backgroundColor = 'lightgreen';
        }
    }
    function show_infos(lines) {
        for (let line of lines) {
            let ele = document.getElementById(String(line) + "-info");
            if (ele) {
                ele.style.display = ele.style.display == 'none'? '': 'none'
            }
        }
    }
</script>
    <style type="text/css">
    .sum table {
    font-family: arial, sans-serif;
    border-collapse: collapse;
    width: 100%;
    }

    .sum td, .sum th {
    border: 1px solid #dddddd;
    text-align: left;
    padding: 8px;
    }

    .sum tr:hover {background-color: #D6EEEE;}
</style>
<center>
<table class="sum">
  <tbody><tr>
    <th>Leakage</th>
    <th>#Detected</th>
    <th>Locations</th>
  </tr>
  <tr>
    <td>Pre-processing leakage</td>
    <td>1</td>
    <td><a href="#317"><button type="button" style="line-height: 85%; None" onclick="None">317</button></a></td>
  </tr>
  <tr>
    <td>Overlap leakage</td>
    <td>0</td>
    <td></td>
  </tr>
  <tr>
    <td>No independence test data</td>
    <td>1</td>
    <td><a href="#317"><button type="button" style="line-height: 85%; None" onclick="None">317</button></a></td>
  </tr>
</tbody></table></center>

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<!--
generated by Pygments <https://pygments.org/>
Copyright 2006-2021 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
-->
<html>
<head>
  <title></title>
  <meta http-equiv="content-type" content="text/html; charset=None">
  <style type="text/css">
/*
generated by Pygments <https://pygments.org/>
Copyright 2006-2021 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
pre { line-height: 145%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
body .hll { background-color: #ffffcc }
body { background: #f8f8f8; }
body .c { color: #408080; font-style: italic } /* Comment */
body .err { border: 1px solid #FF0000 } /* Error */
body .k { color: #008000; font-weight: bold } /* Keyword */
body .o { color: #666666 } /* Operator */
body .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
body .cm { color: #408080; font-style: italic } /* Comment.Multiline */
body .cp { color: #BC7A00 } /* Comment.Preproc */
body .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
body .c1 { color: #408080; font-style: italic } /* Comment.Single */
body .cs { color: #408080; font-style: italic } /* Comment.Special */
body .gd { color: #A00000 } /* Generic.Deleted */
body .ge { font-style: italic } /* Generic.Emph */
body .gr { color: #FF0000 } /* Generic.Error */
body .gh { color: #000080; font-weight: bold } /* Generic.Heading */
body .gi { color: #00A000 } /* Generic.Inserted */
body .go { color: #888888 } /* Generic.Output */
body .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
body .gs { font-weight: bold } /* Generic.Strong */
body .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
body .gt { color: #0044DD } /* Generic.Traceback */
body .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
body .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
body .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
body .kp { color: #008000 } /* Keyword.Pseudo */
body .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
body .kt { color: #B00040 } /* Keyword.Type */
body .m { color: #666666 } /* Literal.Number */
body .s { color: #BA2121 } /* Literal.String */
body .na { color: #7D9029 } /* Name.Attribute */
body .nb { color: #008000 } /* Name.Builtin */
body .nc { color: #0000FF; font-weight: bold } /* Name.Class */
body .no { color: #880000 } /* Name.Constant */
body .nd { color: #AA22FF } /* Name.Decorator */
body .ni { color: #999999; font-weight: bold } /* Name.Entity */
body .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
body .nf { color: #0000FF } /* Name.Function */
body .nl { color: #A0A000 } /* Name.Label */
body .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
body .nt { color: #008000; font-weight: bold } /* Name.Tag */
body .nv { color: #19177C } /* Name.Variable */
body .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
body .w { color: #bbbbbb } /* Text.Whitespace */
body .mb { color: #666666 } /* Literal.Number.Bin */
body .mf { color: #666666 } /* Literal.Number.Float */
body .mh { color: #666666 } /* Literal.Number.Hex */
body .mi { color: #666666 } /* Literal.Number.Integer */
body .mo { color: #666666 } /* Literal.Number.Oct */
body .sa { color: #BA2121 } /* Literal.String.Affix */
body .sb { color: #BA2121 } /* Literal.String.Backtick */
body .sc { color: #BA2121 } /* Literal.String.Char */
body .dl { color: #BA2121 } /* Literal.String.Delimiter */
body .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
body .s2 { color: #BA2121 } /* Literal.String.Double */
body .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
body .sh { color: #BA2121 } /* Literal.String.Heredoc */
body .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
body .sx { color: #008000 } /* Literal.String.Other */
body .sr { color: #BB6688 } /* Literal.String.Regex */
body .s1 { color: #BA2121 } /* Literal.String.Single */
body .ss { color: #19177C } /* Literal.String.Symbol */
body .bp { color: #008000 } /* Name.Builtin.Pseudo */
body .fm { color: #0000FF } /* Name.Function.Magic */
body .vc { color: #19177C } /* Name.Variable.Class */
body .vg { color: #19177C } /* Name.Variable.Global */
body .vi { color: #19177C } /* Name.Variable.Instance */
body .vm { color: #19177C } /* Name.Variable.Magic */
body .il { color: #666666 } /* Literal.Number.Integer.Long */

  </style>
</head>
<body>
<h2></h2>

<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span id="2"><span class="c1"># coding: utf-8</span></span>
<span id="3"></span>
<span id="4"><span class="c1"># # Machine Learning Engineer Nanodegree</span></span>
<span id="5"><span class="c1"># ## Supervised Learning</span></span>
<span id="6"><span class="c1"># ## Project 2: Building a Student Intervention System</span></span>
<span id="7"></span>
<span id="8"><span class="c1"># Welcome to the second project of the Machine Learning Engineer Nanodegree! In this notebook, some template code has already been provided for you, and it will be your job to implement the additional functionality necessary to successfully complete this project. Sections that begin with **&#39;Implementation&#39;** in the header indicate that the following block of code will require additional functionality which you must provide. Instructions will be provided for each section and the specifics of the implementation are marked in the code block with a `&#39;TODO&#39;` statement. Please be sure to read the instructions carefully!</span></span>
<span id="9"><span class="c1"># </span></span>
<span id="10"><span class="c1"># In addition to implementing code, there will be questions that you must answer which relate to the project and your implementation. Each section where you will answer a question is preceded by a **&#39;Question X&#39;** header. Carefully read each question and provide thorough answers in the following text boxes that begin with **&#39;Answer:&#39;**. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide.  </span></span>
<span id="11"><span class="c1"># </span></span>
<span id="12"><span class="c1"># &gt;**Note:** Code and Markdown cells can be executed using the **Shift + Enter** keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode.</span></span>
<span id="13"></span>
<span id="14"><span class="c1"># ### Question 1 - Classification vs. Regression</span></span>
<span id="15"><span class="c1"># *Your goal for this project is to identify students who might need early intervention before they fail to graduate. Which type of supervised learning problem is this, classification or regression? Why?*</span></span>
<span id="16"></span>
<span id="17"><span class="c1"># **Answer: ** Esse é um problema de classificação porque a variável que estamos prevendo é categórica e não contínua.</span></span>
<span id="18"></span>
<span id="19"><span class="c1"># ## Exploring the Data</span></span>
<span id="20"><span class="c1"># Run the code cell below to load necessary Python libraries and load the student data. Note that the last column from this dataset, `&#39;passed&#39;`, will be our target label (whether the student graduated or didn&#39;t graduate). All other columns are features about each student.</span></span>
<span id="21"></span>
<span id="22"><span class="c1"># In[1]:</span></span>
<span id="23"></span>
<span id="24"></span>
<span id="25"><span class="c1"># Import libraries</span></span>
<span id="26"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span></span>
<span id="27"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span></span>
<span id="28"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span></span>
<span id="29"><span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span></span>
<span id="30"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">f1_score</span></span>
<span id="31"></span>
<span id="32"><span class="c1"># Pretty display for notebooks</span></span>
<span id="33"><span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">&#39;matplotlib&#39;</span><span class="p">,</span> <span class="s1">&#39;inline&#39;</span><span class="p">)</span></span>
<span id="34"></span>
<span id="35"><span class="c1"># Read student data</span></span>
<span id="36"><span class="n">student_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s2">&quot;student-data.csv&quot;</span><span class="p">)</span></span>
<span id="37"><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Student data read successfully!&quot;</span><span class="p">)</span></span>
<span id="38"></span>
<span id="39"></span>
<span id="40"><span class="c1"># ### Implementation: Data Exploration</span></span>
<span id="41"><span class="c1"># Let&#39;s begin by investigating the dataset to determine how many students we have information on, and learn about the graduation rate among these students. In the code cell below, you will need to compute the following:</span></span>
<span id="42"><span class="c1"># - The total number of students, `n_students`.</span></span>
<span id="43"><span class="c1"># - The total number of features for each student, `n_features`.</span></span>
<span id="44"><span class="c1"># - The number of those students who passed, `n_passed`.</span></span>
<span id="45"><span class="c1"># - The number of those students who failed, `n_failed`.</span></span>
<span id="46"><span class="c1"># - The graduation rate of the class, `grad_rate`, in percent (%).</span></span>
<span id="47"><span class="c1"># </span></span>
<span id="48"></span>
<span id="49"><span class="c1"># In[2]:</span></span>
<span id="50"></span>
<span id="51"></span>
<span id="52"><span class="n">n_students</span> <span class="o">=</span> <span class="n">student_data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></span>
<span id="53"><span class="nb">print</span><span class="p">(</span><span class="n">n_students</span><span class="p">)</span></span>
<span id="54"></span>
<span id="55"></span>
<span id="56"><span class="c1"># In[3]:</span></span>
<span id="57"></span>
<span id="58"></span>
<span id="59"><span class="n">n_features</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">student_data</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">values</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="c1">#menos a coluna passed</span></span>
<span id="60"><span class="nb">print</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span> <span class="c1">#número de features</span></span>
<span id="61"></span>
<span id="62"></span>
<span id="63"><span class="c1"># In[4]:</span></span>
<span id="64"></span>
<span id="65"></span>
<span id="66"><span class="n">n_passed</span> <span class="o">=</span> <span class="n">student_data</span><span class="o">.</span><span class="n">passed</span><span class="o">.</span><span class="n">value_counts</span><span class="p">()[</span><span class="s1">&#39;yes&#39;</span><span class="p">]</span></span>
<span id="67"><span class="n">n_failed</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">student_data</span><span class="o">.</span><span class="n">passed</span> <span class="o">==</span> <span class="s2">&quot;no&quot;</span><span class="p">)</span></span>
<span id="68"><span class="nb">print</span><span class="p">(</span><span class="n">n_passed</span><span class="p">)</span></span>
<span id="69"><span class="nb">print</span><span class="p">(</span><span class="n">n_failed</span><span class="p">)</span></span>
<span id="70"></span>
<span id="71"></span>
<span id="72"><span class="c1"># In[5]:</span></span>
<span id="73"></span>
<span id="74"></span>
<span id="75"><span class="n">grad_rate</span> <span class="o">=</span> <span class="nb">float</span><span class="p">(</span><span class="n">n_passed</span><span class="p">)</span><span class="o">/</span><span class="nb">float</span><span class="p">(</span><span class="n">n_students</span><span class="p">)</span> <span class="o">*</span> <span class="mi">100</span></span>
<span id="76"></span>
<span id="77"><span class="c1"># Print the results</span></span>
<span id="78"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Total number of students: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_students</span><span class="p">)))</span></span>
<span id="79"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Number of features: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_features</span><span class="p">)))</span></span>
<span id="80"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Number of students who passed: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_passed</span><span class="p">)))</span></span>
<span id="81"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Number of students who failed: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">n_failed</span><span class="p">)))</span></span>
<span id="82"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Graduation rate of the class: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">grad_rate</span><span class="p">)))</span></span>
<span id="83"></span>
<span id="84"></span>
<span id="85"><span class="c1"># ## Preparing the Data</span></span>
<span id="86"><span class="c1"># In this section, we will prepare the data for modeling, training and testing.</span></span>
<span id="87"><span class="c1"># </span></span>
<span id="88"><span class="c1"># ### Identify feature and target columns</span></span>
<span id="89"><span class="c1"># It is often the case that the data you obtain contains non-numeric features. This can be a problem, as most machine learning algorithms expect numeric data to perform computations with.</span></span>
<span id="90"><span class="c1"># </span></span>
<span id="91"><span class="c1"># Run the code cell below to separate the student data into feature and target columns to see if any features are non-numeric.</span></span>
<span id="92"></span>
<span id="93"><span class="c1"># In[6]:</span></span>
<span id="94"></span>
<span id="95"></span>
<span id="96"><span class="c1"># Extract feature columns</span></span>
<span id="97"><span class="n">feature_cols</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">student_data</span><span class="o">.</span><span class="n">columns</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span></span>
<span id="98"></span>
<span id="99"><span class="c1"># Extract target column &#39;passed&#39;</span></span>
<span id="100"><span class="n">target_col</span> <span class="o">=</span> <span class="n">student_data</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> </span>
<span id="101"></span>
<span id="102"><span class="c1"># Show the list of columns</span></span>
<span id="103"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Feature columns:</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">feature_cols</span><span class="p">)))</span></span>
<span id="104"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Target column: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">target_col</span><span class="p">)))</span></span>
<span id="105"></span>
<span id="106"><span class="c1"># Separate the data into feature data and target data (X_all and y_all, respectively)</span></span>
<span id="107"><span class="n">X_all</span> <span class="o">=</span> <span class="n">student_data</span><span class="p">[</span><span class="n">feature_cols</span><span class="p">]</span></span>
<span id="108"><span class="n">y_all</span> <span class="o">=</span> <span class="n">student_data</span><span class="p">[</span><span class="n">target_col</span><span class="p">]</span></span>
<span id="109"></span>
<span id="110"><span class="c1"># Show the feature information by printing the first five rows</span></span>
<span id="111"><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Feature values:&quot;</span><span class="p">)</span></span>
<span id="112"><span class="nb">print</span><span class="p">((</span><span class="n">X_all</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="mi">5</span><span class="p">)))</span></span>
<span id="113"></span>
<span id="114"></span>
<span id="115"><span class="c1"># ### Preprocess Feature Columns</span></span>
<span id="116"><span class="c1"># </span></span>
<span id="117"><span class="c1"># As you can see, there are several non-numeric columns that need to be converted! Many of them are simply `yes`/`no`, e.g. `internet`. These can be reasonably converted into `1`/`0` (binary) values.</span></span>
<span id="118"><span class="c1"># </span></span>
<span id="119"><span class="c1"># Other columns, like `Mjob` and `Fjob`, have more than two values, and are known as _categorical variables_. The recommended way to handle such a column is to create as many columns as possible values (e.g. `Fjob_teacher`, `Fjob_other`, `Fjob_services`, etc.), and assign a `1` to one of them and `0` to all others.</span></span>
<span id="120"><span class="c1"># </span></span>
<span id="121"><span class="c1"># These generated columns are sometimes called _dummy variables_, and we will use the [`pandas.get_dummies()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html?highlight=get_dummies#pandas.get_dummies) function to perform this transformation. Run the code cell below to perform the preprocessing routine discussed in this section.</span></span>
<span id="122"></span>
<span id="123"><span class="c1"># In[7]:</span></span>
<span id="124"></span>
<span id="125"></span>
<span id="126"><span class="k">def</span> <span class="nf">preprocess_features</span><span class="p">(</span><span class="n">X</span><span class="p">):</span></span>
<span id="127">    <span class="sd">&#39;&#39;&#39; Preprocesses the student data and converts non-numeric binary variables into</span></span>
<span id="128"><span class="sd">        binary (0/1) variables. Converts categorical variables into dummy variables. &#39;&#39;&#39;</span></span>
<span id="129">    </span>
<span id="130">    <span class="c1"># Initialize new output DataFrame</span></span>
<span id="131">    <span class="n">output</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">index</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">index</span><span class="p">)</span></span>
<span id="132"></span>
<span id="133">    <span class="c1"># Investigate each feature column for the data</span></span>
<span id="134">    <span class="k">for</span> <span class="n">col</span><span class="p">,</span> <span class="n">col_data</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span></span>
<span id="135">        </span>
<span id="136">        <span class="c1"># If data type is non-numeric, replace all yes/no values with 1/0</span></span>
<span id="137">        <span class="k">if</span> <span class="n">col_data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="nb">object</span><span class="p">:</span></span>
<span id="138">            <span class="n">col_data</span> <span class="o">=</span> <span class="n">col_data</span><span class="o">.</span><span class="n">replace</span><span class="p">([</span><span class="s1">&#39;yes&#39;</span><span class="p">,</span> <span class="s1">&#39;no&#39;</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span></span>
<span id="139"></span>
<span id="140">        <span class="c1"># If data type is categorical, convert to dummy variables</span></span>
<span id="141">        <span class="k">if</span> <span class="n">col_data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="nb">object</span><span class="p">:</span></span>
<span id="142">            <span class="c1"># Example: &#39;school&#39; =&gt; &#39;school_GP&#39; and &#39;school_MS&#39;</span></span>
<span id="143">            <span class="n">col_data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">col_data</span><span class="p">,</span> <span class="n">prefix</span> <span class="o">=</span> <span class="n">col</span><span class="p">)</span>  </span>
<span id="144">        </span>
<span id="145">        <span class="c1"># Collect the revised columns</span></span>
<span id="146">        <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">col_data</span><span class="p">)</span></span>
<span id="147">    </span>
<span id="148">    <span class="k">return</span> <span class="n">output</span></span>
<span id="149"></span>
<span id="150"><span class="n">X_all</span> <span class="o">=</span> <span class="n">preprocess_features</span><span class="p">(</span><span class="n">X_all</span><span class="p">)</span></span>
<span id="151"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Processed feature columns (</span><span class="si">{}</span><span class="s2"> total features):</span><span class="se">\n</span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_all</span><span class="o">.</span><span class="n">columns</span><span class="p">),</span> <span class="nb">list</span><span class="p">(</span><span class="n">X_all</span><span class="o">.</span><span class="n">columns</span><span class="p">))))</span></span>
<span id="152"><span class="n">nomesVar</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">X_all</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span></span>
<span id="153"></span>
<span id="154"></span>
<span id="155"><span class="c1"># In[8]:</span></span>
<span id="156"></span>
<span id="157"></span>
<span id="158"><span class="n">X_all</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></span>
<span id="159"></span>
<span id="160"></span>
<span id="161"><span class="c1"># **Padronizar as variaveis** Para alguns dos algoritmos que utilizaremos é necessário ter as variáveis padronizadas com média igual a 0 e desvio padrão igual a 1.</span></span>
<span id="162"></span>
<span id="163"><span class="c1"># In[9]:</span></span>
<span id="164"></span>
<span id="165"></span>
<span id="166"><span class="n">medias0</span> <span class="o">=</span> <span class="n">X_all</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span></span>
<span id="167"><span class="n">desvios0</span> <span class="o">=</span> <span class="n">X_all</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></span>
<span id="168"><span class="n">eixo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">medias0</span><span class="p">))))</span> <span class="o">+</span> <span class="mi">1</span></span>
<span id="169"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eixo</span><span class="p">,</span> <span class="n">medias0</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">)</span></span>
<span id="170"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;media&#39;</span><span class="p">)</span></span>
<span id="171"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Media das variaveis preditivas nao padronizadas&#39;</span><span class="p">)</span></span>
<span id="172"></span>
<span id="173"></span>
<span id="174"><span class="c1"># In[10]:</span></span>
<span id="175"></span>
<span id="176"></span>
<span id="177"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eixo</span><span class="p">,</span> <span class="n">desvios0</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">)</span></span>
<span id="178"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;desvios padrao&#39;</span><span class="p">)</span></span>
<span id="179"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Desvio padrao das variaveis preditivas nao padronizadas&#39;</span><span class="p">)</span></span>
<span id="180"></span>
<span id="181"></span>
<span id="182"><span class="c1"># In[11]:</span></span>
<span id="183"></span>
<span id="184"></span>
<span id="185"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span></span>
<span id="186"><span class="n">X_all</span> <span class="o">=</span> <span class="n">preprocessing</span><span class="o">.</span><span class="n">scale</span><span class="p">(</span><span class="n">X_all</span><span class="p">)</span></span>
<span id="187"><span class="n">medias0</span> <span class="o">=</span> <span class="n">X_all</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span></span>
<span id="188"><span class="n">desvios0</span> <span class="o">=</span> <span class="n">X_all</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span></span>
<span id="189"><span class="n">eixo</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">medias0</span><span class="p">))))</span> <span class="o">+</span> <span class="mi">1</span></span>
<span id="190"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eixo</span><span class="p">,</span> <span class="n">medias0</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">)</span></span>
<span id="191"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;media&#39;</span><span class="p">)</span></span>
<span id="192"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Media das variaveis preditivas padronizadas&#39;</span><span class="p">)</span></span>
<span id="193"></span>
<span id="194"></span>
<span id="195"><span class="c1"># In[12]:</span></span>
<span id="196"></span>
<span id="197"></span>
<span id="198"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">eixo</span><span class="p">,</span> <span class="n">desvios0</span><span class="p">,</span> <span class="s1">&#39;ro&#39;</span><span class="p">)</span></span>
<span id="199"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;desvios padrao&#39;</span><span class="p">)</span></span>
<span id="200"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Desvio padrao das variaveis preditivas padronizadas&#39;</span><span class="p">)</span></span>
<span id="201"></span>
<span id="202"></span>
<span id="203"><span class="c1"># ### Implementation: Training and Testing Data Split</span></span>
<span id="204"><span class="c1"># So far, we have converted all _categorical_ features into numeric values. For the next step, we split the data (both features and corresponding labels) into training and test sets. In the following code cell below, you will need to implement the following:</span></span>
<span id="205"><span class="c1"># - Randomly shuffle and split the data (`X_all`, `y_all`) into training and testing subsets.</span></span>
<span id="206"><span class="c1">#   - Use 300 training points (approximately 75%) and 95 testing points (approximately 25%).</span></span>
<span id="207"><span class="c1">#   - Set a `random_state` for the function(s) you use, if provided.</span></span>
<span id="208"><span class="c1">#   - Store the results in `X_train`, `X_test`, `y_train`, and `y_test`.</span></span>
<span id="209"></span>
<span id="210"><span class="c1"># In[33]:</span></span>
<span id="211"></span>
<span id="212"></span>
<span id="213"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span></span>
<span id="214"><span class="c1"># TODO: Import any additional functionality you may need here</span></span>
<span id="215"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span>  <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X_all</span><span class="p">,</span> <span class="n">y_all</span><span class="p">,</span><span class="n">test_size</span><span class="o">=</span><span class="mf">0.24</span><span class="p">,</span> </span>
<span id="216">                                                     <span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y_all</span><span class="p">)</span></span>
<span id="217"></span>
<span id="218"><span class="c1"># Show the results of the split</span></span>
<span id="219"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Training set has </span><span class="si">{}</span><span class="s2"> samples.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span></span>
<span id="220"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Testing set has </span><span class="si">{}</span><span class="s2"> samples.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span></span>
<span id="221"></span>
<span id="222"></span>
<span id="223"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Taxa de graduados no conjunto de treinamento: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="s1">&#39;yes&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())))</span></span>
<span id="224"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Taxa de graduados no conjunto de teste: </span><span class="si">{:.2f}</span><span class="s2">%&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="mi">100</span> <span class="o">*</span> <span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="s1">&#39;yes&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">())))</span></span>
<span id="225"></span>
<span id="226"></span>
<span id="227"><span class="c1"># ## Training and Evaluating Models</span></span>
<span id="228"><span class="c1"># In this section, you will choose 3 supervised learning models that are appropriate for this problem and available in `scikit-learn`. You will first discuss the reasoning behind choosing these three models by considering what you know about the data and each model&#39;s strengths and weaknesses. You will then fit the model to varying sizes of training data (100 data points, 200 data points, and 300 data points) and measure the F&lt;sub&gt;1&lt;/sub&gt; score. You will need to produce three tables (one for each model) that shows the training set size, training time, prediction time, F&lt;sub&gt;1&lt;/sub&gt; score on the training set, and F&lt;sub&gt;1&lt;/sub&gt; score on the testing set.</span></span>
<span id="229"><span class="c1"># </span></span>
<span id="230"><span class="c1"># **The following supervised learning models are currently available in** [`scikit-learn`](http://scikit-learn.org/stable/supervised_learning.html) **that you may choose from:**</span></span>
<span id="231"><span class="c1"># - Gaussian Naive Bayes (GaussianNB)</span></span>
<span id="232"><span class="c1"># - Decision Trees</span></span>
<span id="233"><span class="c1"># - Ensemble Methods (Bagging, AdaBoost, Random Forest, Gradient Boosting)</span></span>
<span id="234"><span class="c1"># - K-Nearest Neighbors (KNeighbors)</span></span>
<span id="235"><span class="c1"># - Stochastic Gradient Descent (SGDC)</span></span>
<span id="236"><span class="c1"># - Support Vector Machines (SVM)</span></span>
<span id="237"><span class="c1"># - Logistic Regression</span></span>
<span id="238"></span>
<span id="239"><span class="c1"># ### Question 2 - Model Application</span></span>
<span id="240"><span class="c1"># *List three supervised learning models that are appropriate for this problem. For each model chosen*</span></span>
<span id="241"><span class="c1"># - Describe one real-world application in industry where the model can be applied. *(You may need to do a small bit of research for this — give references!)* </span></span>
<span id="242"><span class="c1"># - What are the strengths of the model; when does it perform well? </span></span>
<span id="243"><span class="c1"># - What are the weaknesses of the model; when does it perform poorly?</span></span>
<span id="244"><span class="c1"># - What makes this model a good candidate for the problem, given what you know about the data?</span></span>
<span id="245"></span>
<span id="246"><span class="c1"># **Answer: **</span></span>
<span id="247"><span class="c1"># </span></span>
<span id="248"><span class="c1"># 395 observacoes sendo 300 para training e 95 para teste. 48 features para previsao</span></span>
<span id="249"><span class="c1"># </span></span>
<span id="250"><span class="c1"># **Gaussian Naive Bayes**: </span></span>
<span id="251"><span class="c1"># Esse modelo é utilizado normalmente em análise de texto: como prever se um e-mail é ou não é *spam*.</span></span>
<span id="252"><span class="c1"># http://sebastianraschka.com/Articles/2014_naive_bayes_1.html#the-decision-rule-for-spam-classification</span></span>
<span id="253"><span class="c1"># https://en.wikipedia.org/wiki/Naive_Bayes_spam_filtering</span></span>
<span id="254"><span class="c1"># </span></span>
<span id="255"><span class="c1"># As vantagens desse modelos é que ele é simples e, por isso, seu algoritmo é fácil de ser implementado e funciona melhor, que outros modelos mais complexos, quando temos poucos dados e muitas variáveis.</span></span>
<span id="256"><span class="c1"># Uma desvantagem desse método é ele assumir que as variáveis previsoras são independentes condicionado a informação da variável resposta. Isso nunca é verdadeiro mas algumas vezes essa dependência condicional é irrelevante e a facilidade de implementação, principalmente quando temos muitas variáveis e poucas observações, supera essa desvantagem.</span></span>
<span id="257"><span class="c1"># Outra desvantagem desse algoritmo é que ele não captura não linearidades na dependência entre as variáveis preditivas contínuas e a variável dependente.</span></span>
<span id="258"><span class="c1"># </span></span>
<span id="259"><span class="c1"># Para esse modelo, se as variáveis contínuas não tiverem distribuição normal, é recomendado que elas sejam normalizadas. O algoritmo assume que a distribuição da variável contínua dado a variável resposta possui distribuição normal .</span></span>
<span id="260"><span class="c1"># </span></span>
<span id="261"><span class="c1"># No nosso modelo a hipotese de independência condicional é falsa, por exemplo, dado que o aluno passou ou não, a informaão da quantidade de anos de educação do pai ainda contém informação a respeito da quantidade de anos de educação da mãe. No entanto, temos também poucos dados (695 observações).Como há poucos dados para muitas variáveis preditivas (41) esse modelo pode ser bom para o problema desse caso.</span></span>
<span id="262"><span class="c1"># </span></span>
<span id="263"><span class="c1"># **Logistic Regression**</span></span>
<span id="264"><span class="c1"># Esse modelo é utilizado na indústria em análise de concessão de crédito. Prevendo a probabilidade do cliente não honrar a dívida.</span></span>
<span id="265"><span class="c1"># https://www2.deloitte.com/content/dam/Deloitte/global/Documents/Financial-Services/gx-be-aers-fsi-credit-scoring.pdf</span></span>
<span id="266"><span class="c1"># As vantagens desse modelo é que ele é simples e rápido de ser implementado e interpretado.</span></span>
<span id="267"><span class="c1"># </span></span>
<span id="268"><span class="c1"># Por ser simples o modelos está menos sujeito ao risco de overfitting dos dados. Como o modelo é totalmente paramétrico e estima uma equação que explica a relação entre as variáveis preditivas e a variável dependente, é muito fácil interpretar o modelo. Isso facilita a visualização e o entendimento das previsões feitas pelo modelo estimado e, dessa forma, facilita saber se o modelo está se comportando conforme o esperado e facilita apresentar os resultados desse para pessoas leigas.</span></span>
<span id="269"><span class="c1"># </span></span>
<span id="270"><span class="c1"># As desvantagens é que por ser simples é difícil de fazê-lo capturar não linearidades existentes nos dados. Para isso é necessário inserir as variáveis preditivas e suas não linearidades (exemplo polinomio dela) explicitamente no modelo já que, a estimação não resulta em um algoritmo que torne o modelo estimado flexível o suficiente para capturar não linearidades.</span></span>
<span id="271"><span class="c1"># </span></span>
<span id="272"><span class="c1"># Esse algoritmo pode ser o ideal para nossos dados porque é fácil de interpretar seus resultados e nosso modelo possui poucas observação e muitas variáveis previsoras (41) então um modelo muito flexível possui um maior risco de overfitting.</span></span>
<span id="273"><span class="c1"># </span></span>
<span id="274"><span class="c1"># **knn**</span></span>
<span id="275"><span class="c1"># knn é um algoritmo utilizado quando queremos classificar os itens por similaridade. Nesse contexo ele é utilizado na detecção de falhas pois a partir de uma amostra inicial que já sabemos o resultado da variável dependente podemos classificar novos itens por similaridade.</span></span>
<span id="276"><span class="c1"># http://fumblog.um.ac.ir/gallery/170/Fault%20Detection%20Using%20the%20k-Nearest%20Neighbor%20Rule%20for%20Semiconductor%20Manufacturing%20Processes.pdf</span></span>
<span id="277"><span class="c1"># </span></span>
<span id="278"><span class="c1"># A vantagem do knn é que não é necessário treinar o modelo, as previsões de novos dados são feitas simplesmente comparando as novas amostras com as já observadas e classificando-as de acordo com a similaridade. Dessa maneira é possível construir previsões a partir de poucas observações. Outra vantagem é que é fácil de entender as previsões realizadas pelo modelo uma vez que a regra de classificação é bem simples. Além disso, o algoritmo tem capacidade de produzir modelos preditivos totalmente não lineares já que é um modelo não paramétrico.</span></span>
<span id="279"><span class="c1"># </span></span>
<span id="280"><span class="c1"># A desvantagem desse método é que ele pode ser lento para fazer previsões, como não utiliza uma equação paramétrica para fazer previsão é preciso calcular a distância entre a nova observação e todas outras da amostra de treino. Uma outra desvantagem é que por ser muito flexível o modelo é mais sujeito a overfitting. Além disso, como o algoritmo depende de uma medidade de distância entre os pontos é necessário normalizar as variáveis. </span></span>
<span id="281"><span class="c1"># </span></span>
<span id="282"><span class="c1"># Esse algoritmo, ao contrário, dos apresentados anteriormente é muito flexível e seu algoritmo tem o potencial de capturar uma não linearidade na relação enre as variáveis preditivas e a variável dependente, por isso, se isso for algo importante nos nossos dados esse algoritmo se dará melhor que os anteriores na tarefa de previsão.</span></span>
<span id="283"><span class="c1"># </span></span>
<span id="284"><span class="c1"># </span></span>
<span id="285"><span class="c1"># </span></span>
<span id="286"><span class="c1"># </span></span>
<span id="287"><span class="c1"># </span></span>
<span id="288"><span class="c1"># </span></span>
<span id="289"><span class="c1"># </span></span>
<span id="290"></span>
<span id="291"><span class="c1"># ### Setup</span></span>
<span id="292"><span class="c1"># Run the code cell below to initialize three helper functions which you can use for training and testing the three supervised learning models you&#39;ve chosen above. The functions are as follows:</span></span>
<span id="293"><span class="c1"># - `train_classifier` - takes as input a classifier and training data and fits the classifier to the data.</span></span>
<span id="294"><span class="c1"># - `predict_labels` - takes as input a fit classifier, features, and a target labeling and makes predictions using the F&lt;sub&gt;1&lt;/sub&gt; score.</span></span>
<span id="295"><span class="c1"># - `train_predict` - takes as input a classifier, and the training and testing data, and performs `train_clasifier` and `predict_labels`.</span></span>
<span id="296"><span class="c1">#  - This function will report the F&lt;sub&gt;1&lt;/sub&gt; score for both the training and testing data separately.</span></span>
<span id="297"></span>
<span id="298"><span class="c1"># In[34]:</span></span>
<span id="299"></span>
<span id="300"></span>
<span id="301"><span class="k">def</span> <span class="nf">train_classifier</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span></span>
<span id="302">    <span class="sd">&#39;&#39;&#39; Fits a classifier to the training data. &#39;&#39;&#39;</span></span>
<span id="303">    </span>
<span id="304">    <span class="c1"># Start the clock, train the classifier, then stop the clock</span></span>
<span id="305">    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span></span>
<span id="306">    <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">train</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([306, 317])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">no independent test data</button>
<span id="307">    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span></span>
<span id="308">    </span>
<span id="309">    <span class="c1"># Print the results</span></span>
<span id="310">    <span class="k">return</span> <span class="nb">round</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">,</span><span class="mi">3</span><span class="p">)</span></span>
<span id="311">    </span>
<span id="312"><span class="k">def</span> <span class="nf">predict_labels</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span></span>
<span id="313">    <span class="sd">&#39;&#39;&#39; Makes predictions using a fit classifier based on F1 score. &#39;&#39;&#39;</span></span>
<span id="314">    </span>
<span id="315">    <span class="c1"># Start the clock, make predictions, then stop the clock</span></span>
<span id="316">    <span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span></span>
<span id="317">    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">features</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">validation</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([317, 517])">highlight train/test sites</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([317, 543])">highlight train/test sites</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([306, 317])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">potential preprocessing leakage</button> <a href="#166"><button type="button" style="line-height: 85%; None" onclick="mark_leak_lines([166, 167, 186])">show and go to first leak src</button></a> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">used multiple times</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([317])">highlight other usage</button>
<span id="318">    <span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span></span>
<span id="319">    </span>
<span id="320">    <span class="c1"># Print and return results</span></span>
<span id="321">    <span class="k">return</span> <span class="p">[</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">,</span> <span class="n">f1_score</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">values</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">pos_label</span><span class="o">=</span><span class="s1">&#39;yes&#39;</span><span class="p">)]</span></span>
<span id="322"></span>
<span id="323"></span>
<span id="324"><span class="k">def</span> <span class="nf">train_predict</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">):</span></span>
<span id="325">    <span class="sd">&#39;&#39;&#39; Train and predict using a classifer based on F1 score. &#39;&#39;&#39;</span></span>
<span id="326">    </span>
<span id="327">    <span class="c1"># Indicate the classifier and the training set size    </span></span>
<span id="328">    <span class="c1"># Train the classifier</span></span>
<span id="329">    <span class="n">tempoTreinamento</span> <span class="o">=</span> <span class="n">train_classifier</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></span>
<span id="330">    <span class="n">tempoPrevTreino</span><span class="p">,</span> <span class="n">prevTreino</span> <span class="o">=</span> <span class="n">predict_labels</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></span>
<span id="331">    <span class="n">tempoPrevTeste</span><span class="p">,</span> <span class="n">prevTeste</span> <span class="o">=</span> <span class="n">predict_labels</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span></span>
<span id="332">    </span>
<span id="333">    <span class="c1"># Print the results of prediction for both training and testing</span></span>
<span id="334">    <span class="k">return</span> <span class="p">[</span><span class="n">clf</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> </span>
<span id="335">            <span class="nb">round</span><span class="p">(</span><span class="n">tempoTreinamento</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span><span class="nb">round</span><span class="p">(</span><span class="n">prevTreino</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span><span class="nb">round</span><span class="p">(</span><span class="n">tempoPrevTeste</span><span class="p">,</span><span class="mi">3</span><span class="p">),</span></span>
<span id="336">            <span class="nb">round</span><span class="p">(</span><span class="n">prevTeste</span><span class="p">,</span><span class="mi">3</span><span class="p">)]</span></span>
<span id="337"></span>
<span id="338"></span>
<span id="339"><span class="c1"># ### Implementation: Model Performance Metrics</span></span>
<span id="340"><span class="c1"># With the predefined functions above, you will now import the three supervised learning models of your choice and run the `train_predict` function for each one. Remember that you will need to train and predict on each classifier for three different training set sizes: 100, 200, and 300. Hence, you should expect to have 9 different outputs below — 3 for each model using the varying training set sizes. In the following code cell, you will need to implement the following:</span></span>
<span id="341"><span class="c1"># - Import the three supervised learning models you&#39;ve discussed in the previous section.</span></span>
<span id="342"><span class="c1"># - Initialize the three models and store them in `clf_A`, `clf_B`, and `clf_C`.</span></span>
<span id="343"><span class="c1">#  - Use a `random_state` for each model you use, if provided.</span></span>
<span id="344"><span class="c1">#  - **Note:** Use the default settings for each model — you will tune one specific model in a later section.</span></span>
<span id="345"><span class="c1"># - Create the different training set sizes to be used to train each model.</span></span>
<span id="346"><span class="c1">#  - *Do not reshuffle and resplit the data! The new training points should be drawn from `X_train` and `y_train`.*</span></span>
<span id="347"><span class="c1"># - Fit each model with each training set size and make predictions on the test set (9 in total).  </span></span>
<span id="348"><span class="c1"># **Note:** Three tables are provided after the following code cell which can be used to store your results.</span></span>
<span id="349"></span>
<span id="350"><span class="c1"># In[60]:</span></span>
<span id="351"></span>
<span id="352"></span>
<span id="353"><span class="c1"># TODO: Import the three supervised learning models from sklearn</span></span>
<span id="354"><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span></span>
<span id="355"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span></span>
<span id="356"><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span></span>
<span id="357"></span>
<span id="358"><span class="c1"># TODO: Initialize the three models</span></span>
<span id="359"><span class="n">clf_A</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span></span>
<span id="360"><span class="n">clf_B</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">123</span><span class="p">)</span></span>
<span id="361"><span class="n">clf_C</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span></span>
<span id="362"></span>
<span id="363"><span class="n">resultado</span> <span class="o">=</span> <span class="p">[]</span></span>
<span id="364"><span class="n">scoreTrain</span> <span class="o">=</span> <span class="p">[]</span></span>
<span id="365"><span class="n">scoreTeste</span> <span class="o">=</span> <span class="p">[]</span></span>
<span id="366"><span class="n">nObs</span> <span class="o">=</span> <span class="p">[]</span></span>
<span id="367"><span class="n">nomeModelo</span> <span class="o">=</span> <span class="p">[]</span></span>
<span id="368"></span>
<span id="369"><span class="k">for</span> <span class="n">modelo</span> <span class="ow">in</span> <span class="p">[</span><span class="n">clf_A</span><span class="p">,</span> <span class="n">clf_B</span><span class="p">,</span> <span class="n">clf_C</span><span class="p">]:</span></span>
<span id="370">    <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">300</span><span class="p">]:</span></span>
<span id="371">        <span class="n">x</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span></span>
<span id="372">        <span class="n">y</span> <span class="o">=</span> <span class="n">y_train</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span></span>
<span id="373">        <span class="n">temp</span> <span class="o">=</span> <span class="n">train_predict</span><span class="p">(</span><span class="n">modelo</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y_test</span><span class="p">)</span></span>
<span id="374">        <span class="n">resultado</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span></span>
<span id="375">        <span class="n">scoreTrain</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span></span>
<span id="376">        <span class="n">scoreTeste</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp</span><span class="p">[</span><span class="mi">5</span><span class="p">])</span></span>
<span id="377">        <span class="n">nomeModelo</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">temp</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span></span>
<span id="378">        <span class="n">nObs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">n</span><span class="p">)</span></span>
<span id="379"></span>
<span id="380"><span class="nb">print</span><span class="p">(</span><span class="n">resultado</span><span class="p">)</span>      </span>
<span id="381"></span>
<span id="382"></span>
<span id="383"><span class="c1"># In[62]:</span></span>
<span id="384"></span>
<span id="385"></span>
<span id="386"><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span></span>
<span id="387">    <span class="p">{</span><span class="s1">&#39;nomeModelo&#39;</span><span class="p">:</span> <span class="n">nomeModelo</span><span class="p">,</span></span>
<span id="388">     <span class="s1">&#39;scoreTrain&#39;</span><span class="p">:</span> <span class="n">scoreTrain</span><span class="p">,</span></span>
<span id="389">     <span class="s1">&#39;scoreTeste&#39;</span><span class="p">:</span> <span class="n">scoreTeste</span><span class="p">,</span></span>
<span id="390">     <span class="s1">&#39;nObs&#39;</span><span class="p">:</span> <span class="n">nObs</span></span>
<span id="391">    <span class="p">})</span></span>
<span id="392"></span>
<span id="393"><span class="nb">print</span><span class="p">(</span><span class="n">df</span><span class="p">)</span></span>
<span id="394"></span>
<span id="395"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span></span>
<span id="396"></span>
<span id="397"><span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s2">&quot;nObs&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s2">&quot;scoreTeste&quot;</span><span class="p">,</span> <span class="n">hue</span><span class="o">=</span><span class="s2">&quot;nomeModelo&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">);</span></span>
<span id="398"></span>
<span id="399"></span>
<span id="400"><span class="c1"># ### Tabular Results</span></span>
<span id="401"><span class="c1"># Edit the cell below to see how a table can be designed in [Markdown](https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet#tables). You can record your results from above in the tables provided.</span></span>
<span id="402"></span>
<span id="403"><span class="c1"># ** Classifer 1 - Gaussian NB**  </span></span>
<span id="404"><span class="c1"># </span></span>
<span id="405"><span class="c1"># | Training Set Size | Training Time | Prediction Time (test) | F1 Score (train) | F1 Score (test) |</span></span>
<span id="406"><span class="c1"># | :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |</span></span>
<span id="407"><span class="c1"># | 100               |          0.004s         |            0.001s            |     0.829             |     0.703            |</span></span>
<span id="408"><span class="c1"># | 200               |        0.004s          |             0.001s           |        0.753          |        0.689         |</span></span>
<span id="409"><span class="c1"># | 300               |         0.012s                |      0.001s                  |      0.788            |    0.754      |</span></span>
<span id="410"><span class="c1"># </span></span>
<span id="411"><span class="c1"># ** Classifer 2 - Logistic Regression**  </span></span>
<span id="412"><span class="c1"># </span></span>
<span id="413"><span class="c1"># | Training Set Size | Training Time | Prediction Time (test) | F1 Score (train) | F1 Score (test) |</span></span>
<span id="414"><span class="c1"># | :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |</span></span>
<span id="415"><span class="c1"># | 100               |        0.038s                 |         0s               |       0.885           |     0.729            |</span></span>
<span id="416"><span class="c1"># | 200               |     0.001s             |                0s        |     0.840             |      0.731           |</span></span>
<span id="417"><span class="c1"># | 300               |       0.01s                  |         0s             |           0.844       |     0.731    |</span></span>
<span id="418"><span class="c1"># </span></span>
<span id="419"><span class="c1"># ** Classifer 3 - k nearest**  </span></span>
<span id="420"><span class="c1"># </span></span>
<span id="421"><span class="c1"># | Training Set Size | Training Time | Prediction Time (test) | F1 Score (train) | F1 Score (test) |</span></span>
<span id="422"><span class="c1"># | :---------------: | :---------------------: | :--------------------: | :--------------: | :-------------: |</span></span>
<span id="423"><span class="c1"># | 100               |         0.004s                |           0.006s             |         0.862       |        0.735         |</span></span>
<span id="424"><span class="c1"># | 200               |         0.002s                |            0.008s            |         0.844         |      0.776           |</span></span>
<span id="425"><span class="c1"># | 300               |         0.003s                |           0.013s             |         0.836         |      0.763          |</span></span>
<span id="426"></span>
<span id="427"><span class="c1"># ## Choosing the Best Model</span></span>
<span id="428"><span class="c1"># In this final section, you will choose from the three supervised learning models the *best* model to use on the student data. You will then perform a grid search optimization for the model over the entire training set (`X_train` and `y_train`) by tuning at least one parameter to improve upon the untuned model&#39;s F&lt;sub&gt;1&lt;/sub&gt; score. </span></span>
<span id="429"></span>
<span id="430"><span class="c1"># ### Question 3 - Choosing the Best Model</span></span>
<span id="431"><span class="c1"># *Based on the experiments you performed earlier, in one to two paragraphs, explain to the board of supervisors what single model you chose as the best model. Which model is generally the most appropriate based on the available data, limited resources, cost, and performance?*</span></span>
<span id="432"></span>
<span id="433"><span class="c1"># **Answer: **</span></span>
<span id="434"><span class="c1"># O modelo escolhido foi o k nearest neighbour. O modelo foi o melhor independente do tamanho da amostra de treino (n), como mostrado no gráfico de barras.</span></span>
<span id="435"><span class="c1"># Aparentemente a regressão logística não tem ganhos quando aumentamos o tamanho da amostra. Provavelmente por se tratar de um modelo muito simples para a complexidade dos dados então o aumento da amostra não está resultando em ganhos no F1 score para os dados de teste.</span></span>
<span id="436"><span class="c1"># Se obtivermos mais observações, vale a pena continuar testando com o modelo Gaussian Naive Bayes pois, como pode ser visto no gráfico, esse modelo está tendo ganhos no F1 score da amostra de teste quando aumentamos a quantidade de observações usadas na estimação.</span></span>
<span id="437"><span class="c1"># No entanto, para os tamanhos de amostra disponíveis o melhor modelo é o knn. </span></span>
<span id="438"><span class="c1"># Quanto ao tempo de estimação e previsão nenhum dos modelos se mostrou inferior visto que na amostra desse tamanho todos os modelos rodaram com grande rapidez e facilidade.</span></span>
<span id="439"></span>
<span id="440"><span class="c1"># ### Question 4 - Model in Layman&#39;s Terms</span></span>
<span id="441"><span class="c1"># *In one to two paragraphs, explain to the board of directors in layman&#39;s terms how the final model chosen is supposed to work. Be sure that you are describing the major qualities of the model, such as how the model is trained and how the model makes a prediction. Avoid using advanced mathematical or technical jargon, such as describing equations or discussing the algorithm implementation.*</span></span>
<span id="442"></span>
<span id="443"><span class="c1"># **Answer: **</span></span>
<span id="444"><span class="c1"># O modelo escolhido k nearest neighbour funciona de uma maneira bem simples, para toda nova observação analisamos a semelhança das características dela com as características das observações das quais temos a informação do resultado final (passou ou falhou).</span></span>
<span id="445"><span class="c1"># </span></span>
<span id="446"><span class="c1"># Essa semelhança entre as observações é feita de acordo com uma métrica de distância nas variáveis normalizadas para que a unidade da variável não afete a previsão.</span></span>
<span id="447"><span class="c1"># </span></span>
<span id="448"><span class="c1"># Exemplo:</span></span>
<span id="449"><span class="c1"># </span></span>
<span id="450"><span class="c1"># Suponha que tenhamos apenas duas variáveis previsoras idade e gênero e tenhamos as seguintes observações:</span></span>
<span id="451"><span class="c1"># </span></span>
<span id="452"><span class="c1"># * idade 14, absences 1, passou</span></span>
<span id="453"><span class="c1"># * idade 16, absences 10, falhou</span></span>
<span id="454"><span class="c1"># * idade 15, absences 0, falhou</span></span>
<span id="455"><span class="c1"># * idade 15, absences 20, falhou</span></span>
<span id="456"><span class="c1"># </span></span>
<span id="457"><span class="c1"># E queremos prever a observação abaixo</span></span>
<span id="458"><span class="c1"># </span></span>
<span id="459"><span class="c1"># * idade 14, absences 0</span></span>
<span id="460"><span class="c1"># </span></span>
<span id="461"><span class="c1"># As observações mais semelhantes são: </span></span>
<span id="462"><span class="c1"># </span></span>
<span id="463"><span class="c1"># * idade 14, absences 1, passou</span></span>
<span id="464"><span class="c1"># * idade 15, absences 0, falhou</span></span>
<span id="465"><span class="c1"># </span></span>
<span id="466"><span class="c1"># Note que a diferença dos valores das variáveis dessa nova observação é igual a 1 quando comparada com os valores já observados acima destacados.</span></span>
<span id="467"><span class="c1"># </span></span>
<span id="468"><span class="c1"># Para continuar essa comparação (e decidir com quem a nova observação é mais semelhante) temos que normalizar as variáveis porque a variável absences (varia entre 0 e 20) tem uma variação muito maior que idade (varia entre 14 e 16).</span></span>
<span id="469"><span class="c1"># </span></span>
<span id="470"><span class="c1"># Após a normalização a nova observação será mais semelhante à observação:</span></span>
<span id="471"><span class="c1"># </span></span>
<span id="472"><span class="c1"># * idade 14, absences 1, passou</span></span>
<span id="473"><span class="c1"># </span></span>
<span id="474"><span class="c1"># Pois a diferença de idade é mais relevante que a diferença em absences uma vez que a variância da última é menor.</span></span>
<span id="475"><span class="c1"># </span></span>
<span id="476"><span class="c1"># Nesse caso iremos prever que o aluno passou visto que o novo dado é mais semelhante com uma observação que sabemos que passou.</span></span>
<span id="477"><span class="c1"># </span></span>
<span id="478"><span class="c1"># No exemplo acima descrevemos o algoritmo como se k igual a 1, ou seja, só levamos em consideração a observação mais &quot;similar&quot; para fazer a previsão. No entanto, o algoritmo permite ajustar esse parâmetro k e dessa maneira, por exemplo, fazer a previsão levando em consideração não só a observação mais semelhante mas sim as k observações mais semelhantes. Dessa maneira avaliamos se a maioria das observações semelhantes é passou ou não passou.</span></span>
<span id="479"><span class="c1"># </span></span>
<span id="480"><span class="c1"># O resultado do modelo apresentado acima foi obtido usando-se k igual a 5.</span></span>
<span id="481"><span class="c1"># </span></span>
<span id="482"></span>
<span id="483"><span class="c1"># ### Implementation: Model Tuning</span></span>
<span id="484"><span class="c1"># Fine tune the chosen model. Use grid search (`GridSearchCV`) with at least one important parameter tuned with at least 3 different values. You will need to use the entire training set for this. In the code cell below, you will need to implement the following:</span></span>
<span id="485"><span class="c1"># - Import [`sklearn.grid_search.gridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) and [`sklearn.metrics.make_scorer`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html).</span></span>
<span id="486"><span class="c1"># - Create a dictionary of parameters you wish to tune for the chosen model.</span></span>
<span id="487"><span class="c1">#  - Example: `parameters = {&#39;parameter&#39; : [list of values]}`.</span></span>
<span id="488"><span class="c1"># - Initialize the classifier you&#39;ve chosen and store it in `clf`.</span></span>
<span id="489"><span class="c1"># - Create the F&lt;sub&gt;1&lt;/sub&gt; scoring function using `make_scorer` and store it in `f1_scorer`.</span></span>
<span id="490"><span class="c1">#  - Set the `pos_label` parameter to the correct value!</span></span>
<span id="491"><span class="c1"># - Perform grid search on the classifier `clf` using `f1_scorer` as the scoring method, and store it in `grid_obj`.</span></span>
<span id="492"><span class="c1"># - Fit the grid search object to the training data (`X_train`, `y_train`), and store it in `grid_obj`.</span></span>
<span id="493"></span>
<span id="494"><span class="c1"># In[74]:</span></span>
<span id="495"></span>
<span id="496"></span>
<span id="497"><span class="c1"># TODO: Import &#39;GridSearchCV&#39; and &#39;make_scorer&#39;</span></span>
<span id="498"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">ShuffleSplit</span></span>
<span id="499"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">make_scorer</span></span>
<span id="500"><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span></span>
<span id="501"><span class="c1"># TODO: Create the parameters list you wish to tune</span></span>
<span id="502"><span class="n">parameters</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;n_neighbors&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">6</span><span class="p">,</span><span class="mi">7</span><span class="p">,</span><span class="mi">8</span><span class="p">,</span><span class="mi">9</span><span class="p">,</span><span class="mi">10</span><span class="p">]</span> <span class="p">}</span></span>
<span id="503"></span>
<span id="504"><span class="n">cv_sets</span> <span class="o">=</span> <span class="n">ShuffleSplit</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">n_iter</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.10</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span></span>
<span id="505"></span>
<span id="506"><span class="c1"># TODO: Initialize the classifier</span></span>
<span id="507"><span class="n">clf</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span></span>
<span id="508"></span>
<span id="509"><span class="c1"># TODO: Make an f1 scoring function using &#39;make_scorer&#39; </span></span>
<span id="510"><span class="n">f1_scorer</span> <span class="o">=</span> <span class="n">make_scorer</span><span class="p">(</span><span class="n">f1_score</span><span class="p">,</span> <span class="n">pos_label</span> <span class="o">=</span> <span class="s2">&quot;yes&quot;</span><span class="p">)</span></span>
<span id="511"></span>
<span id="512"><span class="c1"># TODO: Perform grid search on the classifier using the f1_scorer as the scoring method</span></span>
<span id="513"><span class="n">grid_obj</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">clf</span><span class="p">,</span> <span class="n">param_grid</span> <span class="o">=</span> <span class="n">parameters</span><span class="p">,</span> <span class="n">scoring</span> <span class="o">=</span> <span class="n">f1_scorer</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="n">cv_sets</span><span class="p">)</span></span>
<span id="514"></span>
<span id="515"></span>
<span id="516"><span class="c1"># TODO: Fit the grid search object to the training data and find the optimal parameters</span></span>
<span id="517"><span class="n">grid_obj</span> <span class="o">=</span> <span class="n">grid_obj</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">train</button> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">validation</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([317, 517])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">no independent test data</button>
<span id="518"></span>
<span id="519"><span class="c1"># Get the estimator</span></span>
<span id="520"><span class="n">clf</span> <span class="o">=</span> <span class="n">grid_obj</span><span class="o">.</span><span class="n">best_estimator_</span></span>
<span id="521"><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="p">)</span></span>
<span id="522"></span>
<span id="523"></span>
<span id="524"><span class="c1"># In[67]:</span></span>
<span id="525"></span>
<span id="526"></span>
<span id="527"><span class="n">tempo</span><span class="p">,</span> <span class="n">prevTreino</span> <span class="o">=</span> <span class="n">predict_labels</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></span>
<span id="528"><span class="n">tempo</span><span class="p">,</span> <span class="n">prevTeste</span> <span class="o">=</span> <span class="n">predict_labels</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span></span>
<span id="529"></span>
<span id="530"></span>
<span id="531"><span class="c1"># In[68]:</span></span>
<span id="532"></span>
<span id="533"></span>
<span id="534"><span class="c1"># Report the final F1 score for training and testing after parameter tuning</span></span>
<span id="535"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Tuned model has a training F1 score of </span><span class="si">{:.4f}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prevTreino</span><span class="p">)))</span></span>
<span id="536"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Tuned model has a testing F1 score of </span><span class="si">{:.4f}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prevTeste</span><span class="p">)))</span></span>
<span id="537"></span>
<span id="538"></span>
<span id="539"><span class="c1"># In[72]:</span></span>
<span id="540"></span>
<span id="541"></span>
<span id="542"><span class="n">clf_default</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span></span>
<span id="543"><span class="n">clf_default</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">train</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([317, 543])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">no independent test data</button>
<span id="544"><span class="c1">#Resultados do modelo default</span></span>
<span id="545"><span class="n">tempo</span><span class="p">,</span> <span class="n">prevTreino2</span> <span class="o">=</span> <span class="n">predict_labels</span><span class="p">(</span><span class="n">clf_default</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span></span>
<span id="546"><span class="n">tempo</span><span class="p">,</span> <span class="n">prevTeste2</span> <span class="o">=</span> <span class="n">predict_labels</span><span class="p">(</span><span class="n">clf_default</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span></span>
<span id="547"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Tuned model has a training F1 score of </span><span class="si">{:.4f}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prevTreino2</span><span class="p">)))</span></span>
<span id="548"><span class="nb">print</span><span class="p">((</span><span class="s2">&quot;Tuned model has a testing F1 score of </span><span class="si">{:.4f}</span><span class="s2">.&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">prevTeste2</span><span class="p">)))</span></span>
<span id="549"></span>
<span id="550"></span>
<span id="551"><span class="c1"># In[73]:</span></span>
<span id="552"></span>
<span id="553"></span>
<span id="554"><span class="nb">print</span><span class="p">((</span><span class="n">clf</span><span class="o">.</span><span class="n">get_params</span><span class="p">))</span></span>
<span id="555"><span class="nb">print</span><span class="p">((</span><span class="n">clf_default</span><span class="o">.</span><span class="n">get_params</span><span class="p">))</span></span>
<span id="556"></span>
<span id="557"></span>
<span id="558"><span class="c1"># ### Question 5 - Final F&lt;sub&gt;1&lt;/sub&gt; Score</span></span>
<span id="559"><span class="c1"># *What is the final model&#39;s F&lt;sub&gt;1&lt;/sub&gt; score for training and testing? How does that score compare to the untuned model?*</span></span>
<span id="560"></span>
<span id="561"><span class="c1"># **Answer: **</span></span>
<span id="562"><span class="c1"># O modelo escolhido foi o com k igual a 9, ou seja, ele usa as 9 observações mais próximas em que o resultado é conhecido para fazer a previsão da nova observação. a performance do novo modelo no conjunto de dados de teste é de 0.7919 maior que a obtida pelo modelo com k igual a 5 (o default) que foi de 0.7626.</span></span>
<span id="563"></span>
<span id="564"><span class="c1"># &gt; **Note**: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to  </span></span>
<span id="565"><span class="c1"># **File -&gt; Download as -&gt; HTML (.html)**. Include the finished document along with this notebook as your submission.</span></span>
<span id="566"></span>
<span id="567"><span class="c1"># In[ ]:</span></span>
</pre></div>
</td></tr></table></body>
</html>
