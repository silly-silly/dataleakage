<script>
    let highlighted = [];
    function highlight_lines(lines) {
        for (let line of highlighted) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = '';
        }
        highlighted = lines;
        for (let line of highlighted) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = 'yellow';
        }
    }
    let marked = [];
    function mark_leak_lines(lines) {
        for (let line of marked) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = '';
        }
        marked = lines;
        for (let line of marked) {
            let ele = document.getElementById(String(line));
            ele.style.backgroundColor = ele.style.backgroundColor = 'lightgreen';
        }
    }
    function show_infos(lines) {
        for (let line of lines) {
            let ele = document.getElementById(String(line) + "-info");
            if (ele) {
                ele.style.display = ele.style.display == 'none'? '': 'none'
            }
        }
    }
</script>
    <style type="text/css">
    .sum table {
    font-family: arial, sans-serif;
    border-collapse: collapse;
    width: 100%;
    }

    .sum td, .sum th {
    border: 1px solid #dddddd;
    text-align: left;
    padding: 8px;
    }

    .sum tr:hover {background-color: #D6EEEE;}
</style>
<center>
<table class="sum">
  <tbody><tr>
    <th>Leakage</th>
    <th>#Detected</th>
    <th>Locations</th>
  </tr>
  <tr>
    <td>Pre-processing leakage</td>
    <td>0</td>
    <td></td>
  </tr>
  <tr>
    <td>Overlap leakage</td>
    <td>0</td>
    <td></td>
  </tr>
  <tr>
    <td>No independence test data</td>
    <td>1</td>
    <td><a href="#305"><button type="button" style="line-height: 85%; None" onclick="None">305</button></a> <a href="#313"><button type="button" style="line-height: 85%; None" onclick="None">313</button></a> <a href="#321"><button type="button" style="line-height: 85%; None" onclick="None">321</button></a> <a href="#443"><button type="button" style="line-height: 85%; None" onclick="None">443</button></a> <a href="#591"><button type="button" style="line-height: 85%; None" onclick="None">591</button></a></td>
  </tr>
</tbody></table></center>

<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<!--
generated by Pygments <https://pygments.org/>
Copyright 2006-2021 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
-->
<html>
<head>
  <title></title>
  <meta http-equiv="content-type" content="text/html; charset=None">
  <style type="text/css">
/*
generated by Pygments <https://pygments.org/>
Copyright 2006-2021 by the Pygments team.
Licensed under the BSD license, see LICENSE for details.
*/
pre { line-height: 145%; }
td.linenos .normal { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
span.linenos { color: inherit; background-color: transparent; padding-left: 5px; padding-right: 5px; }
td.linenos .special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
span.linenos.special { color: #000000; background-color: #ffffc0; padding-left: 5px; padding-right: 5px; }
body .hll { background-color: #ffffcc }
body { background: #f8f8f8; }
body .c { color: #408080; font-style: italic } /* Comment */
body .err { border: 1px solid #FF0000 } /* Error */
body .k { color: #008000; font-weight: bold } /* Keyword */
body .o { color: #666666 } /* Operator */
body .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
body .cm { color: #408080; font-style: italic } /* Comment.Multiline */
body .cp { color: #BC7A00 } /* Comment.Preproc */
body .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
body .c1 { color: #408080; font-style: italic } /* Comment.Single */
body .cs { color: #408080; font-style: italic } /* Comment.Special */
body .gd { color: #A00000 } /* Generic.Deleted */
body .ge { font-style: italic } /* Generic.Emph */
body .gr { color: #FF0000 } /* Generic.Error */
body .gh { color: #000080; font-weight: bold } /* Generic.Heading */
body .gi { color: #00A000 } /* Generic.Inserted */
body .go { color: #888888 } /* Generic.Output */
body .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
body .gs { font-weight: bold } /* Generic.Strong */
body .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
body .gt { color: #0044DD } /* Generic.Traceback */
body .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
body .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
body .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
body .kp { color: #008000 } /* Keyword.Pseudo */
body .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
body .kt { color: #B00040 } /* Keyword.Type */
body .m { color: #666666 } /* Literal.Number */
body .s { color: #BA2121 } /* Literal.String */
body .na { color: #7D9029 } /* Name.Attribute */
body .nb { color: #008000 } /* Name.Builtin */
body .nc { color: #0000FF; font-weight: bold } /* Name.Class */
body .no { color: #880000 } /* Name.Constant */
body .nd { color: #AA22FF } /* Name.Decorator */
body .ni { color: #999999; font-weight: bold } /* Name.Entity */
body .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
body .nf { color: #0000FF } /* Name.Function */
body .nl { color: #A0A000 } /* Name.Label */
body .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
body .nt { color: #008000; font-weight: bold } /* Name.Tag */
body .nv { color: #19177C } /* Name.Variable */
body .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
body .w { color: #bbbbbb } /* Text.Whitespace */
body .mb { color: #666666 } /* Literal.Number.Bin */
body .mf { color: #666666 } /* Literal.Number.Float */
body .mh { color: #666666 } /* Literal.Number.Hex */
body .mi { color: #666666 } /* Literal.Number.Integer */
body .mo { color: #666666 } /* Literal.Number.Oct */
body .sa { color: #BA2121 } /* Literal.String.Affix */
body .sb { color: #BA2121 } /* Literal.String.Backtick */
body .sc { color: #BA2121 } /* Literal.String.Char */
body .dl { color: #BA2121 } /* Literal.String.Delimiter */
body .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
body .s2 { color: #BA2121 } /* Literal.String.Double */
body .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
body .sh { color: #BA2121 } /* Literal.String.Heredoc */
body .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
body .sx { color: #008000 } /* Literal.String.Other */
body .sr { color: #BB6688 } /* Literal.String.Regex */
body .s1 { color: #BA2121 } /* Literal.String.Single */
body .ss { color: #19177C } /* Literal.String.Symbol */
body .bp { color: #008000 } /* Name.Builtin.Pseudo */
body .fm { color: #0000FF } /* Name.Function.Magic */
body .vc { color: #19177C } /* Name.Variable.Class */
body .vg { color: #19177C } /* Name.Variable.Global */
body .vi { color: #19177C } /* Name.Variable.Instance */
body .vm { color: #19177C } /* Name.Variable.Magic */
body .il { color: #666666 } /* Literal.Number.Integer.Long */

  </style>
</head>
<body>
<h2></h2>

<table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">  1</span>
<span class="normal">  2</span>
<span class="normal">  3</span>
<span class="normal">  4</span>
<span class="normal">  5</span>
<span class="normal">  6</span>
<span class="normal">  7</span>
<span class="normal">  8</span>
<span class="normal">  9</span>
<span class="normal"> 10</span>
<span class="normal"> 11</span>
<span class="normal"> 12</span>
<span class="normal"> 13</span>
<span class="normal"> 14</span>
<span class="normal"> 15</span>
<span class="normal"> 16</span>
<span class="normal"> 17</span>
<span class="normal"> 18</span>
<span class="normal"> 19</span>
<span class="normal"> 20</span>
<span class="normal"> 21</span>
<span class="normal"> 22</span>
<span class="normal"> 23</span>
<span class="normal"> 24</span>
<span class="normal"> 25</span>
<span class="normal"> 26</span>
<span class="normal"> 27</span>
<span class="normal"> 28</span>
<span class="normal"> 29</span>
<span class="normal"> 30</span>
<span class="normal"> 31</span>
<span class="normal"> 32</span>
<span class="normal"> 33</span>
<span class="normal"> 34</span>
<span class="normal"> 35</span>
<span class="normal"> 36</span>
<span class="normal"> 37</span>
<span class="normal"> 38</span>
<span class="normal"> 39</span>
<span class="normal"> 40</span>
<span class="normal"> 41</span>
<span class="normal"> 42</span>
<span class="normal"> 43</span>
<span class="normal"> 44</span>
<span class="normal"> 45</span>
<span class="normal"> 46</span>
<span class="normal"> 47</span>
<span class="normal"> 48</span>
<span class="normal"> 49</span>
<span class="normal"> 50</span>
<span class="normal"> 51</span>
<span class="normal"> 52</span>
<span class="normal"> 53</span>
<span class="normal"> 54</span>
<span class="normal"> 55</span>
<span class="normal"> 56</span>
<span class="normal"> 57</span>
<span class="normal"> 58</span>
<span class="normal"> 59</span>
<span class="normal"> 60</span>
<span class="normal"> 61</span>
<span class="normal"> 62</span>
<span class="normal"> 63</span>
<span class="normal"> 64</span>
<span class="normal"> 65</span>
<span class="normal"> 66</span>
<span class="normal"> 67</span>
<span class="normal"> 68</span>
<span class="normal"> 69</span>
<span class="normal"> 70</span>
<span class="normal"> 71</span>
<span class="normal"> 72</span>
<span class="normal"> 73</span>
<span class="normal"> 74</span>
<span class="normal"> 75</span>
<span class="normal"> 76</span>
<span class="normal"> 77</span>
<span class="normal"> 78</span>
<span class="normal"> 79</span>
<span class="normal"> 80</span>
<span class="normal"> 81</span>
<span class="normal"> 82</span>
<span class="normal"> 83</span>
<span class="normal"> 84</span>
<span class="normal"> 85</span>
<span class="normal"> 86</span>
<span class="normal"> 87</span>
<span class="normal"> 88</span>
<span class="normal"> 89</span>
<span class="normal"> 90</span>
<span class="normal"> 91</span>
<span class="normal"> 92</span>
<span class="normal"> 93</span>
<span class="normal"> 94</span>
<span class="normal"> 95</span>
<span class="normal"> 96</span>
<span class="normal"> 97</span>
<span class="normal"> 98</span>
<span class="normal"> 99</span>
<span class="normal">100</span>
<span class="normal">101</span>
<span class="normal">102</span>
<span class="normal">103</span>
<span class="normal">104</span>
<span class="normal">105</span>
<span class="normal">106</span>
<span class="normal">107</span>
<span class="normal">108</span>
<span class="normal">109</span>
<span class="normal">110</span>
<span class="normal">111</span>
<span class="normal">112</span>
<span class="normal">113</span>
<span class="normal">114</span>
<span class="normal">115</span>
<span class="normal">116</span>
<span class="normal">117</span>
<span class="normal">118</span>
<span class="normal">119</span>
<span class="normal">120</span>
<span class="normal">121</span>
<span class="normal">122</span>
<span class="normal">123</span>
<span class="normal">124</span>
<span class="normal">125</span>
<span class="normal">126</span>
<span class="normal">127</span>
<span class="normal">128</span>
<span class="normal">129</span>
<span class="normal">130</span>
<span class="normal">131</span>
<span class="normal">132</span>
<span class="normal">133</span>
<span class="normal">134</span>
<span class="normal">135</span>
<span class="normal">136</span>
<span class="normal">137</span>
<span class="normal">138</span>
<span class="normal">139</span>
<span class="normal">140</span>
<span class="normal">141</span>
<span class="normal">142</span>
<span class="normal">143</span>
<span class="normal">144</span>
<span class="normal">145</span>
<span class="normal">146</span>
<span class="normal">147</span>
<span class="normal">148</span>
<span class="normal">149</span>
<span class="normal">150</span>
<span class="normal">151</span>
<span class="normal">152</span>
<span class="normal">153</span>
<span class="normal">154</span>
<span class="normal">155</span>
<span class="normal">156</span>
<span class="normal">157</span>
<span class="normal">158</span>
<span class="normal">159</span>
<span class="normal">160</span>
<span class="normal">161</span>
<span class="normal">162</span>
<span class="normal">163</span>
<span class="normal">164</span>
<span class="normal">165</span>
<span class="normal">166</span>
<span class="normal">167</span>
<span class="normal">168</span>
<span class="normal">169</span>
<span class="normal">170</span>
<span class="normal">171</span>
<span class="normal">172</span>
<span class="normal">173</span>
<span class="normal">174</span>
<span class="normal">175</span>
<span class="normal">176</span>
<span class="normal">177</span>
<span class="normal">178</span>
<span class="normal">179</span>
<span class="normal">180</span>
<span class="normal">181</span>
<span class="normal">182</span>
<span class="normal">183</span>
<span class="normal">184</span>
<span class="normal">185</span>
<span class="normal">186</span>
<span class="normal">187</span>
<span class="normal">188</span>
<span class="normal">189</span>
<span class="normal">190</span>
<span class="normal">191</span>
<span class="normal">192</span>
<span class="normal">193</span>
<span class="normal">194</span>
<span class="normal">195</span>
<span class="normal">196</span>
<span class="normal">197</span>
<span class="normal">198</span>
<span class="normal">199</span>
<span class="normal">200</span>
<span class="normal">201</span>
<span class="normal">202</span>
<span class="normal">203</span>
<span class="normal">204</span>
<span class="normal">205</span>
<span class="normal">206</span>
<span class="normal">207</span>
<span class="normal">208</span>
<span class="normal">209</span>
<span class="normal">210</span>
<span class="normal">211</span>
<span class="normal">212</span>
<span class="normal">213</span>
<span class="normal">214</span>
<span class="normal">215</span>
<span class="normal">216</span>
<span class="normal">217</span>
<span class="normal">218</span>
<span class="normal">219</span>
<span class="normal">220</span>
<span class="normal">221</span>
<span class="normal">222</span>
<span class="normal">223</span>
<span class="normal">224</span>
<span class="normal">225</span>
<span class="normal">226</span>
<span class="normal">227</span>
<span class="normal">228</span>
<span class="normal">229</span>
<span class="normal">230</span>
<span class="normal">231</span>
<span class="normal">232</span>
<span class="normal">233</span>
<span class="normal">234</span>
<span class="normal">235</span>
<span class="normal">236</span>
<span class="normal">237</span>
<span class="normal">238</span>
<span class="normal">239</span>
<span class="normal">240</span>
<span class="normal">241</span>
<span class="normal">242</span>
<span class="normal">243</span>
<span class="normal">244</span>
<span class="normal">245</span>
<span class="normal">246</span>
<span class="normal">247</span>
<span class="normal">248</span>
<span class="normal">249</span>
<span class="normal">250</span>
<span class="normal">251</span>
<span class="normal">252</span>
<span class="normal">253</span>
<span class="normal">254</span>
<span class="normal">255</span>
<span class="normal">256</span>
<span class="normal">257</span>
<span class="normal">258</span>
<span class="normal">259</span>
<span class="normal">260</span>
<span class="normal">261</span>
<span class="normal">262</span>
<span class="normal">263</span>
<span class="normal">264</span>
<span class="normal">265</span>
<span class="normal">266</span>
<span class="normal">267</span>
<span class="normal">268</span>
<span class="normal">269</span>
<span class="normal">270</span>
<span class="normal">271</span>
<span class="normal">272</span>
<span class="normal">273</span>
<span class="normal">274</span>
<span class="normal">275</span>
<span class="normal">276</span>
<span class="normal">277</span>
<span class="normal">278</span>
<span class="normal">279</span>
<span class="normal">280</span>
<span class="normal">281</span>
<span class="normal">282</span>
<span class="normal">283</span>
<span class="normal">284</span>
<span class="normal">285</span>
<span class="normal">286</span>
<span class="normal">287</span>
<span class="normal">288</span>
<span class="normal">289</span>
<span class="normal">290</span>
<span class="normal">291</span>
<span class="normal">292</span>
<span class="normal">293</span>
<span class="normal">294</span>
<span class="normal">295</span>
<span class="normal">296</span>
<span class="normal">297</span>
<span class="normal">298</span>
<span class="normal">299</span>
<span class="normal">300</span>
<span class="normal">301</span>
<span class="normal">302</span>
<span class="normal">303</span>
<span class="normal">304</span>
<span class="normal">305</span>
<span class="normal">306</span>
<span class="normal">307</span>
<span class="normal">308</span>
<span class="normal">309</span>
<span class="normal">310</span>
<span class="normal">311</span>
<span class="normal">312</span>
<span class="normal">313</span>
<span class="normal">314</span>
<span class="normal">315</span>
<span class="normal">316</span>
<span class="normal">317</span>
<span class="normal">318</span>
<span class="normal">319</span>
<span class="normal">320</span>
<span class="normal">321</span>
<span class="normal">322</span>
<span class="normal">323</span>
<span class="normal">324</span>
<span class="normal">325</span>
<span class="normal">326</span>
<span class="normal">327</span>
<span class="normal">328</span>
<span class="normal">329</span>
<span class="normal">330</span>
<span class="normal">331</span>
<span class="normal">332</span>
<span class="normal">333</span>
<span class="normal">334</span>
<span class="normal">335</span>
<span class="normal">336</span>
<span class="normal">337</span>
<span class="normal">338</span>
<span class="normal">339</span>
<span class="normal">340</span>
<span class="normal">341</span>
<span class="normal">342</span>
<span class="normal">343</span>
<span class="normal">344</span>
<span class="normal">345</span>
<span class="normal">346</span>
<span class="normal">347</span>
<span class="normal">348</span>
<span class="normal">349</span>
<span class="normal">350</span>
<span class="normal">351</span>
<span class="normal">352</span>
<span class="normal">353</span>
<span class="normal">354</span>
<span class="normal">355</span>
<span class="normal">356</span>
<span class="normal">357</span>
<span class="normal">358</span>
<span class="normal">359</span>
<span class="normal">360</span>
<span class="normal">361</span>
<span class="normal">362</span>
<span class="normal">363</span>
<span class="normal">364</span>
<span class="normal">365</span>
<span class="normal">366</span>
<span class="normal">367</span>
<span class="normal">368</span>
<span class="normal">369</span>
<span class="normal">370</span>
<span class="normal">371</span>
<span class="normal">372</span>
<span class="normal">373</span>
<span class="normal">374</span>
<span class="normal">375</span>
<span class="normal">376</span>
<span class="normal">377</span>
<span class="normal">378</span>
<span class="normal">379</span>
<span class="normal">380</span>
<span class="normal">381</span>
<span class="normal">382</span>
<span class="normal">383</span>
<span class="normal">384</span>
<span class="normal">385</span>
<span class="normal">386</span>
<span class="normal">387</span>
<span class="normal">388</span>
<span class="normal">389</span>
<span class="normal">390</span>
<span class="normal">391</span>
<span class="normal">392</span>
<span class="normal">393</span>
<span class="normal">394</span>
<span class="normal">395</span>
<span class="normal">396</span>
<span class="normal">397</span>
<span class="normal">398</span>
<span class="normal">399</span>
<span class="normal">400</span>
<span class="normal">401</span>
<span class="normal">402</span>
<span class="normal">403</span>
<span class="normal">404</span>
<span class="normal">405</span>
<span class="normal">406</span>
<span class="normal">407</span>
<span class="normal">408</span>
<span class="normal">409</span>
<span class="normal">410</span>
<span class="normal">411</span>
<span class="normal">412</span>
<span class="normal">413</span>
<span class="normal">414</span>
<span class="normal">415</span>
<span class="normal">416</span>
<span class="normal">417</span>
<span class="normal">418</span>
<span class="normal">419</span>
<span class="normal">420</span>
<span class="normal">421</span>
<span class="normal">422</span>
<span class="normal">423</span>
<span class="normal">424</span>
<span class="normal">425</span>
<span class="normal">426</span>
<span class="normal">427</span>
<span class="normal">428</span>
<span class="normal">429</span>
<span class="normal">430</span>
<span class="normal">431</span>
<span class="normal">432</span>
<span class="normal">433</span>
<span class="normal">434</span>
<span class="normal">435</span>
<span class="normal">436</span>
<span class="normal">437</span>
<span class="normal">438</span>
<span class="normal">439</span>
<span class="normal">440</span>
<span class="normal">441</span>
<span class="normal">442</span>
<span class="normal">443</span>
<span class="normal">444</span>
<span class="normal">445</span>
<span class="normal">446</span>
<span class="normal">447</span>
<span class="normal">448</span>
<span class="normal">449</span>
<span class="normal">450</span>
<span class="normal">451</span>
<span class="normal">452</span>
<span class="normal">453</span>
<span class="normal">454</span>
<span class="normal">455</span>
<span class="normal">456</span>
<span class="normal">457</span>
<span class="normal">458</span>
<span class="normal">459</span>
<span class="normal">460</span>
<span class="normal">461</span>
<span class="normal">462</span>
<span class="normal">463</span>
<span class="normal">464</span>
<span class="normal">465</span>
<span class="normal">466</span>
<span class="normal">467</span>
<span class="normal">468</span>
<span class="normal">469</span>
<span class="normal">470</span>
<span class="normal">471</span>
<span class="normal">472</span>
<span class="normal">473</span>
<span class="normal">474</span>
<span class="normal">475</span>
<span class="normal">476</span>
<span class="normal">477</span>
<span class="normal">478</span>
<span class="normal">479</span>
<span class="normal">480</span>
<span class="normal">481</span>
<span class="normal">482</span>
<span class="normal">483</span>
<span class="normal">484</span>
<span class="normal">485</span>
<span class="normal">486</span>
<span class="normal">487</span>
<span class="normal">488</span>
<span class="normal">489</span>
<span class="normal">490</span>
<span class="normal">491</span>
<span class="normal">492</span>
<span class="normal">493</span>
<span class="normal">494</span>
<span class="normal">495</span>
<span class="normal">496</span>
<span class="normal">497</span>
<span class="normal">498</span>
<span class="normal">499</span>
<span class="normal">500</span>
<span class="normal">501</span>
<span class="normal">502</span>
<span class="normal">503</span>
<span class="normal">504</span>
<span class="normal">505</span>
<span class="normal">506</span>
<span class="normal">507</span>
<span class="normal">508</span>
<span class="normal">509</span>
<span class="normal">510</span>
<span class="normal">511</span>
<span class="normal">512</span>
<span class="normal">513</span>
<span class="normal">514</span>
<span class="normal">515</span>
<span class="normal">516</span>
<span class="normal">517</span>
<span class="normal">518</span>
<span class="normal">519</span>
<span class="normal">520</span>
<span class="normal">521</span>
<span class="normal">522</span>
<span class="normal">523</span>
<span class="normal">524</span>
<span class="normal">525</span>
<span class="normal">526</span>
<span class="normal">527</span>
<span class="normal">528</span>
<span class="normal">529</span>
<span class="normal">530</span>
<span class="normal">531</span>
<span class="normal">532</span>
<span class="normal">533</span>
<span class="normal">534</span>
<span class="normal">535</span>
<span class="normal">536</span>
<span class="normal">537</span>
<span class="normal">538</span>
<span class="normal">539</span>
<span class="normal">540</span>
<span class="normal">541</span>
<span class="normal">542</span>
<span class="normal">543</span>
<span class="normal">544</span>
<span class="normal">545</span>
<span class="normal">546</span>
<span class="normal">547</span>
<span class="normal">548</span>
<span class="normal">549</span>
<span class="normal">550</span>
<span class="normal">551</span>
<span class="normal">552</span>
<span class="normal">553</span>
<span class="normal">554</span>
<span class="normal">555</span>
<span class="normal">556</span>
<span class="normal">557</span>
<span class="normal">558</span>
<span class="normal">559</span>
<span class="normal">560</span>
<span class="normal">561</span>
<span class="normal">562</span>
<span class="normal">563</span>
<span class="normal">564</span>
<span class="normal">565</span>
<span class="normal">566</span>
<span class="normal">567</span>
<span class="normal">568</span>
<span class="normal">569</span>
<span class="normal">570</span>
<span class="normal">571</span>
<span class="normal">572</span>
<span class="normal">573</span>
<span class="normal">574</span>
<span class="normal">575</span>
<span class="normal">576</span>
<span class="normal">577</span>
<span class="normal">578</span>
<span class="normal">579</span>
<span class="normal">580</span>
<span class="normal">581</span>
<span class="normal">582</span>
<span class="normal">583</span>
<span class="normal">584</span>
<span class="normal">585</span>
<span class="normal">586</span>
<span class="normal">587</span>
<span class="normal">588</span>
<span class="normal">589</span>
<span class="normal">590</span>
<span class="normal">591</span>
<span class="normal">592</span>
<span class="normal">593</span>
<span class="normal">594</span>
<span class="normal">595</span>
<span class="normal">596</span>
<span class="normal">597</span>
<span class="normal">598</span>
<span class="normal">599</span>
<span class="normal">600</span>
<span class="normal">601</span>
<span class="normal">602</span>
<span class="normal">603</span>
<span class="normal">604</span>
<span class="normal">605</span>
<span class="normal">606</span>
<span class="normal">607</span>
<span class="normal">608</span>
<span class="normal">609</span>
<span class="normal">610</span>
<span class="normal">611</span>
<span class="normal">612</span>
<span class="normal">613</span>
<span class="normal">614</span>
<span class="normal">615</span>
<span class="normal">616</span>
<span class="normal">617</span>
<span class="normal">618</span>
<span class="normal">619</span>
<span class="normal">620</span>
<span class="normal">621</span>
<span class="normal">622</span>
<span class="normal">623</span>
<span class="normal">624</span>
<span class="normal">625</span>
<span class="normal">626</span>
<span class="normal">627</span>
<span class="normal">628</span>
<span class="normal">629</span>
<span class="normal">630</span>
<span class="normal">631</span>
<span class="normal">632</span>
<span class="normal">633</span>
<span class="normal">634</span>
<span class="normal">635</span>
<span class="normal">636</span>
<span class="normal">637</span>
<span class="normal">638</span>
<span class="normal">639</span>
<span class="normal">640</span>
<span class="normal">641</span>
<span class="normal">642</span>
<span class="normal">643</span>
<span class="normal">644</span>
<span class="normal">645</span>
<span class="normal">646</span>
<span class="normal">647</span>
<span class="normal">648</span>
<span class="normal">649</span>
<span class="normal">650</span>
<span class="normal">651</span>
<span class="normal">652</span>
<span class="normal">653</span>
<span class="normal">654</span>
<span class="normal">655</span>
<span class="normal">656</span>
<span class="normal">657</span>
<span class="normal">658</span>
<span class="normal">659</span>
<span class="normal">660</span>
<span class="normal">661</span>
<span class="normal">662</span>
<span class="normal">663</span>
<span class="normal">664</span>
<span class="normal">665</span>
<span class="normal">666</span>
<span class="normal">667</span>
<span class="normal">668</span>
<span class="normal">669</span>
<span class="normal">670</span>
<span class="normal">671</span>
<span class="normal">672</span>
<span class="normal">673</span>
<span class="normal">674</span>
<span class="normal">675</span>
<span class="normal">676</span>
<span class="normal">677</span>
<span class="normal">678</span>
<span class="normal">679</span>
<span class="normal">680</span>
<span class="normal">681</span>
<span class="normal">682</span>
<span class="normal">683</span>
<span class="normal">684</span>
<span class="normal">685</span>
<span class="normal">686</span>
<span class="normal">687</span>
<span class="normal">688</span>
<span class="normal">689</span>
<span class="normal">690</span>
<span class="normal">691</span>
<span class="normal">692</span>
<span class="normal">693</span>
<span class="normal">694</span>
<span class="normal">695</span>
<span class="normal">696</span>
<span class="normal">697</span>
<span class="normal">698</span>
<span class="normal">699</span>
<span class="normal">700</span>
<span class="normal">701</span>
<span class="normal">702</span>
<span class="normal">703</span>
<span class="normal">704</span>
<span class="normal">705</span>
<span class="normal">706</span>
<span class="normal">707</span>
<span class="normal">708</span>
<span class="normal">709</span>
<span class="normal">710</span>
<span class="normal">711</span>
<span class="normal">712</span></pre></div></td><td class="code"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span id="2"><span class="c1"># coding: utf-8</span></span>
<span id="3"></span>
<span id="4"><span class="c1"># In[1]:</span></span>
<span id="5"></span>
<span id="6"></span>
<span id="7"><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span></span>
<span id="8"><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span> </span>
<span id="9"><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span></span>
<span id="10"><span class="kn">from</span> <span class="nn">patsy</span> <span class="kn">import</span> <span class="n">dmatrices</span></span>
<span id="11"><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span></span>
<span id="12"><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span></span>
<span id="13"><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span></span>
<span id="14"><span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span></span>
<span id="15"><span class="n">plt</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">use</span><span class="p">(</span><span class="s1">&#39;fivethirtyeight&#39;</span><span class="p">)</span></span>
<span id="16"><span class="kn">import</span> <span class="nn">patsy</span></span>
<span id="17"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">classification_report</span><span class="p">,</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span></span>
<span id="18"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_auc_score</span></span>
<span id="19"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">roc_curve</span></span>
<span id="20"><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">auc</span></span>
<span id="21"><span class="kn">from</span> <span class="nn">sklearn.grid_search</span> <span class="kn">import</span> <span class="n">GridSearchCV</span></span>
<span id="22"><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span></span>
<span id="23"></span>
<span id="24"></span>
<span id="25"><span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">&#39;matplotlib&#39;</span><span class="p">,</span> <span class="s1">&#39;inline&#39;</span><span class="p">)</span></span>
<span id="26"></span>
<span id="27"></span>
<span id="28"><span class="c1"># In[2]:</span></span>
<span id="29"></span>
<span id="30"></span>
<span id="31"><span class="n">get_ipython</span><span class="p">()</span><span class="o">.</span><span class="n">run_line_magic</span><span class="p">(</span><span class="s1">&#39;load_ext&#39;</span><span class="p">,</span> <span class="s1">&#39;sql&#39;</span><span class="p">)</span></span>
<span id="32"></span>
<span id="33"></span>
<span id="34"><span class="c1"># ## Pre-Task: Describe the goals of your study</span></span>
<span id="35"></span>
<span id="36"><span class="c1"># Wealthier Women and children had a higher chance of surviving</span></span>
<span id="37"></span>
<span id="38"><span class="c1">#   </span></span>
<span id="39"></span>
<span id="40"><span class="c1"># ## Part 1: Aquire the Data</span></span>
<span id="41"></span>
<span id="42"><span class="c1"># psql -h dsi.c20gkj5cvu3l.us-east-1.rds.amazonaws.com -p 5432 -U dsi_student titanic</span></span>
<span id="43"><span class="c1"># password: gastudents</span></span>
<span id="44"><span class="c1"># CONNECTED IN TERMINAL</span></span>
<span id="45"></span>
<span id="46"><span class="c1"># #### 1. Connect to the remote database</span></span>
<span id="47"></span>
<span id="48"><span class="c1"># In[3]:</span></span>
<span id="49"></span>
<span id="50"></span>
<span id="51"><span class="c1">#import remote data into python notebook</span></span>
<span id="52"><span class="kn">from</span> <span class="nn">sqlalchemy</span> <span class="kn">import</span> <span class="n">create_engine</span></span>
<span id="53"><span class="n">engine</span> <span class="o">=</span> <span class="n">create_engine</span><span class="p">(</span><span class="s1">&#39;postgresql://dsi_student:gastudents@dsi.c20gkj5cvu3l.us-east-1.rds.amazonaws.com/titanic&#39;</span><span class="p">)</span></span>
<span id="54"></span>
<span id="55"></span>
<span id="56"><span class="c1"># In[4]:</span></span>
<span id="57"></span>
<span id="58"></span>
<span id="59"><span class="c1">#create dataframe in pandas from titanic data</span></span>
<span id="60"><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_sql</span><span class="p">(</span><span class="s1">&#39;SELECT * FROM train&#39;</span><span class="p">,</span> <span class="n">engine</span><span class="p">)</span></span>
<span id="61"></span>
<span id="62"></span>
<span id="63"><span class="c1"># #### 2. Query the database and aggregate the data</span></span>
<span id="64"></span>
<span id="65"><span class="c1"># In[5]:</span></span>
<span id="66"></span>
<span id="67"></span>
<span id="68"><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></span>
<span id="69"></span>
<span id="70"></span>
<span id="71"><span class="c1"># #### 5. What are the risks and assumptions of our data? </span></span>
<span id="72"></span>
<span id="73"><span class="c1"># Assume that the data without age can be dropped, assume that this is representative of all data. Assume that survived is accurate. </span></span>
<span id="74"></span>
<span id="75"><span class="c1"># ## Part 2: Exploratory Data Analysis</span></span>
<span id="76"></span>
<span id="77"><span class="c1"># #### 1. Describe the Data</span></span>
<span id="78"></span>
<span id="79"><span class="c1"># In[6]:</span></span>
<span id="80"></span>
<span id="81"></span>
<span id="82"><span class="n">df</span><span class="o">.</span><span class="n">describe</span><span class="p">()</span></span>
<span id="83"></span>
<span id="84"></span>
<span id="85"><span class="c1"># In[7]:</span></span>
<span id="86"></span>
<span id="87"></span>
<span id="88"><span class="n">df</span><span class="o">.</span><span class="n">info</span><span class="p">()</span></span>
<span id="89"></span>
<span id="90"></span>
<span id="91"><span class="c1"># #### 2. Visualize the Data</span></span>
<span id="92"></span>
<span id="93"><span class="c1"># In[9]:</span></span>
<span id="94"></span>
<span id="95"></span>
<span id="96"><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">(),</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></span>
<span id="97"></span>
<span id="98"></span>
<span id="99"><span class="c1"># In[10]:</span></span>
<span id="100"></span>
<span id="101"></span>
<span id="102"><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></span>
<span id="103"><span class="n">sns</span><span class="o">.</span><span class="n">clustermap</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">corr</span><span class="p">(),</span> <span class="n">annot</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></span>
<span id="104"></span>
<span id="105"></span>
<span id="106"><span class="c1"># In[12]:</span></span>
<span id="107"></span>
<span id="108"></span>
<span id="109"><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">1.5</span><span class="p">)</span></span>
<span id="110"><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">x_vars</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Survived&quot;</span><span class="p">],</span> <span class="n">y_vars</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Age&quot;</span><span class="p">,</span> <span class="s2">&quot;Parch&quot;</span><span class="p">,</span> <span class="s2">&quot;Pclass&quot;</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span></span>
<span id="111"></span>
<span id="112"></span>
<span id="113"><span class="c1"># In[13]:</span></span>
<span id="114"></span>
<span id="115"></span>
<span id="116"><span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">())</span></span>
<span id="117"></span>
<span id="118"></span>
<span id="119"><span class="c1"># In[14]:</span></span>
<span id="120"></span>
<span id="121"></span>
<span id="122"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;AgeRounded&#39;</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">]]</span></span>
<span id="123"></span>
<span id="124"></span>
<span id="125"><span class="c1"># In[15]:</span></span>
<span id="126"></span>
<span id="127"></span>
<span id="128"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;AgeRounded&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span></span>
<span id="129"></span>
<span id="130"></span>
<span id="131"><span class="c1"># In[16]:</span></span>
<span id="132"></span>
<span id="133"></span>
<span id="134"><span class="n">survived_age</span> <span class="o">=</span> <span class="n">df</span><span class="p">[[</span><span class="s2">&quot;AgeRounded&quot;</span><span class="p">,</span> <span class="s2">&quot;Survived&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s1">&#39;AgeRounded&#39;</span><span class="p">],</span><span class="n">as_index</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span></span>
<span id="135"><span class="n">survived_age</span></span>
<span id="136"></span>
<span id="137"></span>
<span id="138"><span class="c1"># In[17]:</span></span>
<span id="139"></span>
<span id="140"></span>
<span id="141"><span class="n">sns</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">font_scale</span><span class="o">=</span><span class="mf">2.2</span><span class="p">)</span></span>
<span id="142"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">45</span><span class="p">,</span><span class="mi">30</span><span class="p">))</span></span>
<span id="143"></span>
<span id="144"><span class="n">sns</span><span class="o">.</span><span class="n">barplot</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s1">&#39;AgeRounded&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Survived&#39;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">survived_age</span><span class="p">)</span></span>
<span id="145"></span>
<span id="146"></span>
<span id="147"><span class="c1"># ## Part 3: Data Wrangling</span></span>
<span id="148"></span>
<span id="149"><span class="c1"># #### 1. Create Dummy Variables for *Sex* </span></span>
<span id="150"></span>
<span id="151"><span class="c1"># In[18]:</span></span>
<span id="152"></span>
<span id="153"></span>
<span id="154"><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></span>
<span id="155"></span>
<span id="156"></span>
<span id="157"><span class="c1"># In[19]:</span></span>
<span id="158"></span>
<span id="159"></span>
<span id="160"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;female&#39;</span><span class="p">]</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span> <span class="k">if</span> <span class="n">x</span><span class="o">==</span><span class="s1">&#39;female&#39;</span> <span class="k">else</span> <span class="mi">0</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">df</span><span class="p">[</span><span class="s1">&#39;Sex&#39;</span><span class="p">]]</span></span>
<span id="161"></span>
<span id="162"></span>
<span id="163"><span class="c1"># In[20]:</span></span>
<span id="164"></span>
<span id="165"></span>
<span id="166"><span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></span>
<span id="167"></span>
<span id="168"></span>
<span id="169"><span class="c1"># ## Part 4: Logistic Regression and Model Validation</span></span>
<span id="170"></span>
<span id="171"><span class="c1"># #### 1. Define the variables that we will use in our classification analysis</span></span>
<span id="172"></span>
<span id="173"><span class="c1"># In[21]:</span></span>
<span id="174"></span>
<span id="175"></span>
<span id="176"><span class="n">df</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">dropna</span><span class="p">(</span><span class="n">subset</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">])</span></span>
<span id="177"></span>
<span id="178"></span>
<span id="179"><span class="c1"># In[22]:</span></span>
<span id="180"></span>
<span id="181"></span>
<span id="182"><span class="sd">&#39;&#39;&#39;according to National Statistical Standards the age distribution of the population for demographic purposes should be</span></span>
<span id="183"><span class="sd">given in five-year age groups extending to 85 years and over.&#39;&#39;&#39;</span></span>
<span id="184"><span class="c1">#May try another one with smaller groups</span></span>
<span id="185"></span>
<span id="186"><span class="n">bins</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">19</span><span class="p">,</span> <span class="mi">24</span><span class="p">,</span> <span class="mi">29</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span> <span class="mi">39</span><span class="p">,</span> <span class="mi">44</span><span class="p">,</span> <span class="mi">49</span><span class="p">,</span> <span class="mi">54</span><span class="p">,</span> <span class="mi">59</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">69</span><span class="p">,</span> <span class="mi">74</span><span class="p">,</span> <span class="mi">79</span><span class="p">,</span> <span class="mi">84</span><span class="p">]</span></span>
<span id="187"><span class="c1">#highest age is 80</span></span>
<span id="188"><span class="n">group_names</span> <span class="o">=</span><span class="p">[</span><span class="s1">&#39;0-4 years&#39;</span><span class="p">,</span> <span class="s1">&#39;5-9 years&#39;</span><span class="p">,</span><span class="s1">&#39;10-14 years&#39;</span><span class="p">,</span> <span class="s1">&#39;15-19 years&#39;</span><span class="p">,</span> <span class="s1">&#39;20-24 years&#39;</span><span class="p">,</span> <span class="s1">&#39;25-29 years&#39;</span><span class="p">,</span> <span class="s1">&#39;30-34 years&#39;</span><span class="p">,</span><span class="s1">&#39;35-39 years&#39;</span><span class="p">,</span> <span class="s1">&#39;40-44 years&#39;</span><span class="p">,</span> <span class="s1">&#39;45-49 years&#39;</span><span class="p">,</span> <span class="s1">&#39;50-54 years&#39;</span><span class="p">,</span> <span class="s1">&#39;55-59 years&#39;</span><span class="p">,</span> <span class="s1">&#39;60-64 years&#39;</span><span class="p">,</span> <span class="s1">&#39;65-69 years&#39;</span><span class="p">,</span><span class="s1">&#39;70-74 years&#39;</span><span class="p">,</span> <span class="s1">&#39;75-79 years&#39;</span><span class="p">,</span> <span class="s1">&#39;80-84 years&#39;</span><span class="p">]</span></span>
<span id="189"></span>
<span id="190"></span>
<span id="191"><span class="c1"># In[23]:</span></span>
<span id="192"></span>
<span id="193"></span>
<span id="194"><span class="c1">#create new column with age labels</span></span>
<span id="195"><span class="n">df</span><span class="p">[</span><span class="s1">&#39;agebin&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">cut</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Age&#39;</span><span class="p">],</span> <span class="n">bins</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">group_names</span><span class="p">)</span></span>
<span id="196"></span>
<span id="197"></span>
<span id="198"><span class="c1"># In[28]:</span></span>
<span id="199"></span>
<span id="200"></span>
<span id="201"><span class="c1">#define features</span></span>
<span id="202"><span class="n">features</span><span class="o">=</span><span class="n">df</span><span class="p">[[</span><span class="s1">&#39;agebin&#39;</span><span class="p">,</span><span class="s1">&#39;Sex&#39;</span><span class="p">,</span><span class="s1">&#39;Pclass&#39;</span><span class="p">,</span> <span class="s1">&#39;Parch&#39;</span><span class="p">,</span> <span class="s1">&#39;Fare&#39;</span> <span class="p">]]</span></span>
<span id="203"></span>
<span id="204"></span>
<span id="205"><span class="c1"># In[29]:</span></span>
<span id="206"></span>
<span id="207"></span>
<span id="208"><span class="n">features</span><span class="o">.</span><span class="n">head</span><span class="p">()</span></span>
<span id="209"></span>
<span id="210"></span>
<span id="211"><span class="c1"># In[30]:</span></span>
<span id="212"></span>
<span id="213"></span>
<span id="214"><span class="n">farelist</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Fare&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">unique</span><span class="p">()</span></span>
<span id="215"></span>
<span id="216"></span>
<span id="217"><span class="c1"># In[31]:</span></span>
<span id="218"></span>
<span id="219"></span>
<span id="220"><span class="nb">sorted</span><span class="p">(</span><span class="n">farelist</span><span class="p">)</span></span>
<span id="221"><span class="c1">#decide to note include fare as a predictor</span></span>
<span id="222"></span>
<span id="223"></span>
<span id="224"><span class="c1"># In[33]:</span></span>
<span id="225"></span>
<span id="226"></span>
<span id="227"><span class="c1">#define Predictors in categorical contexts</span></span>
<span id="228"><span class="n">X</span><span class="o">=</span> <span class="n">patsy</span><span class="o">.</span><span class="n">dmatrix</span><span class="p">(</span><span class="s1">&#39;~ C(agebin) + C(Sex) + C(Pclass)+ C(Parch)&#39;</span><span class="p">,</span> <span class="n">df</span><span class="p">)</span></span>
<span id="229"></span>
<span id="230"></span>
<span id="231"><span class="c1"># In[34]:</span></span>
<span id="232"></span>
<span id="233"></span>
<span id="234"><span class="n">X</span></span>
<span id="235"></span>
<span id="236"></span>
<span id="237"><span class="c1"># #### 2. Transform &quot;Y&quot; into a 1-Dimensional Array for SciKit-Learn</span></span>
<span id="238"></span>
<span id="239"><span class="c1"># In[35]:</span></span>
<span id="240"></span>
<span id="241"></span>
<span id="242"><span class="c1">#Define category that will be predicted</span></span>
<span id="243"><span class="n">y</span><span class="o">=</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Survived&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span></span>
<span id="244"></span>
<span id="245"></span>
<span id="246"><span class="c1"># #### 3. Conduct the logistic regression</span></span>
<span id="247"></span>
<span id="248"><span class="c1"># In[36]:</span></span>
<span id="249"></span>
<span id="250"></span>
<span id="251"><span class="c1">#DECIDED NOT TO SCALE BECUASE THERE ARE NO CONTINUOUS VARIABLES</span></span>
<span id="252"></span>
<span id="253"></span>
<span id="254"><span class="c1"># In[37]:</span></span>
<span id="255"></span>
<span id="256"></span>
<span id="257"><span class="c1">#split data into test and train</span></span>
<span id="258"><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">.33</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">66</span><span class="p">)</span></span>
<span id="259"></span>
<span id="260"></span>
<span id="261"><span class="c1"># In[38]:</span></span>
<span id="262"></span>
<span id="263"></span>
<span id="264"><span class="c1">#create &#39;vanilla&#39; linear model</span></span>
<span id="265"><span class="n">lr</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">solver</span><span class="o">=</span><span class="s1">&#39;liblinear&#39;</span><span class="p">)</span> </span>
<span id="266"><span class="n">lr_model</span> <span class="o">=</span> <span class="n">lr</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">train</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([266, 305, 313, 321])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">no independent test data</button>
<span id="267"></span>
<span id="268"></span>
<span id="269"><span class="c1"># #### 4. Examine the coefficients to see our correlations</span></span>
<span id="270"></span>
<span id="271"><span class="c1"># In[39]:</span></span>
<span id="272"></span>
<span id="273"></span>
<span id="274"><span class="n">lr_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span></span>
<span id="275"></span>
<span id="276"></span>
<span id="277"><span class="c1"># In[40]:</span></span>
<span id="278"></span>
<span id="279"></span>
<span id="280"><span class="n">featurenames</span><span class="o">=</span><span class="n">X</span><span class="o">.</span><span class="n">design_info</span><span class="o">.</span><span class="n">column_names</span></span>
<span id="281"></span>
<span id="282"></span>
<span id="283"><span class="c1"># In[43]:</span></span>
<span id="284"></span>
<span id="285"></span>
<span id="286"><span class="n">coeff_logreg</span><span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">featurenames</span><span class="p">,</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">coef_</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span></span>
<span id="287"><span class="n">coeff_logreg</span><span class="o">.</span><span class="n">columns</span><span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Feature_Name&#39;</span><span class="p">,</span> <span class="s1">&#39;Coefficient&#39;</span><span class="p">]</span></span>
<span id="288"><span class="n">coeff_logreg</span></span>
<span id="289"></span>
<span id="290"></span>
<span id="291"><span class="c1"># #### 6. Test the Model by introducing a *Test* or *Validaton* set </span></span>
<span id="292"></span>
<span id="293"><span class="c1"># In[44]:</span></span>
<span id="294"></span>
<span id="295"></span>
<span id="296"><span class="c1">#Done ABOVE</span></span>
<span id="297"></span>
<span id="298"></span>
<span id="299"><span class="c1"># #### 7. Predict the class labels for the *Test* set</span></span>
<span id="300"></span>
<span id="301"><span class="c1"># In[45]:</span></span>
<span id="302"></span>
<span id="303"></span>
<span id="304"><span class="c1">#predict the survival for X_test </span></span>
<span id="305"><span class="n">y_pred</span><span class="o">=</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">validation</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([266, 305, 313, 321])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">used multiple times</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([305, 591])">highlight other usage</button>
<span id="306"></span>
<span id="307"></span>
<span id="308"><span class="c1"># #### 8. Predict the class probabilities for the *Test* set</span></span>
<span id="309"></span>
<span id="310"><span class="c1"># In[46]:</span></span>
<span id="311"></span>
<span id="312"></span>
<span id="313"><span class="n">lr_model</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">validation</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([266, 305, 313, 321])">highlight train/test sites</button>
<span id="314"></span>
<span id="315"></span>
<span id="316"><span class="c1"># #### 9. Evaluate the *Test* set</span></span>
<span id="317"></span>
<span id="318"><span class="c1"># In[47]:</span></span>
<span id="319"></span>
<span id="320"></span>
<span id="321"><span class="n">lr_model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">validation</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([266, 305, 313, 321])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">used multiple times</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([321, 591])">highlight other usage</button>
<span id="322"></span>
<span id="323"></span>
<span id="324"><span class="c1"># #### 10. Cross validate the test set</span></span>
<span id="325"></span>
<span id="326"><span class="c1"># In[48]:</span></span>
<span id="327"></span>
<span id="328"></span>
<span id="329"><span class="n">cross_val_score</span><span class="p">(</span><span class="n">lr_model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;f1_weighted&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span></span>
<span id="330"></span>
<span id="331"></span>
<span id="332"><span class="c1"># #### 11. Check the Classification Report</span></span>
<span id="333"></span>
<span id="334"><span class="c1"># In[49]:</span></span>
<span id="335"></span>
<span id="336"></span>
<span id="337"><span class="nb">print</span><span class="p">((</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">lr_model</span><span class="o">.</span><span class="n">classes_</span><span class="p">)))</span></span>
<span id="338"></span>
<span id="339"></span>
<span id="340"><span class="c1"># #### 12. What do the classification metrics tell us?</span></span>
<span id="341"></span>
<span id="342"><span class="c1"># #F1 score is weighted average of the precision and recall. This is where 1 is best and 0 is worst. </span></span>
<span id="343"></span>
<span id="344"><span class="c1"># #### 13. Check the Confusion Matrix</span></span>
<span id="345"></span>
<span id="346"><span class="c1"># In[50]:</span></span>
<span id="347"></span>
<span id="348"></span>
<span id="349"><span class="n">cm</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">lr_model</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span></span>
<span id="350"><span class="n">cm</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cm</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">lr_model</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">lr_model</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span></span>
<span id="351"><span class="n">cm</span></span>
<span id="352"></span>
<span id="353"></span>
<span id="354"><span class="c1"># #### 14. What does the Confusion Matrix tell us? </span></span>
<span id="355"></span>
<span id="356"><span class="c1"># Errors of predicted survival tend to occur 18% of the time. Specifically, there were 15 occurances of a false positive and 28 occurances of false negatives</span></span>
<span id="357"></span>
<span id="358"><span class="c1"># #### 15. Plot the ROC curve</span></span>
<span id="359"></span>
<span id="360"><span class="c1"># In[51]:</span></span>
<span id="361"></span>
<span id="362"></span>
<span id="363"><span class="c1">#get score for the y prediction</span></span>
<span id="364"><span class="n">y_score</span> <span class="o">=</span> <span class="n">lr_model</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></span>
<span id="365"></span>
<span id="366"></span>
<span id="367"><span class="c1"># In[52]:</span></span>
<span id="368"></span>
<span id="369"></span>
<span id="370"><span class="n">FPR</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span></span>
<span id="371"><span class="n">TPR</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span></span>
<span id="372"><span class="n">ROC_AUC</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span></span>
<span id="373"></span>
<span id="374"><span class="c1"># For class 1, find the area under the curve</span></span>
<span id="375"><span class="n">FPR</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">TPR</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_score</span><span class="p">)</span></span>
<span id="376"><span class="n">ROC_AUC</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">FPR</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">TPR</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span></span>
<span id="377"></span>
<span id="378"><span class="c1"># Plot of a ROC curve for class 1 (has_cancer)</span></span>
<span id="379"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">11</span><span class="p">,</span><span class="mi">9</span><span class="p">])</span></span>
<span id="380"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">FPR</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">TPR</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ROC curve (area = </span><span class="si">%0.2f</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="n">ROC_AUC</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span></span>
<span id="381"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span></span>
<span id="382"><span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span></span>
<span id="383"><span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span></span>
<span id="384"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span></span>
<span id="385"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span></span>
<span id="386"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Receiver operating characteristic for high/low income&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span></span>
<span id="387"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span></span>
<span id="388"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span>
<span id="389"></span>
<span id="390"></span>
<span id="391"><span class="c1"># #### 16. What does the ROC curve tell us?</span></span>
<span id="392"></span>
<span id="393"><span class="c1"># &quot;This means that the top left corner of the plot is the ideal point - a false positive rate of zero, and a true positive rate of one.&quot;-http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html</span></span>
<span id="394"></span>
<span id="395"><span class="c1"># ## Part 5: Gridsearch</span></span>
<span id="396"></span>
<span id="397"><span class="c1"># #### 1. Use GridSearchCV with logistic regression to search for optimal parameters </span></span>
<span id="398"><span class="c1"># </span></span>
<span id="399"><span class="c1"># - Use the provided parameter grid. Feel free to add if you like (such as n_jobs).</span></span>
<span id="400"><span class="c1"># - Use 5-fold cross-validation.</span></span>
<span id="401"></span>
<span id="402"><span class="c1"># In[53]:</span></span>
<span id="403"></span>
<span id="404"></span>
<span id="405"><span class="n">logreg_parameters</span> <span class="o">=</span> <span class="p">{</span></span>
<span id="406">    <span class="s1">&#39;penalty&#39;</span><span class="p">:[</span><span class="s1">&#39;l1&#39;</span><span class="p">,</span><span class="s1">&#39;l2&#39;</span><span class="p">],</span></span>
<span id="407">    <span class="s1">&#39;C&#39;</span><span class="p">:</span><span class="n">np</span><span class="o">.</span><span class="n">logspace</span><span class="p">(</span><span class="o">-</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">50</span><span class="p">),</span></span>
<span id="408">    <span class="s1">&#39;solver&#39;</span><span class="p">:[</span><span class="s1">&#39;liblinear&#39;</span><span class="p">]</span></span>
<span id="409"><span class="p">}</span></span>
<span id="410"></span>
<span id="411"></span>
<span id="412"><span class="c1"># In[54]:</span></span>
<span id="413"></span>
<span id="414"></span>
<span id="415"><span class="c1">#run gridsearch on the dict</span></span>
<span id="416"><span class="n">gs</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">lr_model</span><span class="p">,</span> <span class="p">{</span><span class="s1">&#39;penalty&#39;</span><span class="p">:</span> <span class="n">logreg_parameters</span><span class="p">[</span><span class="s1">&#39;penalty&#39;</span><span class="p">],</span> <span class="s1">&#39;C&#39;</span><span class="p">:</span> <span class="n">logreg_parameters</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">]},</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">15</span><span class="p">)</span></span>
<span id="417"><span class="n">gs</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">train</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">no independent test data</button>
<span id="418"></span>
<span id="419"></span>
<span id="420"><span class="c1"># In[55]:</span></span>
<span id="421"></span>
<span id="422"></span>
<span id="423"><span class="c1">#return best model parameters</span></span>
<span id="424"><span class="n">gs</span><span class="o">.</span><span class="n">best_params_</span></span>
<span id="425"></span>
<span id="426"></span>
<span id="427"><span class="c1"># In[56]:</span></span>
<span id="428"></span>
<span id="429"></span>
<span id="430"><span class="c1">#return best score</span></span>
<span id="431"><span class="n">gs</span><span class="o">.</span><span class="n">best_score_</span></span>
<span id="432"></span>
<span id="433"></span>
<span id="434"><span class="c1"># In[57]:</span></span>
<span id="435"></span>
<span id="436"></span>
<span id="437"><span class="c1">#fit model with gridsearch info</span></span>
<span id="438"><span class="n">logreg</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="n">gs</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">&#39;C&#39;</span><span class="p">],</span> <span class="n">penalty</span><span class="o">=</span><span class="n">gs</span><span class="o">.</span><span class="n">best_params_</span><span class="p">[</span><span class="s1">&#39;penalty&#39;</span><span class="p">])</span></span>
<span id="439"><span class="n">gs_model</span> <span class="o">=</span> <span class="n">logreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">train</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([439, 443])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">no independent test data</button>
<span id="440"></span>
<span id="441"> </span>
<span id="442"><span class="c1">#predict y</span></span>
<span id="443"><span class="n">gs_pred</span> <span class="o">=</span> <span class="n">gs_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">validation</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([439, 443])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">used multiple times</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([443, 591])">highlight other usage</button>
<span id="444"></span>
<span id="445"></span>
<span id="446"><span class="c1"># In[58]:</span></span>
<span id="447"></span>
<span id="448"></span>
<span id="449"><span class="c1">#create confusion matrix</span></span>
<span id="450"><span class="n">cm1</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">gs_pred</span><span class="p">,</span> <span class="n">labels</span><span class="o">=</span><span class="n">logreg</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span></span>
<span id="451"><span class="n">cm1</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cm1</span><span class="p">,</span> <span class="n">columns</span><span class="o">=</span><span class="n">logreg</span><span class="o">.</span><span class="n">classes_</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">logreg</span><span class="o">.</span><span class="n">classes_</span><span class="p">)</span></span>
<span id="452"><span class="n">cm1</span></span>
<span id="453"></span>
<span id="454"></span>
<span id="455"><span class="c1"># #### 2. Print out the best parameters and best score. Are they better than the vanilla logistic regression?</span></span>
<span id="456"></span>
<span id="457"><span class="c1"># Seen above... It seems as though the false positive rate grew in the gridsearch model and the false negative fell. The model now errors on the side of saying someone survived. </span></span>
<span id="458"></span>
<span id="459"><span class="c1"># In[59]:</span></span>
<span id="460"></span>
<span id="461"></span>
<span id="462"><span class="n">y_score2</span> <span class="o">=</span> <span class="n">gs_model</span><span class="o">.</span><span class="n">decision_function</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></span>
<span id="463"></span>
<span id="464"></span>
<span id="465"><span class="c1"># In[60]:</span></span>
<span id="466"></span>
<span id="467"></span>
<span id="468"><span class="n">FPR_GS</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span></span>
<span id="469"><span class="n">TPR_GS</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span></span>
<span id="470"><span class="n">ROC_AUC_GS</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span></span>
<span id="471"></span>
<span id="472"><span class="c1"># For class 1, find the area under the curve</span></span>
<span id="473"><span class="n">FPR_GS</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">TPR_GS</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">_</span> <span class="o">=</span> <span class="n">roc_curve</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">y_score2</span><span class="p">)</span></span>
<span id="474"><span class="n">ROC_AUC_GS</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">auc</span><span class="p">(</span><span class="n">FPR_GS</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">TPR_GS</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span></span>
<span id="475"></span>
<span id="476"><span class="c1"># Plot of a ROC curve for class 1 (has_cancer)</span></span>
<span id="477"><span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">[</span><span class="mi">11</span><span class="p">,</span><span class="mi">9</span><span class="p">])</span></span>
<span id="478"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">FPR_GS</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">TPR_GS</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ROC curve (area = </span><span class="si">%0.2f</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="n">ROC_AUC_GS</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span></span>
<span id="479"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">FPR</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">TPR</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ROC curve (area = </span><span class="si">%0.2f</span><span class="s1">)&#39;</span> <span class="o">%</span> <span class="n">ROC_AUC</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span></span>
<span id="480"><span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="s1">&#39;k--&#39;</span><span class="p">,</span> <span class="n">linewidth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span></span>
<span id="481"><span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">])</span></span>
<span id="482"><span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">([</span><span class="mf">0.0</span><span class="p">,</span> <span class="mf">1.05</span><span class="p">])</span></span>
<span id="483"><span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;False Positive Rate&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span></span>
<span id="484"><span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;True Positive Rate&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span></span>
<span id="485"><span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Receiver operating characteristic for high/low income&#39;</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">18</span><span class="p">)</span></span>
<span id="486"><span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;lower right&quot;</span><span class="p">)</span></span>
<span id="487"><span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span></span>
<span id="488"></span>
<span id="489"></span>
<span id="490"><span class="c1"># #### 3. Explain the difference between the difference between the L1 (Lasso) and L2 (Ridge) penalties on the model coefficients.</span></span>
<span id="491"></span>
<span id="492"><span class="c1"># L1 tends to drop one variable. In a way the coefficient becomes 0. L2(Ridge) tends to change the coefficients in a smaller way.</span></span>
<span id="493"></span>
<span id="494"><span class="c1"># #### 4. What hypothetical situations are the Ridge and Lasso penalties useful?</span></span>
<span id="495"></span>
<span id="496"><span class="c1"># Use Lasso when there are too many variables and you would like to remove one/some. Ridge is used when you would like to find a middle ground between variables. </span></span>
<span id="497"></span>
<span id="498"><span class="c1"># #### 5. [BONUS] Explain how the regularization strength (C) modifies the regression loss function. Why do the Ridge and Lasso penalties have their respective effects on the coefficients?</span></span>
<span id="499"></span>
<span id="500"><span class="c1"># In[ ]:</span></span>
<span id="501"></span>
<span id="502"></span>
<span id="503"></span>
<span id="504"></span>
<span id="505"></span>
<span id="506"><span class="c1"># #### 6.a. [BONUS] You decide that you want to minimize false positives. Use the predicted probabilities from the model to set your threshold for labeling the positive class to need at least 90% confidence. How and why does this affect your confusion matrix?</span></span>
<span id="507"></span>
<span id="508"><span class="c1"># In[ ]:</span></span>
<span id="509"></span>
<span id="510"></span>
<span id="511"></span>
<span id="512"></span>
<span id="513"></span>
<span id="514"><span class="c1"># ## Part 6: Gridsearch and kNN</span></span>
<span id="515"></span>
<span id="516"><span class="c1"># #### 1. Perform Gridsearch for the same classification problem as above, but use KNeighborsClassifier as your estimator</span></span>
<span id="517"><span class="c1"># </span></span>
<span id="518"><span class="c1"># At least have number of neighbors and weights in your parameters dictionary.</span></span>
<span id="519"></span>
<span id="520"><span class="c1"># In[61]:</span></span>
<span id="521"></span>
<span id="522"></span>
<span id="523"><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">()</span></span>
<span id="524"></span>
<span id="525"></span>
<span id="526"><span class="c1"># In[62]:</span></span>
<span id="527"></span>
<span id="528"></span>
<span id="529"><span class="c1">#create dictionary for gridsearch</span></span>
<span id="530"><span class="n">param_dict</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">51</span><span class="p">)),</span>                  <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;uniform&#39;</span><span class="p">,</span> <span class="s1">&#39;distance&#39;</span><span class="p">])</span></span>
<span id="531"></span>
<span id="532"></span>
<span id="533"><span class="c1"># #### 2. Print the best parameters and score for the gridsearched kNN model. How does it compare to the logistic regression model?</span></span>
<span id="534"></span>
<span id="535"><span class="c1"># In[63]:</span></span>
<span id="536"></span>
<span id="537"></span>
<span id="538"><span class="c1">#Perform gridsearch</span></span>
<span id="539"><span class="n">gscv</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">knn</span><span class="p">,</span> <span class="n">param_dict</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">)</span></span>
<span id="540"></span>
<span id="541"></span>
<span id="542"><span class="c1"># In[64]:</span></span>
<span id="543"></span>
<span id="544"></span>
<span id="545"><span class="c1">#fit model based upon gridsearch</span></span>
<span id="546"><span class="n">gscv_model</span> <span class="o">=</span> <span class="n">gscv</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">train</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([546, 591])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">no independent test data</button>
<span id="547"></span>
<span id="548"></span>
<span id="549"><span class="c1"># In[65]:</span></span>
<span id="550"></span>
<span id="551"></span>
<span id="552"><span class="c1">#return estimator info</span></span>
<span id="553"><span class="n">gscv_model</span><span class="o">.</span><span class="n">best_estimator_</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span></span>
<span id="554"></span>
<span id="555"></span>
<span id="556"><span class="c1"># In[66]:</span></span>
<span id="557"></span>
<span id="558"></span>
<span id="559"><span class="c1">#return best parameters</span></span>
<span id="560"><span class="n">gscv</span><span class="o">.</span><span class="n">best_params_</span></span>
<span id="561"></span>
<span id="562"></span>
<span id="563"><span class="c1"># In[67]:</span></span>
<span id="564"></span>
<span id="565"></span>
<span id="566"><span class="c1">#return score for these parameters</span></span>
<span id="567"><span class="n">gscv</span><span class="o">.</span><span class="n">best_score_</span></span>
<span id="568"></span>
<span id="569"></span>
<span id="570"><span class="c1"># #### 3. How does the number of neighbors affect the bias-variance tradeoff of your model?</span></span>
<span id="571"><span class="c1"># </span></span>
<span id="572"><span class="c1"># #### [BONUS] Why?</span></span>
<span id="573"></span>
<span id="574"><span class="c1"># The lower neighbors the possibility of a neighbor skewing the result is higher. Too high of neighbors could yeild just a majority of the data set</span></span>
<span id="575"></span>
<span id="576"><span class="c1"># #### 4. In what hypothetical scenario(s) might you prefer logistic regression over kNN, aside from model performance metrics?</span></span>
<span id="577"></span>
<span id="578"><span class="c1"># the data doesnt seem to have a clear split between classification</span></span>
<span id="579"></span>
<span id="580"><span class="c1"># #### 5. Fit a new kNN model with the optimal parameters found in gridsearch. </span></span>
<span id="581"></span>
<span id="582"><span class="c1"># In[69]:</span></span>
<span id="583"></span>
<span id="584"></span>
<span id="585"><span class="c1">#model fit above with parameters</span></span>
<span id="586"></span>
<span id="587"></span>
<span id="588"><span class="c1"># In[68]:</span></span>
<span id="589"></span>
<span id="590"></span>
<span id="591"><span class="n">gscv_ypred</span> <span class="o">=</span> <span class="n">gscv_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span></span> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">test</button> <button type="button" style="line-height: 85%; background-color: green; color: white; border:none;" onclick="None">validation</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([546, 591])">highlight train/test sites</button> <button type="button" style="line-height: 85%; background-color: red; color: white; border:none;" onclick="None">used multiple times</button> <button type="button" style="line-height: 85%; None" onclick="highlight_lines([305, 321, 443, 591])">highlight other usage</button>
<span id="592"></span>
<span id="593"></span>
<span id="594"><span class="c1"># In[70]:</span></span>
<span id="595"></span>
<span id="596"></span>
<span id="597"><span class="nb">print</span><span class="p">((</span><span class="n">classification_report</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">gscv_ypred</span><span class="p">)))</span></span>
<span id="598"></span>
<span id="599"></span>
<span id="600"><span class="c1"># #### 6. Construct the confusion matrix for the optimal kNN model. Is it different from the logistic regression model? If so, how?</span></span>
<span id="601"></span>
<span id="602"><span class="c1"># In[71]:</span></span>
<span id="603"></span>
<span id="604"></span>
<span id="605"><span class="n">cm2</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">Y_test</span><span class="p">,</span> <span class="n">gscv_ypred</span><span class="p">)</span></span>
<span id="606"><span class="n">cm2</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">cm2</span><span class="p">)</span></span>
<span id="607"><span class="n">cm2</span></span>
<span id="608"></span>
<span id="609"></span>
<span id="610"><span class="c1"># In[72]:</span></span>
<span id="611"></span>
<span id="612"></span>
<span id="613"><span class="c1">#looks similar to gridsearch done with logistic regression</span></span>
<span id="614"></span>
<span id="615"></span>
<span id="616"><span class="c1"># In[ ]:</span></span>
<span id="617"></span>
<span id="618"></span>
<span id="619"></span>
<span id="620"></span>
<span id="621"></span>
<span id="622"><span class="c1"># #### 7. [BONUS] Plot the ROC curves for the optimized logistic regression model and the optimized kNN model on the same plot.</span></span>
<span id="623"></span>
<span id="624"><span class="c1"># In[ ]:</span></span>
<span id="625"></span>
<span id="626"></span>
<span id="627"></span>
<span id="628"></span>
<span id="629"></span>
<span id="630"><span class="c1"># ## Part 7: [BONUS] Precision-recall</span></span>
<span id="631"></span>
<span id="632"><span class="c1"># #### 1. Gridsearch the same parameters for logistic regression but change the scoring function to &#39;average_precision&#39;</span></span>
<span id="633"><span class="c1"># </span></span>
<span id="634"><span class="c1"># `&#39;average_precision&#39;` will optimize parameters for area under the precision-recall curve instead of for accuracy.</span></span>
<span id="635"></span>
<span id="636"><span class="c1"># In[ ]:</span></span>
<span id="637"></span>
<span id="638"></span>
<span id="639"></span>
<span id="640"></span>
<span id="641"></span>
<span id="642"><span class="c1"># #### 2. Examine the best parameters and score. Are they different than the logistic regression gridsearch in part 5?</span></span>
<span id="643"></span>
<span id="644"><span class="c1"># In[ ]:</span></span>
<span id="645"></span>
<span id="646"></span>
<span id="647"></span>
<span id="648"></span>
<span id="649"></span>
<span id="650"><span class="c1"># #### 3. Create the confusion matrix. Is it different than when you optimized for the accuracy? If so, why would this be?</span></span>
<span id="651"></span>
<span id="652"><span class="c1"># In[ ]:</span></span>
<span id="653"></span>
<span id="654"></span>
<span id="655"></span>
<span id="656"></span>
<span id="657"></span>
<span id="658"><span class="c1"># #### 4. Plot the precision-recall curve. What does this tell us as opposed to the ROC curve?</span></span>
<span id="659"><span class="c1"># </span></span>
<span id="660"><span class="c1"># [See the sklearn plotting example here.](http://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)</span></span>
<span id="661"></span>
<span id="662"><span class="c1"># In[ ]:</span></span>
<span id="663"></span>
<span id="664"></span>
<span id="665"></span>
<span id="666"></span>
<span id="667"></span>
<span id="668"><span class="c1"># ## Part 8: [VERY BONUS] Decision trees, ensembles, bagging</span></span>
<span id="669"></span>
<span id="670"><span class="c1"># #### 1. Gridsearch a decision tree classifier model on the data, searching for optimal depth. Create a new decision tree model with the optimal parameters.</span></span>
<span id="671"></span>
<span id="672"><span class="c1"># In[ ]:</span></span>
<span id="673"></span>
<span id="674"></span>
<span id="675"></span>
<span id="676"></span>
<span id="677"></span>
<span id="678"><span class="c1"># #### 2. Compare the performace of the decision tree model to the logistic regression and kNN models.</span></span>
<span id="679"></span>
<span id="680"><span class="c1"># In[ ]:</span></span>
<span id="681"></span>
<span id="682"></span>
<span id="683"></span>
<span id="684"></span>
<span id="685"></span>
<span id="686"><span class="c1"># #### 3. Plot all three optimized models&#39; ROC curves on the same plot. </span></span>
<span id="687"></span>
<span id="688"><span class="c1"># In[ ]:</span></span>
<span id="689"></span>
<span id="690"></span>
<span id="691"></span>
<span id="692"></span>
<span id="693"></span>
<span id="694"><span class="c1"># #### 4. Use sklearn&#39;s BaggingClassifier with the base estimator your optimized decision tree model. How does the performance compare to the single decision tree classifier?</span></span>
<span id="695"></span>
<span id="696"><span class="c1"># In[ ]:</span></span>
<span id="697"></span>
<span id="698"></span>
<span id="699"></span>
<span id="700"></span>
<span id="701"></span>
<span id="702"><span class="c1"># #### 5. Gridsearch the optimal n_estimators, max_samples, and max_features for the bagging classifier.</span></span>
<span id="703"></span>
<span id="704"><span class="c1"># In[ ]:</span></span>
<span id="705"></span>
<span id="706"></span>
<span id="707"></span>
<span id="708"></span>
<span id="709"></span>
<span id="710"><span class="c1"># #### 6. Create a bagging classifier model with the optimal parameters and compare it&#39;s performance to the other two models.</span></span>
<span id="711"></span>
<span id="712"><span class="c1"># In[ ]:</span></span>
</pre></div>
</td></tr></table></body>
</html>
